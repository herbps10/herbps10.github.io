<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Derivatives of a Gaussian Process | Herb Susmann</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <link href="//cdn.bootcss.com/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">


<link rel="stylesheet" href="/css/custom.css">

  </head>

  <body>
    <svg id='graph' width='800' height='800'></svg>
    <nav>
    <a href='/' class='main-title'>Herb Susmann</a>
    <ul class="menu">
      
      <li><a href="/about">About</a></li>
      
      <li><a href="/post">Posts</a></li>
      
      <li><a href="/notebooks">Notebooks</a></li>
      
      <li><a href="/research">Research</a></li>
      
      <li><a href="/software">Software</a></li>
      
    </ul>
    </nav>
    
  <div class='container universal-wrapper'>
<div class="article-meta">
<h1><span class="title">Derivatives of a Gaussian Process</span></h1>

<h3 class="date">July 7, 2020</h3>
</div>

<main>

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div class="figure">
<img src="/img/klee_mountains.jpg" alt="" />
<p class="caption">Paul Klee: Joyfal Mountain Landscape (1929)</p>
</div>
<p>A <a href="https://en.wikipedia.org/wiki/Gaussian_process"><em>Gaussian Process</em></a> (GP) is a process for which any finite set of observations follows a multivariate normal distribution. GPs are defined by their mean and a kernel function that gives the covariance between any two observations. They are useful in Bayesian statistics as priors over function spaces.</p>
<p>To denote a GP prior over a set of observations <span class="math inline">\(\bm{y} = \{y_1, y_2, \cdots, y_n\}\)</span> at points <span class="math inline">\(\bm{x} = \{ x_1, x_2, \cdots, x_n\}\)</span>, we write:
<span class="math display">\[
\bm{y} \sim \mathcal{GP}(\bm{\mu}, \bm{\Sigma})
\]</span></p>
<p>where the covariance matrix <span class="math inline">\(\bm{\Sigma}\)</span> is computed via a kernel function <span class="math inline">\(k(x, x^\prime)\)</span>. A popular choice of kernel function that we will consider in this post is the <a href="https://www.cs.toronto.edu/~duvenaud/cookbook"><em>squared exponential kernel</em></a>:
<span class="math display">\[
k(x_i, x_j) = \alpha^2 \exp\left(-
\frac{(x_i - x_j)^2}{2\ell^2} \right)
\]</span></p>
<p>where <span class="math inline">\(\alpha\)</span> is a scale parameter and <span class="math inline">\(\ell\)</span> is a length-scale parameter which controls the strength of the association between points as they become farther apart.</p>
<p>Intuitively, in order to define a GP we need to be able to write down the covariance between any two points.</p>
<p>Let’s write this kernel function in R, and use it to draw samples from a GP. For simplicity, we will fix <span class="math inline">\(\bm{\mu} = \bm{0}\)</span> for all the GPs we work with in this post. First, define the kernel function:</p>
<pre class="r"><code>k &lt;- function(x_i, x_j, alpha, l) {
  alpha^2 * exp(-(x_i - x_j)^2 / (2 * l^2))
}</code></pre>
<p>Let’s write a helper function that, given a kernel and a set of points, generates a full covariance matrix:</p>
<pre class="r"><code>covariance_from_kernel &lt;- function(x, kernel, ...) {
  outer(x, x, kernel, ...)
}</code></pre>
<p>and a function to draw from a mean-zero GP with a given covariance kernel:</p>
<pre class="r"><code>gp_draw &lt;- function(draws, x, Sigma, ...) {
  mu &lt;- rep(0, length(x))
  mvtnorm::rmvnorm(draws, mu, Sigma)
}</code></pre>
<p>Now we can draw 10 samples from a mean-zero GP with a squared-exponential kernel with parameters <span class="math inline">\(\alpha=1\)</span>, <span class="math inline">\(l=1\)</span>:</p>
<pre class="r"><code>n &lt;- 100 # number of points to draw
x &lt;- seq(1, 10, length.out = n) # position of each point

# Kernel parameters
alpha &lt;- 1
l &lt;- 1

set.seed(1)

# Draw 10 samples
Sigma &lt;- covariance_from_kernel(x, k, alpha = alpha, l = l)
y &lt;- gp_draw(10, x, Sigma)

matplot(x, t(y), type = &#39;l&#39;, xlab = &#39;x&#39;, ylab = &#39;y&#39;)</code></pre>
<p><img src="/post/2020-07-06-gaussian-process-derivatives.en_files/figure-html/draws_from_gp-1.png" width="672" /></p>
<div id="derivative-of-a-gp" class="section level2">
<h2>Derivative of a GP</h2>
<p>The derivative of a GP is also a GP, with its existence determined by the differentiability of its mean and kernel functions. The squared exponential kernel is infinitely differentiable, so the associated Gaussian Process has infinitely many derivatives.</p>
<p>This derivative is useful because we may have prior knowledge about likely values of the derivative. For example, monotonicity constraints can be encoded as prior knowledge that the derivative is always positive or negative. In other cases we may have direct observations of the derivative that we would like to incorporate into model fitting.</p>
<p>Let <span class="math inline">\(\bm{y}^\prime = \{ y^\prime_1, y^\prime_2, \cdots, y^\prime_{n^\prime} \}\)</span> be a set of derivative observations. The derivative GP, which we’ll denote <span class="math inline">\(\mathcal{GP}^\prime\)</span>, is given by
<span class="math display">\[
\bm{y}^\prime \sim \mathcal{GP}^\prime(\frac{d}{dx}\bm{\mu}, \frac{d}{dx}\bm{\Sigma})
\]</span>
where the derivative of the covariance matrix is defined by the derivative of the original kernel function with respect to both of its inputs, which we will denote <span class="math inline">\(k_{11}\)</span> to indicate that both arguments are derivative observations:
<span class="math display">\[
  k_{11}(x_i, x_j) = \frac{\partial}{\partial x_i \ \partial x_j} k(x_i, x_j) = \frac{ \alpha^2 }{ \ell^4 }\left( l^2 - (x_i - x_j)^2 \right) \exp\left( -\frac{(x_i - x_j)^2}{2\ell^2} \right)
\]</span></p>
<p>This is enough information to be able to draw random samples from the derivative of a GP. Let’s write the new kernel <span class="math inline">\(k_11\)</span> in R, and sample from the derivative GP:</p>
<pre class="r"><code>k_11 &lt;- function(x_i, x_j, alpha, l) {
  alpha^2 / l^4 * (l^2 - (x_i - x_j)^2) * exp(-(x_i - x_j)^2 / (2*l^2))
}

n_prime &lt;- 100
x_prime &lt;- seq(1, 10, length.out = n_prime)

# Draw 10 samples
Sigma_prime &lt;- covariance_from_kernel(x_prime, k_11, alpha = alpha, l = l)
y_prime &lt;- gp_draw(10, x_prime, Sigma_prime)

matplot(x_prime, t(y_prime), type = &#39;l&#39;, xlab = &#39;x&#39;, ylab = &quot;y&#39;&quot;)</code></pre>
<p><img src="/post/2020-07-06-gaussian-process-derivatives.en_files/figure-html/draws_from_derivative_gp-1.png" width="672" /></p>
<p>It’s hard to interpret this plot because we can’t compare these derivative values to the corresponding normal GP. Fortunately, it’s possible to sample from the joint distributions of the observations and its derivatives, which is in fact also a GP. To define it, we need to know the covariance between an observation and a derivative observation.</p>
<p>Let <span class="math inline">\(k_{01}(x_i, x_j)\)</span> be the covariance between an observation at <span class="math inline">\(x_i\)</span> and a derivative observation at <span class="math inline">\(x_j\)</span>. Then
<span class="math display">\[
k_{01}(x_i, x_j) = \frac{\alpha^2}{\ell^2} (x_i - x_j) \exp\left( -\frac{(x_i - x_j)^2}{2\ell^2} \right)
\]</span></p>
<p>Similarly, <span class="math inline">\(k_{10}(x_i, x_j)\)</span> is the covariance between a derivative observation at <span class="math inline">\(x_i\)</span> and an observation at <span class="math inline">\(x_j\)</span>:
<span class="math display">\[
k_{10}(x_i, x_j) = \frac{\alpha^2}{\ell^2} (x_j - x_i) \exp\left( -\frac{(x_i - x_j)^2}{2\ell^2} \right)
\]</span>
As we would expect from the symmetry of covariance matrices, <span class="math inline">\(k_{01}(x_i, x_j) = k_{10}(x_j, x_i)\)</span>. That means we can get away with just defining one of them in R:</p>
<pre class="r"><code>k_01 &lt;- function(x_i, x_j, alpha, l) {
  alpha^2 / l^2 * (x_i - x_j) * exp(-(x_i - x_j)^2 / (2*l^2))
}</code></pre>
<p>Now construct a combined vector by concatenating the observations <span class="math inline">\(y\)</span> and derivative observations <span class="math inline">\(y^\prime\)</span>, denoted <span class="math inline">\(y^\mathrm{all}\)</span>, of length <span class="math inline">\(n + n^\prime\)</span>. Let <span class="math inline">\(\bm{x}^{\mathrm{all}}\)</span> be the corresponding positions of each observation in <span class="math inline">\(\bm{y}^{\mathrm{all}}\)</span>. Let <span class="math inline">\(\bm{d}^{\mathrm{all}}\)</span> be a <span class="math inline">\(n+n^\prime\)</span> length vector where <span class="math inline">\(d^\mathrm{all}_i = 1\)</span> if <span class="math inline">\(y^{\mathrm{all}}_i\)</span> is a derivative observation, and <span class="math inline">\(d^{\mathrm{all}}_i = 0\)</span> if <span class="math inline">\(y^{\mathrm{all}}_i\)</span> is a normal observation. Then define a new kernel over the joint observations:
<span class="math display">\[
k^{\mathrm{all}}(x_i, x_j, d_i, d_j) = \begin{cases}
  k(x_i, x_j) &amp; d_i = 0, d_j = 0 \text{ (both normal observations)} \\
  k_{01}(x_i, x_j) &amp; d_i = 0, d_j = 0 \text{ (one derivative, one normal)} \\
  k_{10}(x_i, x_j) &amp; d_i = 1, d_j = 0 \text{ (one derivative, one normal)} \\
  k_{11}(x_i, x_j) &amp; d_i = 1, d_j = 0 \text{ (both derivatives)}
\end{cases}
\]</span></p>
<p>In R:</p>
<pre class="r"><code>k_all &lt;- function(x_i, x_j, d_i, d_j, ...) {
  dplyr::case_when(
    d_i == 0 &amp; d_j == 0 ~ k(x_i, x_j, ...),
    d_i == 0 &amp; d_j == 1 ~ k_01(x_i, x_j, ...),
    d_i == 1 &amp; d_j == 0 ~ k_01(x_j, x_i, ...),
    d_i == 1 &amp; d_j == 1 ~ k_11(x_i, x_j, ...),
  )
}</code></pre>
<p>We also need a new function to form the joint covariance matrix:</p>
<pre class="r"><code>joint_covariance_from_kernel &lt;- function(x, d, kernel, ...) {
  outer(1:length(x), 1:length(x),
        function(i, j) kernel(x[i], x[j], d[i], d[j], ...))
}</code></pre>
<p>Finally, I’m going to split out the plotting code into a separate function as it gets more complicated than before:</p>
<pre class="r"><code>plot_joint_gp &lt;- function(x, y, d) {
  plot(x[d == 0], y[d == 0], type = &#39;l&#39;, ylim = range(y), 
       col = &#39;black&#39;, xlab = &#39;x&#39;, ylab = &#39;y&#39;)
  lines(x[d == 1], y[d == 1], type = &#39;l&#39;, col = &#39;blue&#39;, lty = 2)
  abline(h = 0, lty = 3, col = &quot;gray&quot;)
  legend(&#39;topright&#39;, legend = c(&quot;GP&quot;, &quot;Derivative of GP&quot;),
         col = c(&quot;black&quot;, &quot;blue&quot;), lty = 1:2)
}</code></pre>
<p>Now we can sample from the joint distribution of the observations and derivatives:</p>
<pre class="r"><code>x_all &lt;- c(x, x_prime)
d_all &lt;- c(rep(0, length(x)), rep(1, length(x_prime)))

Sigma_all &lt;- joint_covariance_from_kernel(x_all, d_all, k_all, alpha = alpha, l = l)
y_all &lt;- gp_draw(1, x_all, Sigma_all)

plot_joint_gp(x_all, y_all[1,], d_all)</code></pre>
<p><img src="/post/2020-07-06-gaussian-process-derivatives.en_files/figure-html/draw_from_joint_gp-1.png" width="672" /></p>
<p>Let’s plot one more:</p>
<pre class="r"><code>y_all &lt;- gp_draw(1, x_all, Sigma_all)
plot_joint_gp(x_all, y_all[1,], d_all)</code></pre>
<p><img src="/post/2020-07-06-gaussian-process-derivatives.en_files/figure-html/draw_from_joint_gp2-1.png" width="672" /></p>
<p>So far we’ve seen how derivatives of GPs are defined, and how to draw from the joint distribution of a GP and its derivative. In future posts we’ll look at fitting GPs in Stan with derivative observations, and at shape-constrained GPs.</p>
<div id="references" class="section level3">
<h3>References</h3>
<ul>
<li>Andrew McHutchon, <a href="http://mlg.eng.cam.ac.uk/mchutchon/DifferentiatingGPs.pdf">Differentiating Gaussian Processes</a></li>
<li>Rasmussen et al. 2006, <em>Gaussian Processes for Machine Learning</em>, <a href="http://www.gaussianprocess.org/gpml/chapters/RW9.pdf">Chapter 9</a></li>
</ul>
</div>
</div>

</main>

  </div>
  <footer>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous">


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js" integrity="sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY" crossorigin="anonymous"></script>


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>

<script src="//cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.12.0/languages/stan.min.js"></script>

<script>
hljs.configure({languages: []});
hljs.initHighlightingOnLoad();
</script>
<script src="//cdnjs.cloudflare.com/ajax/libs/d3/3.5.17/d3.js"></script>

<script src="/js/graph.js"></script>
  
  <br/><br/>
  <hr/>
  
  </footer>
  </body>
</html>


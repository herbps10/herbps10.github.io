<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Conditioning on Gaussian Process Derivative Observations | Herb Susmann</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <link href="//cdn.bootcss.com/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">


<link rel="stylesheet" href="/css/custom.css">

  </head>

  <body>
    <svg id='graph' width='800' height='800'></svg>
    <nav>
    <a href='/' class='main-title'>Herb Susmann</a>
    <ul class="menu">
      
      <li><a href="/about">About</a></li>
      
      <li><a href="/post">Posts</a></li>
      
      <li><a href="/notebooks">Notebooks</a></li>
      
      <li><a href="/research">Research</a></li>
      
      <li><a href="/software">Software</a></li>
      
    </ul>
    </nav>
    
  <div class='container universal-wrapper'>
<div class="article-meta">
<h1><span class="title">Conditioning on Gaussian Process Derivative Observations</span></h1>

<h3 class="date">December 127, 2020</h3>
</div>

<main>

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div class="figure">
<img src="/img/klee_may.jpg" alt="" />
<p class="caption">Paul Klee: May Picture (1925)</p>
</div>
<p>In the <a href="http://herbsusmann.com/2020/07/06/gaussian-process-derivatives/">first post</a> of this series, we looked at how to draw from the joint distribution of a Gaussian Process and its derivative. In this post, we will show how to condition a Gaussian Process on derivative observations. Overall this is pretty straightforward because the conditional distribution of the multivariate normal has a closed form, although applying it in this context requires some somewhat tedious notation and bookkeeping.</p>
<p>I’m going to take an aside to introduce a slightly different notation than I used in the last post. The new notation looks at Gaussian Processes as priors over a function class. It takes a bit of setup, and we will end up re-writing some of what was already done in the first post, but overall it’s a useful (and more rigorous) way of talking about Gaussian Processes.</p>
<p>Let’s say we we have some observations of a univariate function <span class="math inline">\(f(x) : \mathbb{R} \mapsto \mathbb{R}\)</span>. We wish to use these observations to estimate the function <span class="math inline">\(f\)</span> over its entire domain. Without any prior knowledge, it could be any of an infinity of possible functions. The set of all possible functions that <span class="math inline">\(f\)</span> could be is called a <em>function space</em>. In this example, <span class="math inline">\(f\)</span> is in the space of all functions that map numbers in <span class="math inline">\(\mathbb{R}\)</span> to <span class="math inline">\(\mathbb{R}\)</span>.</p>
<p>We could stop here and say that <span class="math inline">\(f\)</span> could truly be any function. However, in many real world settings we actually do know something about what the function can look like. Broadly, we probably know something about the <em>smoothness</em> of <span class="math inline">\(f\)</span>. In some cases we might also know that it should be periodic. Gaussian Processes provide a flexible way to encode these types of prior knowledge about <span class="math inline">\(f\)</span> as a prior over an infinite dimensional function space.</p>
<p>When thinking about Gaussian Processes as a prior over a function space, the notation is slightly different than in the previous post. We write:
<span class="math display">\[
f \sim \mathcal{GP}(\mu(x), k(x_i, x_j))
\]</span>
where <span class="math inline">\(\mu\)</span> is a function that gives the mean of the GP at any point <span class="math inline">\(x\)</span>, and <span class="math inline">\(k\)</span> is a kernel function that can be used to form a covariance matrix between any points <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_j\)</span>.</p>
<p>Setting this prior places a restriction on the functions <span class="math inline">\(f\)</span>: for any finite set of points <span class="math inline">\(\bm{x}\)</span> in the domain of <span class="math inline">\(f\)</span>, the corresponding function values <span class="math inline">\(f(\bm{x})\)</span> will be multivariate normally distributed:
<span class="math display">\[
f(\bm{x}) \sim MVN\left( \mu(\bm{x}), k(\bm{x}, \bm{x}) \right)
\]</span>
This connects the idea of a Gaussian Process as a <em>prior distribution over a function space</em> to its practical implementation as a multivariate normal distribution over data points.</p>
<p>In practice, using GPs mostly involves thinking about and manipulating multivariate normal distributions. Looking at GPs theoretically as a prior over a function space, however, is a useful way to interpret them, especially when we start getting into using Bayesian inference to fit GPs.</p>
<p>Now, back to the main subject of this post. Suppose we observe <span class="math inline">\(N\)</span> observations <span class="math inline">\(\bm{y} = \{y_1, y_2, \cdots, y_n \}\)</span> at points <span class="math inline">\(\bm{x} = \{x_1, x_2, \cdots, x_n \}\)</span> of a function <span class="math inline">\(f\)</span>. In addition, we have <span class="math inline">\(N^\prime\)</span> observations <span class="math inline">\(\bm{y}^\prime\)</span> at points <span class="math inline">\(\bm{x}^\prime\)</span> of the derivative <span class="math inline">\(f^\prime\)</span> of <span class="math inline">\(f\)</span>.</p>
<p>We wish to use both sets of observations to estimate values of the function <span class="math inline">\(f\)</span> at a new set of points, which we will call <span class="math inline">\(\bm{\tilde{x}}\)</span>.</p>
<p>First, we place a mean-zero Gaussian Process prior with kernel function <span class="math inline">\(k\)</span> on the function <span class="math inline">\(f\)</span>:
<span class="math display">\[
f \sim \mathcal{GP}(0, k_{00}(x_i, x_j))
\]</span>
where the <span class="math inline">\(0\)</span> is a slight abuse of notation indicating that the mean function is always zero, and <span class="math inline">\(k_{00}\)</span> is the kernel function that gives the covariance between function values at <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_j\)</span>.</p>
<p>As we saw in the last post, the derivative of a Gaussian Process is also a Gaussian Process. Setting a GP prior on <span class="math inline">\(f\)</span> therefore implies a GP prior on <span class="math inline">\(f^\prime\)</span>:
<span class="math display">\[
f^\prime \sim \mathcal{GP}(0, k_{11}(x_i, x_j))
\]</span>
where <span class="math inline">\(k_{11}\)</span> is the derivative of the kernel function, giving the covariance between the function derivatives at <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_j\)</span>.</p>
<p>Taken together, this implies that the finite set of realizations of the function and of the function’s derivative that we have observed will have multivariate normal distributions:
<span class="math display">\[
\begin{aligned}
  f(\bm{x})               &amp;\sim MVN(\bm{0}, k_{00}(\bm{x}, \bm{x})) \\
  f^\prime(\bm{x}^\prime) &amp;\sim MVN(\bm{0}, k_{11}(\bm{x}, \bm{x}))
\end{aligned}
\]</span></p>
<p>What’s more, the joint distribution of <span class="math inline">\(f(\bm{x})\)</span> and <span class="math inline">\(f^\prime(\bm{x}^\prime)\)</span> is multivariate normally distributed:
<span class="math display">\[
\begin{bmatrix}f\left(\bm{x}\right)\\
f^{\prime}\left(\bm{x}^{\prime}\right)
\end{bmatrix}\sim MVN\left(\begin{bmatrix}
  \bm{0}\\
  \bm{0}
\end{bmatrix}, \begin{bmatrix}
  k_{00}(\bm{x}, \bm{x}) &amp; k_{01}(\bm{x}, \bm{x}^\prime)\\
  k_{10}(\bm{x}^\prime, \bm{x}) &amp; k_{11}(\bm{x}^\prime, \bm{x}^\prime)
\end{bmatrix}\right)
\]</span>
The functions <span class="math inline">\(k_{01}\)</span> and <span class="math inline">\(k_{10}\)</span> are partial derivatives of the kernel function; see the <a href="http://herbsusmann.com/2020/07/06/gaussian-process-derivatives/">previous post</a> for a full definition.</p>
<p>Now, after all of this notation and setup, we are ready to actually do some prediction. We would like to condition on the observed values <span class="math inline">\(\bm{y}\)</span> (of the function) and <span class="math inline">\(\bm{y}^\prime\)</span> (of the derivative) to predict the values of <span class="math inline">\(f\)</span> at a new set of points <span class="math inline">\(\tilde{\bm{x}}\)</span>.</p>
<p>To make this easier, we’re going to concatenate our observations into one big vector. That is, we make a new vector <span class="math inline">\(\bm{y}^{all}\)</span> that combines <span class="math inline">\(\bm{y}\)</span> and <span class="math inline">\(\bm{y}^\prime\)</span>, <span class="math inline">\(\bm{x}^{all}\)</span> which combines <span class="math inline">\(\bm{x}\)</span> and <span class="math inline">\(\bm{x}^\prime\)</span>, and a vector <span class="math inline">\(\bm{d}^{all}\)</span> which indicates whether each element is a function or derivative observation.</p>
<p>First, let’s generate some observed data. I’m going to take one draw from joint GP and its derivative, using the code from the last post, which we’ll use as the true value of <span class="math inline">\(f\)</span>:</p>
<pre class="r"><code>set.seed(9)

# Set hyperparameters
alpha &lt;- 1
l &lt;- 1

# Points at which to observe the function and its derivative
x &lt;- rep(seq(0, 10, 0.1), 2)
d &lt;- c(rep(0, length(x) / 2), rep(1, length(x) / 2))

# Joint covariance matrix
Sigma &lt;- joint_covariance_from_kernel(x, d, k_all, alpha = alpha, l = l)

# Draw from joint GP
y &lt;- gp_draw(1, x, Sigma)[1, ]</code></pre>
<p>Now let’s choose a few function and derivative values which we’ll use as our observed data:</p>
<pre class="r"><code># Pick a few function and derivative values to use as observed data, making
# sure to pick an equal number of each type
N &lt;- 10
observed_indices &lt;- c(
  sample(which(d == 0), N / 2),
  sample(which(d == 1), N / 2)
)

# We&#39;ll call the observed data y_all so that it matches with the math notation
x_all &lt;- x[observed_indices]
y_all &lt;- y[observed_indices]
d_all &lt;- d[observed_indices]</code></pre>
<p>Let’s plot the observed values, along with the true value of the function:</p>
<pre class="r"><code># Create a data frame for plotting
f &lt;- tibble(
  x = x,
  y = y,
  observed = seq_along(x) %in% observed_indices,
  d = d
)

labeller &lt;- as_labeller(c(&quot;0&quot; = &quot;Function&quot;, &quot;1&quot; = &quot;Derivative&quot;))

ggplot(f, aes(x = x, y = y)) +
  geom_line(aes(lty = &quot;True value&quot;)) +
  geom_point(data = filter(f, observed), aes(color = observed), size = 2) +
  facet_wrap(~d, ncol = 1, labeller = labeller)</code></pre>
<p><img src="/post/2020-12-01-conditioning-on-gaussian-process-derivative-observations.en_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>With the data combined into one long vector, the joint distribution of the observed data <span class="math inline">\(\bm{y}^{all}\)</span> is now given by
<span class="math display">\[
\bm{y}^{all} \sim MVN\left(\bm{0}, k^{all}(\bm{x}^{all}, \bm{x}^{all}, \bm{d}^{all}, \bm{d}^{all}) \right)
\]</span></p>
<p>where <span class="math inline">\(k^{all}\)</span> is a function which computes the covariance between points, choosing the right covariance formula to use depending on if a point is a derivative or function value:
<span class="math display">\[
k^{\mathrm{all}}(x_i, x_j, d_i, d_j) = \begin{cases}
  k(x_i, x_j) &amp; d_i = 0, d_j = 0 \text{ (both normal observations)} \\
  k_{01}(x_i, x_j) &amp; d_i = 0, d_j = 0 \text{ (one derivative, one normal)} \\
  k_{10}(x_i, x_j) &amp; d_i = 1, d_j = 0 \text{ (one derivative, one normal)} \\
  k_{11}(x_i, x_j) &amp; d_i = 1, d_j = 0 \text{ (both derivatives)}
\end{cases}
\]</span></p>
<p>We want to predict both the function values and derivatives at a set of new points <span class="math inline">\(\tilde{\bm{x}}\)</span>. Define <span class="math inline">\(\tilde{\bm{x}}^{all}\)</span> to be a vector formed by repeating <span class="math inline">\(\tilde{\bm{x}}\)</span> twice, with <span class="math inline">\(\tilde{\bm{d}}^{all} = \left\{0, 0, \cdots, 0, 1, 1, \cdots, 1\right\}\)</span> indicating whether each element of <span class="math inline">\(\tilde{\bm{x}}\)</span> refers to a function or derivative prediction.</p>
<pre class="r"><code>x_tilde &lt;- seq(0, 10, 0.1) # Prediction points

x_tilde_all &lt;- c(x_tilde, x_tilde)
d_tilde_all &lt;- c(rep(0, length(x_tilde)), rep(1, length(x_tilde)))</code></pre>
<p>Fortunately, the conditional distribution of a multivariate normal distribution has <a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Conditional_distributions">a closed form</a>. Applying the formula, we arrive at
<span class="math display">\[
\tilde{\bm{y}} \mid \tilde{\bm{x}}^{all}, \bm{y}^{all}, \bm{x}^{all} \sim MVN(\bm{K}^\top \bm{\Sigma}^{-1} \bm{y}^{all}, \bm{\Omega} - \bm{K}^\top \bm{\Sigma}^{-1} \bm{K})
\]</span>
where</p>
<ul>
<li><span class="math inline">\(\bm{\Sigma}\)</span> is the covariance matrix between observed points <span class="math inline">\(\bm{x}^{all}\)</span></li>
<li><span class="math inline">\(\bm{\Omega}\)</span> is the covariance matrix between the new points <span class="math inline">\(\bm{\tilde{x}}\)</span></li>
<li><span class="math inline">\(\bm{K}\)</span> is the covariance matrix between the observed points <span class="math inline">\(\bm{x}^{all}\)</span> and new points <span class="math inline">\(\bm{\tilde{x}}\)</span></li>
</ul>
<p>First, let’s compute each one of these matrices:</p>
<pre class="r"><code># Covariance between the observed points
Sigma &lt;- joint_covariance_from_kernel(x_all, d_all, k_all, alpha = alpha, l = l)

# Due to computational floating point issues, it is sometimes necessary to
# add a small constant to the diagonal of Sigma to make sure it&#39;s not singular
Sigma &lt;- Sigma + diag(1e-4, nrow(Sigma))

# Covariance between the prediction points
Omega &lt;- joint_covariance_from_kernel(x_tilde_all, d_tilde_all, k_all, 
                                      alpha = alpha, l = l)

# Covariance between the observed and prediction points
# We calculate K by computing the covariance between x_all and x_tilde_all
K &lt;- outer(
  1:length(x_all),
  1:length(x_tilde_all),
  function(i, j) {
    k_all(x_all[i], x_tilde_all[j], d_all[i], d_tilde_all[j], alpha = alpha, l = l)
  }
)</code></pre>
<p>Then apply the conditioning formula to get the conditional mean and covariance matrix:</p>
<pre class="r"><code>mu_conditional &lt;- t(K) %*% solve(Sigma) %*% y_all
Sigma_conditional &lt;- Omega - t(K) %*% solve(Sigma) %*% K</code></pre>
<p>For plotting, it’s helpful to extract the marginal variances of each prediction, which are the diagonal entries of the conditional covariance matrix. We can also use this to get a 95% prediction interval:</p>
<pre class="r"><code>variance_conditional &lt;- diag(Sigma_conditional)

# Floating point errors sometimes causes the variances to be very close to zero, 
# but negative. If this happens, just set the variance to zero:
variance_conditional &lt;- ifelse(variance_conditional &lt; 0, 0, variance_conditional)

# 95% intervals
lower_bound &lt;- mu_conditional - 1.96 * sqrt(variance_conditional)
upper_bound &lt;- mu_conditional + 1.96 * sqrt(variance_conditional)</code></pre>
<p>Let’s plot the result:</p>
<pre class="r"><code>f_conditional &lt;- tibble(
  x = x_tilde_all,
  d = d_tilde_all,
  y = mu_conditional,
  lower = lower_bound,
  upper = upper_bound
) 

ggplot(f_conditional, aes(x = x, y = y)) +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2) +
  geom_line(aes(lty = &quot;Conditional mean&quot;)) +
  geom_line(aes(lty = &quot;True value&quot;), data = f) +
  geom_point(aes(color = observed), data = filter(f, observed)) +
  scale_linetype_manual(values = c(2, 1)) +
  facet_wrap(~d, ncol = 1, labeller = labeller)</code></pre>
<p><img src="/post/2020-12-01-conditioning-on-gaussian-process-derivative-observations.en_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>The inclusion of derivative information between <span class="math inline">\(x=2.5\)</span> and <span class="math inline">\(x=5.0\)</span> appears to have a big effect on the predictions. Even though there are no function values in this region, the derivatives are able to guide the predictions.</p>
<p>It would be nice to be able to compare these predictions to ones that don’t use any derivative information. To make this easier, let’s wrap up our code into a helper function which will handle the conditional multivariate normal calculations:</p>
<pre class="r"><code># To make this more readable I&#39;m omitting the &quot;_all&quot; suffixes on variable names
condition_joint_gp &lt;- function(x_tilde, d_tilde, y, x, d, kernel, ...) {
  Sigma &lt;- joint_covariance_from_kernel(x, d, kernel, ...) +
    diag(1e-4, length(x))
  
  Omega &lt;- joint_covariance_from_kernel(x_tilde, d_tilde, kernel, ...)
  
  K &lt;- outer(1:length(x), 1:length(x_tilde),
          function(i, j) kernel(x[i], x_tilde[j], d[i], d_tilde[j], ...))
  
  mu_conditional &lt;- (t(K) %*% solve(Sigma) %*% y)[, 1]
  Sigma_conditional &lt;- Omega - t(K) %*% solve(Sigma) %*% K
  
  var_conditional &lt;- diag(Sigma_conditional)
  var_conditional &lt;- ifelse(var_conditional &lt; 0, 0, var_conditional)
  
  tibble(
    x     = x_tilde,
    d     = d_tilde,
    y     = mu_conditional,
    var   = var_conditional,
    lower = y - 1.96 * sqrt(var_conditional),
    upper = y + 1.96 * sqrt(var_conditional)
  )
}</code></pre>
<p>And let’s also wrap up the plotting code into a function:</p>
<pre class="r"><code>plot_conditional &lt;- function(f_conditional, f) {
  ggplot(f_conditional, aes(x = x, y = y)) +
    geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2) +
    geom_line(aes(lty = &quot;Conditional mean&quot;)) +
    geom_line(aes(lty = &quot;True value&quot;), data = f) +
    geom_point(aes(color = observed), data = filter(f, observed)) +
    scale_linetype_manual(values = c(2, 1)) +
    facet_wrap(~d, ncol = 1, labeller = labeller)
}</code></pre>
<p>Now let’s condition on just the function observations, leaving out the derivative points, and plotting the results.</p>
<pre class="r"><code>non_derivative_indices &lt;- which(d_all == 0)

f_conditional_no_derivatives &lt;- condition_joint_gp(
  x_tilde_all,                   # Prediction points
  d_tilde_all,                   # Derivative indicator at prediction points
  y_all[non_derivative_indices], # Observed function values
  x_all[non_derivative_indices], # Position of observed values
  d_all[non_derivative_indices], # Derivative indicator of observed values
  k_all,
  alpha = alpha,
  l = l
)


# First, plot the predictions that don&#39;t use derivative information
plot_conditional(
  f_conditional_no_derivatives,
  mutate(f, observed = ifelse(d == 1, FALSE, observed))
) +
  ggtitle(&quot;Predictions without derivative observations&quot;)</code></pre>
<p><img src="/post/2020-12-01-conditioning-on-gaussian-process-derivative-observations.en_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<pre class="r"><code># Then plot again the predictions that use the derivatives
plot_conditional(f_conditional, f) +
  ggtitle(&quot;Predictions using derivative observations&quot;)</code></pre>
<p><img src="/post/2020-12-01-conditioning-on-gaussian-process-derivative-observations.en_files/figure-html/unnamed-chunk-11-2.png" width="672" /></p>
<p>As we would expect, including more information by providing some derivative observations leads to much better predictions.</p>
<p>Up until now, we have been fixing the hyperparmaters of the Gaussian Process to known values. However, in the real world it’s rare to know these parameters; we need to estimate them. In a future post I’ll show how to use Stan to estimate the hyperparameter values using full Bayesian inference.</p>
<div id="references" class="section level3">
<h3>References</h3>
<ul>
<li>Stan Reference Manual, <a href="https://mc-stan.org/docs/2_19/stan-users-guide/fit-gp-section.html">Fitting a Gaussian Process</a></li>
</ul>
</div>

</main>

  </div>
  <footer>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous">


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js" integrity="sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY" crossorigin="anonymous"></script>


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>

<script src="//cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.12.0/languages/stan.min.js"></script>

<script>
hljs.configure({languages: []});
hljs.initHighlightingOnLoad();
</script>
<script src="//cdnjs.cloudflare.com/ajax/libs/d3/3.5.17/d3.js"></script>

<script src="/js/graph.js"></script>
  
  <br/><br/>
  <hr/>
  
  </footer>
  </body>
</html>


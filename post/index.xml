<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Herb Susmann</title>
    <link>/post/</link>
      <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 01 Dec 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/icon-192.png</url>
      <title>Posts</title>
      <link>/post/</link>
    </image>
    
    <item>
      <title>Conditioning on Gaussian Process Derivative Observations</title>
      <link>/2020/12/01/conditioning-on-gaussian-process-derivative-observations/</link>
      <pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/2020/12/01/conditioning-on-gaussian-process-derivative-observations/</guid>
      <description>


&lt;p&gt;In the &lt;a href=&#34;http://herbsusmann.com/2020/07/06/gaussian-process-derivatives/&#34;&gt;first post&lt;/a&gt; of this series, we looked at how to draw from the joint distribution of a Gaussian Process and its derivative. In this post, we will show how to condition a Gaussian Process on derivative observations. Overall this is pretty straightforward because the conditional distribution of the multivariate normal has a closed form, although applying it in this context requires some somewhat tedious notation and bookkeeping.&lt;/p&gt;
&lt;p&gt;I’m going to take an aside to introduce a slightly different notation than I used in the last post. The new notation looks at Gaussian Processes as priors over a function class. It takes a bit of setup, and we will end up re-writing some of what was already done in the first post, but overall it’s a useful (and more rigorous) way of talking about Gaussian Processes.&lt;/p&gt;
&lt;p&gt;Let’s say we we have some observations of a univariate function &lt;span class=&#34;math inline&#34;&gt;\(f(x) : \mathbb{R} \mapsto \mathbb{R}\)&lt;/span&gt;. We wish to use these observations to estimate the function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; over its entire domain. Without any prior knowledge, it could be any of an infinity of possible functions. The set of all possible functions that &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; could be is called a &lt;em&gt;function space&lt;/em&gt;. In this example, &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is in the space of all functions that map numbers in &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We could stop here and say that &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; could truly be any function. However, in many real world settings we actually do know something about what the function can look like. Broadly, we probably know something about the &lt;em&gt;smoothness&lt;/em&gt; of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;. In some cases we might also know that it should be periodic. Gaussian Processes provide a flexible way to encode these types of prior knowledge about &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; as a prior over an infinite dimensional function space.&lt;/p&gt;
&lt;p&gt;When thinking about Gaussian Processes as a prior over a function space, the notation is slightly different than in the previous post. We write:
&lt;span class=&#34;math display&#34;&gt;\[
f \sim \mathcal{GP}(\mu(x), k(x_i, x_j))
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is a function that gives the mean of the GP at any point &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; is a kernel function that can be used to form a covariance matrix between any points &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_j\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Setting this prior places a restriction on the functions &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;: for any finite set of points &lt;span class=&#34;math inline&#34;&gt;\(\bm{x}\)&lt;/span&gt; in the domain of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;, the corresponding function values &lt;span class=&#34;math inline&#34;&gt;\(f(\bm{x})\)&lt;/span&gt; will be multivariate normally distributed:
&lt;span class=&#34;math display&#34;&gt;\[
f(\bm{x}) \sim MVN\left( \mu(\bm{x}), k(\bm{x}, \bm{x}) \right)
\]&lt;/span&gt;
This connects the idea of a Gaussian Process as a &lt;em&gt;prior distribution over a function space&lt;/em&gt; to its practical implementation as a multivariate normal distribution over data points.&lt;/p&gt;
&lt;p&gt;In practice, using GPs mostly involves thinking about and manipulating multivariate normal distributions. Looking at GPs theoretically as a prior over a function space, however, is a useful way to interpret them, especially when we start getting into using Bayesian inference to fit GPs.&lt;/p&gt;
&lt;p&gt;Now, back to the main subject of this post. Suppose we observe &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; observations &lt;span class=&#34;math inline&#34;&gt;\(\bm{y} = \{y_1, y_2, \cdots, y_n \}\)&lt;/span&gt; at points &lt;span class=&#34;math inline&#34;&gt;\(\bm{x} = \{x_1, x_2, \cdots, x_n \}\)&lt;/span&gt; of a function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;. In addition, we have &lt;span class=&#34;math inline&#34;&gt;\(N^\prime\)&lt;/span&gt; observations &lt;span class=&#34;math inline&#34;&gt;\(\bm{y}^\prime\)&lt;/span&gt; at points &lt;span class=&#34;math inline&#34;&gt;\(\bm{x}^\prime\)&lt;/span&gt; of the derivative &lt;span class=&#34;math inline&#34;&gt;\(f^\prime\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We wish to use both sets of observations to estimate values of the function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; at a new set of points, which we will call &lt;span class=&#34;math inline&#34;&gt;\(\bm{\tilde{x}}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;First, we place a mean-zero Gaussian Process prior with kernel function &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; on the function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[
f \sim \mathcal{GP}(0, k_{00}(x_i, x_j))
\]&lt;/span&gt;
where the &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; is a slight abuse of notation indicating that the mean function is always zero, and &lt;span class=&#34;math inline&#34;&gt;\(k_{00}\)&lt;/span&gt; is the kernel function that gives the covariance between function values at &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_j\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;As we saw in the last post, the derivative of a Gaussian Process is also a Gaussian Process. Setting a GP prior on &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; therefore implies a GP prior on &lt;span class=&#34;math inline&#34;&gt;\(f^\prime\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[
f^\prime \sim \mathcal{GP}(0, k_{11}(x_i, x_j))
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(k_{11}\)&lt;/span&gt; is the derivative of the kernel function, giving the covariance between the function derivatives at &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_j\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Taken together, this implies that the finite set of realizations of the function and of the function’s derivative that we have observed will have multivariate normal distributions:
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
  f(\bm{x})               &amp;amp;\sim MVN(\bm{0}, k_{00}(\bm{x}, \bm{x})) \\
  f^\prime(\bm{x}^\prime) &amp;amp;\sim MVN(\bm{0}, k_{11}(\bm{x}, \bm{x}))
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;What’s more, the joint distribution of &lt;span class=&#34;math inline&#34;&gt;\(f(\bm{x})\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(f^\prime(\bm{x}^\prime)\)&lt;/span&gt; is multivariate normally distributed:
&lt;span class=&#34;math display&#34;&gt;\[
\begin{bmatrix}f\left(\bm{x}\right)\\
f^{\prime}\left(\bm{x}^{\prime}\right)
\end{bmatrix}\sim MVN\left(\begin{bmatrix}
  \bm{0}\\
  \bm{0}
\end{bmatrix}, \begin{bmatrix}
  k_{00}(\bm{x}, \bm{x}) &amp;amp; k_{01}(\bm{x}, \bm{x}^\prime)\\
  k_{10}(\bm{x}^\prime, \bm{x}) &amp;amp; k_{11}(\bm{x}^\prime, \bm{x}^\prime)
\end{bmatrix}\right)
\]&lt;/span&gt;
The functions &lt;span class=&#34;math inline&#34;&gt;\(k_{01}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(k_{10}\)&lt;/span&gt; are partial derivatives of the kernel function; see the &lt;a href=&#34;http://herbsusmann.com/2020/07/06/gaussian-process-derivatives/&#34;&gt;previous post&lt;/a&gt; for a full definition.&lt;/p&gt;
&lt;p&gt;Now, after all of this notation and setup, we are ready to actually do some prediction. We would like to condition on the observed values &lt;span class=&#34;math inline&#34;&gt;\(\bm{y}\)&lt;/span&gt; (of the function) and &lt;span class=&#34;math inline&#34;&gt;\(\bm{y}^\prime\)&lt;/span&gt; (of the derivative) to predict the values of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; at a new set of points &lt;span class=&#34;math inline&#34;&gt;\(\tilde{\bm{x}}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;To make this easier, we’re going to concatenate our observations into one big vector. That is, we make a new vector &lt;span class=&#34;math inline&#34;&gt;\(\bm{y}^{all}\)&lt;/span&gt; that combines &lt;span class=&#34;math inline&#34;&gt;\(\bm{y}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\bm{y}^\prime\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\bm{x}^{all}\)&lt;/span&gt; which combines &lt;span class=&#34;math inline&#34;&gt;\(\bm{x}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\bm{x}^\prime\)&lt;/span&gt;, and a vector &lt;span class=&#34;math inline&#34;&gt;\(\bm{d}^{all}\)&lt;/span&gt; which indicates whether each element is a function or derivative observation.&lt;/p&gt;
&lt;p&gt;First, let’s generate some observed data. I’m going to take one draw from joint GP and its derivative, using the code from the last post, which we’ll use as the true value of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(9)

# Set hyperparameters
alpha &amp;lt;- 1
l &amp;lt;- 1

# Points at which to observe the function and its derivative
x &amp;lt;- rep(seq(0, 10, 0.1), 2)
d &amp;lt;- c(rep(0, length(x) / 2), rep(1, length(x) / 2))

# Joint covariance matrix
Sigma &amp;lt;- joint_covariance_from_kernel(x, d, k_all, alpha = alpha, l = l)

# Draw from joint GP
y &amp;lt;- gp_draw(1, x, Sigma)[1, ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s choose a few function and derivative values which we’ll use as our observed data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Pick a few function and derivative values to use as observed data, making
# sure to pick an equal number of each type
N &amp;lt;- 10
observed_indices &amp;lt;- c(
  sample(which(d == 0), N / 2),
  sample(which(d == 1), N / 2)
)

# We&amp;#39;ll call the observed data y_all so that it matches with the math notation
x_all &amp;lt;- x[observed_indices]
y_all &amp;lt;- y[observed_indices]
d_all &amp;lt;- d[observed_indices]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s plot the observed values, along with the true value of the function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create a data frame for plotting
f &amp;lt;- tibble(
  x = x,
  y = y,
  observed = seq_along(x) %in% observed_indices,
  d = d
)

ggplot(f, aes(x = x, y = y)) +
  geom_line(aes(lty = &amp;quot;True value&amp;quot;)) +
  geom_point(data = filter(f, observed), aes(color = observed), size = 2) +
  facet_wrap(~d, ncol = 1, labeller = as_labeller(c(&amp;quot;0&amp;quot; = &amp;quot;Function&amp;quot;, &amp;quot;1&amp;quot; = &amp;quot;Derivative&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-01-conditioning-on-gaussian-process-derivative-observations.en_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;With the data combined into one long vector, the joint distribution of the observed data &lt;span class=&#34;math inline&#34;&gt;\(\bm{y}^{all}\)&lt;/span&gt; is now given by
&lt;span class=&#34;math display&#34;&gt;\[
\bm{y}^{all} \sim MVN\left(\bm{0}, k^{all}(\bm{x}^{all}, \bm{x}^{all}, \bm{d}^{all}, \bm{d}^{all}) \right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(k^{all}\)&lt;/span&gt; is a function which computes the covariance between points, choosing the right covariance formula to use depending on if a point is a derivative or function value:
&lt;span class=&#34;math display&#34;&gt;\[
k^{\mathrm{all}}(x_i, x_j, d_i, d_j) = \begin{cases}
  k(x_i, x_j) &amp;amp; d_i = 0, d_j = 0 \text{ (both normal observations)} \\
  k_{01}(x_i, x_j) &amp;amp; d_i = 0, d_j = 0 \text{ (one derivative, one normal)} \\
  k_{10}(x_i, x_j) &amp;amp; d_i = 1, d_j = 0 \text{ (one derivative, one normal)} \\
  k_{11}(x_i, x_j) &amp;amp; d_i = 1, d_j = 0 \text{ (both derivatives)}
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We want to predict both the function values and derivatives at a set of new points &lt;span class=&#34;math inline&#34;&gt;\(\tilde{\bm{x}}\)&lt;/span&gt;. Define &lt;span class=&#34;math inline&#34;&gt;\(\tilde{\bm{x}}^{all}\)&lt;/span&gt; to be a vector formed by repeating &lt;span class=&#34;math inline&#34;&gt;\(\tilde{\bm{x}}\)&lt;/span&gt; twice, with &lt;span class=&#34;math inline&#34;&gt;\(\tilde{\bm{d}}^{all} = \left\{0, 0, \cdots, 0, 1, 1, \cdots, 1\right\}\)&lt;/span&gt; indicating whether each element of &lt;span class=&#34;math inline&#34;&gt;\(\tilde{\bm{x}}\)&lt;/span&gt; refers to a function or derivative prediction.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x_tilde &amp;lt;- seq(0, 10, 0.1) # Prediction points

x_tilde_all &amp;lt;- c(x_tilde, x_tilde)
d_tilde_all &amp;lt;- c(rep(0, length(x_tilde)), rep(1, length(x_tilde)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Fortunately, the conditional distribution of a multivariate normal distribution has &lt;a href=&#34;https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Conditional_distributions&#34;&gt;a closed form&lt;/a&gt;. Applying the formula, we arrive at
&lt;span class=&#34;math display&#34;&gt;\[
\tilde{\bm{y}} \mid \tilde{\bm{x}}^{all}, \bm{y}^{all}, \bm{x}^{all} \sim MVN(\bm{K}^\top \bm{\Sigma}^{-1} \bm{y}^{all}, \bm{\Omega} - \bm{K}^\top \bm{\Sigma}^{-1} \bm{K})
\]&lt;/span&gt;
where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\bm{\Sigma}\)&lt;/span&gt; is the covariance matrix between observed points &lt;span class=&#34;math inline&#34;&gt;\(\bm{x}^{all}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\bm{\Omega}\)&lt;/span&gt; is the covariance matrix between the new points &lt;span class=&#34;math inline&#34;&gt;\(\bm{\tilde{x}}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\bm{K}\)&lt;/span&gt; is the covariance matrix between the observed points &lt;span class=&#34;math inline&#34;&gt;\(\bm{x}^{all}\)&lt;/span&gt; and new points &lt;span class=&#34;math inline&#34;&gt;\(\bm{\tilde{x}}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First, let’s compute each one of these matrices:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Covariance between the observed points
Sigma &amp;lt;- joint_covariance_from_kernel(x_all, d_all, k_all, alpha = alpha, l = l)

# Due to computational floating point issues, it is sometimes necessary to
# add a small constant to the diagonal of Sigma to make sure it&amp;#39;s not singular
Sigma &amp;lt;- Sigma + diag(1e-4, nrow(Sigma))

# Covariance between the prediction points
Omega &amp;lt;- joint_covariance_from_kernel(x_tilde_all, d_tilde_all, k_all, alpha = alpha, l = l)

# Covariance between the observed and prediction points
# We calculate K by computing the covariance between x_all and x_tilde_all
K &amp;lt;- outer(1:length(x_all), 1:length(x_tilde_all),
        function(i, j) k_all(x_all[i], x_tilde_all[j], d_all[i], d_tilde_all[j], alpha = alpha, l = l))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then apply the conditioning formula to get the conditional mean and covariance matrix:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mu_conditional &amp;lt;- t(K) %*% solve(Sigma) %*% y_all
Sigma_conditional &amp;lt;- Omega - t(K) %*% solve(Sigma) %*% K&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For plotting, it’s helpful to extract the marginal variances of each prediction, which are the diagonal entries of the conditional covariance matrix. We can also use this to get a 95% prediction interval:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;variance_conditional &amp;lt;- diag(Sigma_conditional)

# Floating point errors sometimes causes the variances to be very close to zero, but negative.
# if this happens, just set the variance to zero:
variance_conditional &amp;lt;- ifelse(variance_conditional &amp;lt; 0, 0, variance_conditional)

# 95% intervals
lower_bound &amp;lt;- mu_conditional - 1.96 * sqrt(variance_conditional)
upper_bound &amp;lt;- mu_conditional + 1.96 * sqrt(variance_conditional)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s plot the result:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;f_conditional &amp;lt;- tibble(
  x = x_tilde_all,
  d = d_tilde_all,
  y = mu_conditional,
  lower = lower_bound,
  upper = upper_bound
) 

ggplot(f_conditional, aes(x = x, y = y)) +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2) +
  geom_line(aes(lty = &amp;quot;Conditional mean&amp;quot;)) +
  geom_line(aes(lty = &amp;quot;True value&amp;quot;), data = f) +
  geom_point(aes(color = observed), data = filter(f, observed)) +
  scale_linetype_manual(values = c(2, 1)) +
  facet_wrap(~d, ncol = 1, labeller = as_labeller(c(&amp;quot;0&amp;quot; = &amp;quot;Function&amp;quot;, &amp;quot;1&amp;quot; = &amp;quot;Derivative&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-01-conditioning-on-gaussian-process-derivative-observations.en_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The inclusion of derivative information between &lt;span class=&#34;math inline&#34;&gt;\(x=2.5\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x=5.0\)&lt;/span&gt; appears to have a big effect on the predictions. Even though there are no function values in this region, the derivatives are able to guide the predictions.&lt;/p&gt;
&lt;p&gt;It would be nice to be able to compare these predictions to ones that don’t use any derivative information. To make this easier, let’s wrap up our code into a helper function which will handle the conditional multivariate normal calculations:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# To make this more readable I&amp;#39;m omitting the &amp;quot;_all&amp;quot; suffixes on variable names
condition_joint_gp &amp;lt;- function(x_tilde, d_tilde, y, x, d, kernel, ...) {
  Sigma &amp;lt;- joint_covariance_from_kernel(x, d, kernel, ...) +
    diag(1e-4, length(x))
  
  Omega &amp;lt;- joint_covariance_from_kernel(x_tilde, d_tilde, kernel, ...)
  
  K &amp;lt;- outer(1:length(x), 1:length(x_tilde),
          function(i, j) kernel(x[i], x_tilde[j], d[i], d_tilde[j], ...))
  
  mu_conditional &amp;lt;- (t(K) %*% solve(Sigma) %*% y)[, 1]
  Sigma_conditional &amp;lt;- Omega - t(K) %*% solve(Sigma) %*% K
  
  var_conditional &amp;lt;- diag(Sigma_conditional)
  var_conditional &amp;lt;- ifelse(var_conditional &amp;lt; 0, 0, var_conditional)
  
  tibble(
    x     = x_tilde,
    d     = d_tilde,
    y     = mu_conditional,
    var   = var_conditional,
    lower = y - 1.96 * sqrt(var_conditional),
    upper = y + 1.96 * sqrt(var_conditional)
  )
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And let’s also wrap up the plotting code into a function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_conditional &amp;lt;- function(f_conditional, f) {
  ggplot(f_conditional, aes(x = x, y = y)) +
    geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2) +
    geom_line(aes(lty = &amp;quot;Conditional mean&amp;quot;)) +
    geom_line(aes(lty = &amp;quot;True value&amp;quot;), data = f) +
    geom_point(aes(color = observed), data = filter(f, observed)) +
    scale_linetype_manual(values = c(2, 1)) +
    facet_wrap(~d, ncol = 1, labeller = as_labeller(c(&amp;quot;0&amp;quot; = &amp;quot;Function&amp;quot;, &amp;quot;1&amp;quot; = &amp;quot;Derivative&amp;quot;)))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s condition on just the function observations, leaving out the derivative points, and plotting the results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;non_derivative_indices &amp;lt;- which(d_all == 0)

f_conditional_no_derivatives &amp;lt;- condition_joint_gp(
  x_tilde_all,                   # Prediction points
  d_tilde_all,                   # Derivative indicator at prediction points
  y_all[non_derivative_indices], # Observed function values
  x_all[non_derivative_indices], # Position of observed values
  d_all[non_derivative_indices], # Derivative indicator of observed values
  k_all,
  alpha = alpha,
  l = l
)


# First, plot the predictions that don&amp;#39;t use derivative information
plot_conditional(f_conditional_no_derivatives, mutate(f, observed = ifelse(d == 1, FALSE, observed))) +
  ggtitle(&amp;quot;Predictions without derivative observations&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-01-conditioning-on-gaussian-process-derivative-observations.en_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Then plot again the predictions that use the derivatives
plot_conditional(f_conditional, f) +
  ggtitle(&amp;quot;Predictions using derivative observations&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-01-conditioning-on-gaussian-process-derivative-observations.en_files/figure-html/unnamed-chunk-11-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As we would expect, including more information by providing some derivative observations leads to much better predictions.&lt;/p&gt;
&lt;p&gt;Up until now, we have been fixing the hyperparmaters of the Gaussian Process to known values. However, in the real world it’s rare to know these parameters; we need to estimate them. In a future post I’ll show how to use Stan to estimate the hyperparameter values using full Bayesian inference.&lt;/p&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Stan Reference Manual, &lt;a href=&#34;https://mc-stan.org/docs/2_19/stan-users-guide/fit-gp-section.html&#34;&gt;Fitting a Gaussian Process&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>How Many Words Do You Need to Know to Watch Friends?</title>
      <link>/2020/08/04/friends/</link>
      <pubDate>Tue, 04 Aug 2020 00:00:00 +0000</pubDate>
      <guid>/2020/08/04/friends/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/core-js/shim.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/react/react.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/react/react-dom.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/reactwidget/react-tools.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/reactable-binding/reactable.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;A few years ago, The New York Times &lt;a href=&#34;https://www.nytimes.com/2017/09/18/sports/baseball/friends-tv-show-baseball-spanish.html?mcubz=1&#34;&gt;published an article&lt;/a&gt; about several Major League Baseball players who use the sitcom &lt;em&gt;Friends&lt;/em&gt; to improve their English. Friends seems to be a very popular tool for learning English: there’s even an &lt;a href=&#34;https://fluentwithfriends.com/&#34;&gt;ESL program&lt;/a&gt; developed around it, and you can easily find &lt;a href=&#34;https://ejoy-english.com/blog/learn-english-friends/&#34;&gt;advice&lt;/a&gt; on &lt;a href=&#34;https://www.fluentu.com/blog/educator-english/teaching-english-with-friends-tv-series/&#34;&gt;how to use&lt;/a&gt; Friends as a language learning tool.&lt;/p&gt;
&lt;p&gt;In my own language learning I’ve relied heavily on frequency dictionaries, which order words by their popularity. Learning just the top 500-1,000 words in language goes surprisingly far in conversation.&lt;/p&gt;
&lt;p&gt;I wanted to know how many English words someone would need to recognize to be able to watch Friends and understand, say, 90% of all the words.&lt;/p&gt;
&lt;p&gt;Here the definition of “word” becomes important. For the purposes of this article, I’m going to consider most grammatical inflections of a root to be the same word. For example, “go”, “going”, and “went” will be considered one word, since they all relate back to the root word “to go”. Later on we’ll see how to operationalize this via lemmatization algorithms, which will allow us to map words back to their grammatical roots.&lt;/p&gt;
&lt;p&gt;Fortunately, complete transcripts of Friends are available online. Below I go through the data collection and analysis. But first, here’s the final result:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-08-04-friends/index.en_files/figure-html/plot-1.png&#34; width=&#34;672&#34; alt=&#39;You need to know 750 words to know 90% of all the words used in Friends&#39;&gt;&lt;/p&gt;
&lt;p&gt;Around 750 words gets you to 90% coverage. Watching Friends where you don’t know every tenth word could be a frustrating experience, but hopefully context clues would help fill in gaps. The Appendix at the bottom of the post lists the top 750 words.&lt;/p&gt;
&lt;p&gt;The numbers in this post should be considered as rough estimates because I did not filter out other words that we may not want to consider as words, like proper nouns (“Joey”, “Monica”, “New York”, etc.)&lt;/p&gt;
&lt;div id=&#34;data-collection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Collection&lt;/h2&gt;
&lt;p&gt;The script of every episode of Friends is available at &lt;a href=&#34;https://fangj.github.io/friends/&#34;&gt;https://fangj.github.io/friends/&lt;/a&gt;. I used rvest to download every file, and parsed the scripts into individual lines. I only wanted to have to do this once, so I saved the results to a CSV file so that subsequent analyses wouldn’t have to redownload all of the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(rvest)

pages &amp;lt;- read_html(&amp;quot;https://fangj.github.io/friends/&amp;quot;) %&amp;gt;%
  html_nodes(&amp;quot;a&amp;quot;) %&amp;gt;%
  html_attr(&amp;quot;href&amp;quot;)

pages &amp;lt;- str_c(&amp;quot;https://fangj.github.io/friends/&amp;quot;, pages)

parse_episode &amp;lt;- function(url) {
  read_html(url) %&amp;gt;%
    html_nodes(&amp;quot;p:not([align=center])&amp;quot;) %&amp;gt;%
    .[-1] %&amp;gt;%
    html_text() %&amp;gt;%
    enframe(name = NULL, value = &amp;quot;line&amp;quot;) %&amp;gt;%
    mutate(
      url = url,
      episode = str_extract(url, &amp;quot;\\d{4}&amp;quot;), # extract episode number
      character = str_extract(line, &amp;quot;^(\\w+):&amp;quot;), # extract character name
      character = str_replace_all(character, &amp;quot;:$&amp;quot;, &amp;quot;&amp;quot;),
      line = str_replace_all(line, &amp;quot;^\\w+: &amp;quot;, &amp;quot;&amp;quot;),
    ) %&amp;gt;%
    filter(line != &amp;quot;&amp;quot;)
}

friends_raw &amp;lt;- pages %&amp;gt;% map(parse_episode) %&amp;gt;%
  bind_rows()

write_csv(friends_raw, &amp;quot;./friends.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This yields a raw dataset with the lines from every episode:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(friends_raw, n = 10) %&amp;gt;%
  select(episode, character, line) %&amp;gt;%
  gt::gt()&lt;/code&gt;&lt;/pre&gt;
&lt;style&gt;html {
  font-family: -apple-system, BlinkMacSystemFont, &#39;Segoe UI&#39;, Roboto, Oxygen, Ubuntu, Cantarell, &#39;Helvetica Neue&#39;, &#39;Fira Sans&#39;, &#39;Droid Sans&#39;, Arial, sans-serif;
}

#nesysomsrl .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#nesysomsrl .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#nesysomsrl .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#nesysomsrl .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 4px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#nesysomsrl .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#nesysomsrl .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#nesysomsrl .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#nesysomsrl .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#nesysomsrl .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#nesysomsrl .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#nesysomsrl .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#nesysomsrl .gt_group_heading {
  padding: 8px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

#nesysomsrl .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#nesysomsrl .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#nesysomsrl .gt_from_md &gt; :first-child {
  margin-top: 0;
}

#nesysomsrl .gt_from_md &gt; :last-child {
  margin-bottom: 0;
}

#nesysomsrl .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#nesysomsrl .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 12px;
}

#nesysomsrl .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#nesysomsrl .gt_first_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
}

#nesysomsrl .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#nesysomsrl .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#nesysomsrl .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#nesysomsrl .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#nesysomsrl .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding: 4px;
}

#nesysomsrl .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#nesysomsrl .gt_sourcenote {
  font-size: 90%;
  padding: 4px;
}

#nesysomsrl .gt_left {
  text-align: left;
}

#nesysomsrl .gt_center {
  text-align: center;
}

#nesysomsrl .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#nesysomsrl .gt_font_normal {
  font-weight: normal;
}

#nesysomsrl .gt_font_bold {
  font-weight: bold;
}

#nesysomsrl .gt_font_italic {
  font-style: italic;
}

#nesysomsrl .gt_super {
  font-size: 65%;
}

#nesysomsrl .gt_footnote_marks {
  font-style: italic;
  font-size: 65%;
}
&lt;/style&gt;
&lt;div id=&#34;nesysomsrl&#34; style=&#34;overflow-x:auto;overflow-y:auto;width:auto;height:auto;&#34;&gt;&lt;table class=&#34;gt_table&#34;&gt;
  
  &lt;thead class=&#34;gt_col_headings&#34;&gt;
    &lt;tr&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_left&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;episode&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_left&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;character&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_left&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;line&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody class=&#34;gt_table_body&#34;&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0101&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;NA&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;[Scene: Central Perk, Chandler, Joey, Phoebe, and Monica are there.]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0101&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;Monica&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;There&#39;s nothing to tell! He&#39;s just some guy
I work with!&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0101&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;Joey&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;C&#39;mon, you&#39;re going out with the guy! There&#39;s
gotta be something wrong with him!&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0101&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;Chandler&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;All right Joey, be
nice.  So does he have a hump? A hump and a hairpiece?&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0101&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;Phoebe&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;Wait, does he eat chalk?&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0101&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;NA&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;(They all stare, bemused.)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0101&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;Phoebe&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;Just, &#39;cause, I don&#39;t want her to go through
what I went through with Carl- oh!&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0101&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;Monica&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;Okay, everybody relax. This is not even a
date. It&#39;s just two people going out to dinner and- not having sex.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0101&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;Chandler&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;Sounds like a date to me.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0101&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;NA&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;[Time Lapse]&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  
  
&lt;/table&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;cleaning&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Cleaning&lt;/h2&gt;
&lt;p&gt;First, there are some lines that give stage directions which we want to filter out of the dataset, since we are only interested in spoken words. There are also directions given sometimes within a character’s line, which we want to filter.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;friends &amp;lt;- friends_raw %&amp;gt;%
  mutate(
    line = str_replace_all(line, &amp;quot;[\u0091\u0092]&amp;quot;, &amp;quot;&amp;#39;&amp;quot;),
    line = str_replace_all(line, &amp;quot;\u0097&amp;quot;, &amp;quot;-&amp;quot;),
    line = str_replace_all(line, &amp;quot;\\([\\w\n [[:punct:]]]+\\)&amp;quot;, &amp;quot;&amp;quot;),
    line = str_replace_all(line, &amp;quot;\\[[\\w\n [[:punct:]]]+\\]&amp;quot;, &amp;quot;&amp;quot;),
    line = str_trim(line)
  ) %&amp;gt;%
  filter(line != &amp;quot;&amp;quot;)

head(friends, n = 10) %&amp;gt;%
  select(episode, character, line) %&amp;gt;%
  gt::gt()&lt;/code&gt;&lt;/pre&gt;
&lt;style&gt;html {
  font-family: -apple-system, BlinkMacSystemFont, &#39;Segoe UI&#39;, Roboto, Oxygen, Ubuntu, Cantarell, &#39;Helvetica Neue&#39;, &#39;Fira Sans&#39;, &#39;Droid Sans&#39;, Arial, sans-serif;
}

#vaqlnchokx .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#vaqlnchokx .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#vaqlnchokx .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#vaqlnchokx .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 4px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#vaqlnchokx .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#vaqlnchokx .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#vaqlnchokx .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#vaqlnchokx .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#vaqlnchokx .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#vaqlnchokx .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#vaqlnchokx .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#vaqlnchokx .gt_group_heading {
  padding: 8px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

#vaqlnchokx .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#vaqlnchokx .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#vaqlnchokx .gt_from_md &gt; :first-child {
  margin-top: 0;
}

#vaqlnchokx .gt_from_md &gt; :last-child {
  margin-bottom: 0;
}

#vaqlnchokx .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#vaqlnchokx .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 12px;
}

#vaqlnchokx .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#vaqlnchokx .gt_first_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
}

#vaqlnchokx .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#vaqlnchokx .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#vaqlnchokx .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#vaqlnchokx .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#vaqlnchokx .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding: 4px;
}

#vaqlnchokx .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#vaqlnchokx .gt_sourcenote {
  font-size: 90%;
  padding: 4px;
}

#vaqlnchokx .gt_left {
  text-align: left;
}

#vaqlnchokx .gt_center {
  text-align: center;
}

#vaqlnchokx .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#vaqlnchokx .gt_font_normal {
  font-weight: normal;
}

#vaqlnchokx .gt_font_bold {
  font-weight: bold;
}

#vaqlnchokx .gt_font_italic {
  font-style: italic;
}

#vaqlnchokx .gt_super {
  font-size: 65%;
}

#vaqlnchokx .gt_footnote_marks {
  font-style: italic;
  font-size: 65%;
}
&lt;/style&gt;
&lt;div id=&#34;vaqlnchokx&#34; style=&#34;overflow-x:auto;overflow-y:auto;width:auto;height:auto;&#34;&gt;&lt;table class=&#34;gt_table&#34;&gt;
  
  &lt;thead class=&#34;gt_col_headings&#34;&gt;
    &lt;tr&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_left&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;episode&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_left&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;character&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_left&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;line&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody class=&#34;gt_table_body&#34;&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0101&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;Monica&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;There&#39;s nothing to tell! He&#39;s just some guy
I work with!&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0101&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;Joey&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;C&#39;mon, you&#39;re going out with the guy! There&#39;s
gotta be something wrong with him!&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0101&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;Chandler&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;All right Joey, be
nice.  So does he have a hump? A hump and a hairpiece?&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0101&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;Phoebe&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;Wait, does he eat chalk?&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0101&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;Phoebe&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;Just, &#39;cause, I don&#39;t want her to go through
what I went through with Carl- oh!&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0101&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;Monica&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;Okay, everybody relax. This is not even a
date. It&#39;s just two people going out to dinner and- not having sex.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0101&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;Chandler&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;Sounds like a date to me.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0101&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;Chandler&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;Alright, so I&#39;m back in high school, I&#39;m
standing in the middle of the cafeteria, and I realize I am totally naked.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0101&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;All&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;Oh, yeah. Had that dream.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0101&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;Chandler&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;Then I look down, and I realize there&#39;s a
phone... there.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  
  
&lt;/table&gt;&lt;/div&gt;
&lt;p&gt;Now we transform our data from a set of lines from every script into a format that is more useful for analysis. Using &lt;code&gt;tidytext&lt;/code&gt;, we generate a new table that has every word from the corpus and how many times it was used. We also calculate cumulative sums and proportions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Counts how many times each word
# appears, sorts by number of appearances,
# and adds cumulative proportions
add_cumulative_stats &amp;lt;- function(x) {
  x %&amp;gt;%
    count(word, sort = TRUE) %&amp;gt;%
    mutate(cumsum = cumsum(n),
           cumprop = cumsum / sum(n),
           index = 1:n())
}

library(tidytext)

words &amp;lt;- friends %&amp;gt;%
  unnest_tokens(word, line)

word_counts &amp;lt;- add_cumulative_stats(words)

word_counts %&amp;gt;%
  head(n = 10) %&amp;gt;%
  gt::gt()&lt;/code&gt;&lt;/pre&gt;
&lt;style&gt;html {
  font-family: -apple-system, BlinkMacSystemFont, &#39;Segoe UI&#39;, Roboto, Oxygen, Ubuntu, Cantarell, &#39;Helvetica Neue&#39;, &#39;Fira Sans&#39;, &#39;Droid Sans&#39;, Arial, sans-serif;
}

#nutqdpjnfa .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#nutqdpjnfa .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#nutqdpjnfa .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#nutqdpjnfa .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 4px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#nutqdpjnfa .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#nutqdpjnfa .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#nutqdpjnfa .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#nutqdpjnfa .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#nutqdpjnfa .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#nutqdpjnfa .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#nutqdpjnfa .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#nutqdpjnfa .gt_group_heading {
  padding: 8px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

#nutqdpjnfa .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#nutqdpjnfa .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#nutqdpjnfa .gt_from_md &gt; :first-child {
  margin-top: 0;
}

#nutqdpjnfa .gt_from_md &gt; :last-child {
  margin-bottom: 0;
}

#nutqdpjnfa .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#nutqdpjnfa .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 12px;
}

#nutqdpjnfa .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#nutqdpjnfa .gt_first_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
}

#nutqdpjnfa .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#nutqdpjnfa .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#nutqdpjnfa .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#nutqdpjnfa .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#nutqdpjnfa .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding: 4px;
}

#nutqdpjnfa .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#nutqdpjnfa .gt_sourcenote {
  font-size: 90%;
  padding: 4px;
}

#nutqdpjnfa .gt_left {
  text-align: left;
}

#nutqdpjnfa .gt_center {
  text-align: center;
}

#nutqdpjnfa .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#nutqdpjnfa .gt_font_normal {
  font-weight: normal;
}

#nutqdpjnfa .gt_font_bold {
  font-weight: bold;
}

#nutqdpjnfa .gt_font_italic {
  font-style: italic;
}

#nutqdpjnfa .gt_super {
  font-size: 65%;
}

#nutqdpjnfa .gt_footnote_marks {
  font-style: italic;
  font-size: 65%;
}
&lt;/style&gt;
&lt;div id=&#34;nutqdpjnfa&#34; style=&#34;overflow-x:auto;overflow-y:auto;width:auto;height:auto;&#34;&gt;&lt;table class=&#34;gt_table&#34;&gt;
  
  &lt;thead class=&#34;gt_col_headings&#34;&gt;
    &lt;tr&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_left&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;word&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_center&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;n&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_center&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;cumsum&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_right&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;cumprop&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_center&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;index&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody class=&#34;gt_table_body&#34;&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;i&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;23298&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;23298&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;0.04076248&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;you&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;22017&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;45315&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;0.07928371&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;the&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;13292&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;58607&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;0.10253956&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;to&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;11554&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;70161&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;0.12275459&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;a&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;10749&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;80910&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;0.14156118&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;and&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;9405&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;90315&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;0.15801629&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;it&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;7631&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;97946&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;0.17136758&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;that&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;7564&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;105510&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;0.18460166&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;oh&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;7226&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;112736&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;0.19724436&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;9&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;what&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;6355&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;119091&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;0.20836315&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;10&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  
  
&lt;/table&gt;&lt;/div&gt;
&lt;p&gt;From the cumulative proportions in the table we can see that knowing the top 10 words covers around 20% of all the words in the dataset.&lt;/p&gt;
&lt;p&gt;This is a good start, but it’s not quite what we want. Right now the data includes grammatical variations of the same word as different words. For example, “age” and “ages” really represent the same word, but the graph above includes them as two separate words. As such, the data we have so far overestimates the number of unique words that appear in Friends.&lt;/p&gt;
&lt;p&gt;Lemmatizing algorithms try to map every grammatical variation of a word to a “stem”. For example, the words “go”, “going”, and “went” should map to the stem “go”. Stemming algorithms are related, but work by removing suffixes from the ends of words. A stemming algorithm wouldn’t know to map “went” to “go”; for that, we need lemmatization, which uses dictionaries to reverse the inflected forms of words to its grammatical root. The package &lt;a href=&#34;https://cran.r-project.org/web/packages/textstem/index.html&#34;&gt;&lt;code&gt;textstem&lt;/code&gt;&lt;/a&gt; provides a &lt;code&gt;lemmatize_words&lt;/code&gt; function in R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(textstem)

word_lemmas &amp;lt;- words %&amp;gt;%
  mutate(word = lemmatize_words(word))

word_lemma_counts &amp;lt;- add_cumulative_stats(word_lemmas)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are 11,852 words after lemmatization, compared to 15,134 before.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;word_lemma_counts %&amp;gt;%
  head(n = 10) %&amp;gt;%
  gt::gt()&lt;/code&gt;&lt;/pre&gt;
&lt;style&gt;html {
  font-family: -apple-system, BlinkMacSystemFont, &#39;Segoe UI&#39;, Roboto, Oxygen, Ubuntu, Cantarell, &#39;Helvetica Neue&#39;, &#39;Fira Sans&#39;, &#39;Droid Sans&#39;, Arial, sans-serif;
}

#gxslgjtkij .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#gxslgjtkij .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#gxslgjtkij .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#gxslgjtkij .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 4px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#gxslgjtkij .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#gxslgjtkij .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#gxslgjtkij .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#gxslgjtkij .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#gxslgjtkij .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#gxslgjtkij .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#gxslgjtkij .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#gxslgjtkij .gt_group_heading {
  padding: 8px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

#gxslgjtkij .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#gxslgjtkij .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#gxslgjtkij .gt_from_md &gt; :first-child {
  margin-top: 0;
}

#gxslgjtkij .gt_from_md &gt; :last-child {
  margin-bottom: 0;
}

#gxslgjtkij .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#gxslgjtkij .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 12px;
}

#gxslgjtkij .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#gxslgjtkij .gt_first_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
}

#gxslgjtkij .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#gxslgjtkij .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#gxslgjtkij .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#gxslgjtkij .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#gxslgjtkij .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding: 4px;
}

#gxslgjtkij .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#gxslgjtkij .gt_sourcenote {
  font-size: 90%;
  padding: 4px;
}

#gxslgjtkij .gt_left {
  text-align: left;
}

#gxslgjtkij .gt_center {
  text-align: center;
}

#gxslgjtkij .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#gxslgjtkij .gt_font_normal {
  font-weight: normal;
}

#gxslgjtkij .gt_font_bold {
  font-weight: bold;
}

#gxslgjtkij .gt_font_italic {
  font-style: italic;
}

#gxslgjtkij .gt_super {
  font-size: 65%;
}

#gxslgjtkij .gt_footnote_marks {
  font-style: italic;
  font-size: 65%;
}
&lt;/style&gt;
&lt;div id=&#34;gxslgjtkij&#34; style=&#34;overflow-x:auto;overflow-y:auto;width:auto;height:auto;&#34;&gt;&lt;table class=&#34;gt_table&#34;&gt;
  
  &lt;thead class=&#34;gt_col_headings&#34;&gt;
    &lt;tr&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_left&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;word&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_center&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;n&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_center&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;cumsum&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_right&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;cumprop&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_center&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;index&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody class=&#34;gt_table_body&#34;&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;i&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;23298&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;23298&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;0.04076248&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;you&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;22364&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;45662&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;0.07989082&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;be&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;19053&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;64715&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;0.11322620&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;the&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;13292&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;78007&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;0.13648205&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;a&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;11566&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;89573&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;0.15671808&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;to&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;11554&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;101127&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;0.17693310&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;and&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;9405&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;110532&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;0.19338821&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;that&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;7993&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;118525&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;0.20737287&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;it&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;7631&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;126156&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;0.22072416&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;9&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;oh&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;7226&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;133382&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;0.23336687&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;10&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  
  
&lt;/table&gt;&lt;/div&gt;
&lt;p&gt;The top 10 words are the same, but the cumulative proportions have changed since there are fewer words overall. Now knowing the top 10 words covers about 23% of all words.&lt;/p&gt;
&lt;p&gt;A helper function helps compute how many words we need to know to cover a certain percentage of the total:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# How many words do you need to know prop% of all words?
words_for_prop &amp;lt;- function(x, prop) {
  x %&amp;gt;% filter(cumprop &amp;gt; prop) %&amp;gt;% pull(index) %&amp;gt;% first()
}

words_for_prop_result &amp;lt;- tibble(
  prop = c(0.98, 0.95, 0.9, 0.5),
  words = map_int(prop, function(x) words_for_prop(word_lemma_counts, x))
)

words_for_prop_result %&amp;gt;%
  gt() %&amp;gt;%
  fmt_percent(columns = vars(prop), decimals = 0) %&amp;gt;%
  fmt_number(columns = vars(words), use_seps = TRUE, decimals = 0) %&amp;gt;%
  cols_label(prop = &amp;quot;Cumulative percentage&amp;quot;, words = &amp;quot;Number of words&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;style&gt;html {
  font-family: -apple-system, BlinkMacSystemFont, &#39;Segoe UI&#39;, Roboto, Oxygen, Ubuntu, Cantarell, &#39;Helvetica Neue&#39;, &#39;Fira Sans&#39;, &#39;Droid Sans&#39;, Arial, sans-serif;
}

#ymdcyjcmgh .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#ymdcyjcmgh .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#ymdcyjcmgh .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#ymdcyjcmgh .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 4px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#ymdcyjcmgh .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#ymdcyjcmgh .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#ymdcyjcmgh .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#ymdcyjcmgh .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#ymdcyjcmgh .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#ymdcyjcmgh .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#ymdcyjcmgh .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#ymdcyjcmgh .gt_group_heading {
  padding: 8px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

#ymdcyjcmgh .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#ymdcyjcmgh .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#ymdcyjcmgh .gt_from_md &gt; :first-child {
  margin-top: 0;
}

#ymdcyjcmgh .gt_from_md &gt; :last-child {
  margin-bottom: 0;
}

#ymdcyjcmgh .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#ymdcyjcmgh .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 12px;
}

#ymdcyjcmgh .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#ymdcyjcmgh .gt_first_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
}

#ymdcyjcmgh .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#ymdcyjcmgh .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#ymdcyjcmgh .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#ymdcyjcmgh .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#ymdcyjcmgh .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding: 4px;
}

#ymdcyjcmgh .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#ymdcyjcmgh .gt_sourcenote {
  font-size: 90%;
  padding: 4px;
}

#ymdcyjcmgh .gt_left {
  text-align: left;
}

#ymdcyjcmgh .gt_center {
  text-align: center;
}

#ymdcyjcmgh .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#ymdcyjcmgh .gt_font_normal {
  font-weight: normal;
}

#ymdcyjcmgh .gt_font_bold {
  font-weight: bold;
}

#ymdcyjcmgh .gt_font_italic {
  font-style: italic;
}

#ymdcyjcmgh .gt_super {
  font-size: 65%;
}

#ymdcyjcmgh .gt_footnote_marks {
  font-style: italic;
  font-size: 65%;
}
&lt;/style&gt;
&lt;div id=&#34;ymdcyjcmgh&#34; style=&#34;overflow-x:auto;overflow-y:auto;width:auto;height:auto;&#34;&gt;&lt;table class=&#34;gt_table&#34;&gt;
  
  &lt;thead class=&#34;gt_col_headings&#34;&gt;
    &lt;tr&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_right&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;Cumulative percentage&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_center&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;Number of words&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody class=&#34;gt_table_body&#34;&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;98&amp;percnt;&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;4,148&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;95&amp;percnt;&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;1,758&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;90&amp;percnt;&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;743&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;50&amp;percnt;&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;46&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  
  
&lt;/table&gt;&lt;/div&gt;
&lt;p&gt;Using the helper function, we can make a graph of the cumulative distribution of words with cutoffs at 50%, 90%, and 95%:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;word_lemma_counts %&amp;gt;%
  ggplot(aes(x = index, y = cumprop * 100)) +
  geom_line(color = &amp;quot;#2980b9&amp;quot;, size = 1) +
  geom_segment(aes(x = words, xend = words, y = 0, yend = prop * 100), lty = 2, data = words_for_prop_result, color = &amp;quot;#2c3e50&amp;quot;) +
  geom_segment(aes(x = 0, xend = words, y = prop * 100, yend = prop * 100), lty = 2, data = words_for_prop_result, color = &amp;quot;#2c3e50&amp;quot;) +
  geom_point(aes(x = words, y = prop * 100), data = words_for_prop_result, color = &amp;quot;white&amp;quot;, size = 3) +
  geom_point(aes(x = words, y = prop * 100), data = words_for_prop_result, color = &amp;quot;#2c3e50&amp;quot;, size = 2) +
  labs(x = &amp;quot;Words ordered by frequency&amp;quot;, y = &amp;quot;Cumulative percentage&amp;quot;) +
  cowplot::theme_cowplot() +
  scale_y_continuous(labels = function(x) paste0(x, &amp;quot;%&amp;quot;), breaks = c(0, 25, 50, 75, words_for_prop_result$prop * 100), expand = c(0.01, 0.01)) +
  scale_x_continuous(breaks = c(words_for_prop_result$words, 7500, 10000), expand = c(0.01, 0.01)) +
  my_theme +
  ggtitle(&amp;quot;You need to know around 750 words to know 90%\nof all the words used in Friends&amp;quot;) +
  labs(caption = &amp;quot;Analysis: Herb Susmann, transcripts: https://fangj.github.io/friends/&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:plot&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/post/2020-08-04-friends/index.en_files/figure-html/plot-1.png&#34; alt=&#34;You need to know 750 words to know 90% of all the words used in Friends&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: You need to know 750 words to know 90% of all the words used in Friends
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;appendix-top-750-words&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Appendix: top 750 words&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;word_lemma_counts %&amp;gt;%
  mutate(`cumulative proportion` = scales::percent_format(0.01)(cumprop)) %&amp;gt;%
  select(index, n, word, `cumulative proportion`) %&amp;gt;%
  head(n = 750) %&amp;gt;%
  reactable()&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-1&#34; class=&#34;reactable html-widget&#34; style=&#34;width:auto;height:auto;&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;tag&#34;:{&#34;name&#34;:&#34;Reactable&#34;,&#34;attribs&#34;:{&#34;data&#34;:{&#34;index&#34;:[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750],&#34;n&#34;:[23298,22364,19053,13292,11566,11554,9405,7993,7631,7226,7213,6355,5950,5900,5702,5655,5633,5452,5443,5019,5010,4869,4793,4703,4672,4526,4415,4124,4065,4040,3870,3813,3711,3524,3431,3397,3320,3267,3181,3164,3079,2936,2831,2590,2560,2442,2396,2390,2317,2307,2226,2220,2213,2166,2159,2145,2136,2127,2114,2071,1995,1982,1969,1950,1881,1845,1804,1801,1784,1713,1665,1595,1528,1495,1461,1448,1426,1418,1407,1325,1262,1256,1256,1248,1235,1193,1191,1163,1161,1139,1138,1119,1108,1064,1048,1038,1037,1036,1035,1006,999,997,986,950,950,946,890,875,853,850,848,836,828,807,796,780,780,772,757,751,751,742,739,716,710,709,704,701,690,678,671,667,665,654,645,641,634,633,631,627,626,619,618,610,605,604,604,601,600,587,584,580,575,569,565,563,556,556,545,539,529,525,521,509,507,502,497,496,492,490,488,481,478,476,475,474,462,461,458,455,449,447,445,445,445,444,443,439,427,422,422,418,416,416,414,413,413,411,407,407,406,401,400,398,396,396,389,385,384,384,382,382,376,369,367,366,363,361,355,350,346,346,344,343,342,341,339,332,329,316,312,310,310,309,306,299,298,298,296,296,295,292,292,290,290,286,283,283,283,282,281,278,277,276,276,276,276,273,269,269,267,266,264,262,260,259,259,256,255,253,253,251,251,249,249,246,245,244,244,241,240,239,237,237,236,235,235,232,232,231,229,228,228,227,223,223,221,221,217,216,214,213,213,212,210,210,210,209,209,209,207,206,206,206,205,205,204,199,199,198,198,198,198,197,196,194,191,191,189,188,188,186,186,185,185,185,183,181,181,181,180,179,178,178,176,176,176,174,173,173,173,172,171,171,170,170,169,169,169,168,168,167,167,166,165,165,164,164,163,163,162,162,161,161,160,160,160,160,159,158,158,157,157,157,156,155,155,155,151,151,151,150,150,149,149,148,148,148,147,147,147,146,146,146,145,144,144,144,143,142,140,140,140,140,140,139,139,139,138,138,137,137,136,136,136,135,135,135,135,134,133,133,132,132,131,131,131,130,130,130,130,129,129,128,128,128,128,128,127,127,126,125,124,124,123,123,123,123,122,122,122,122,120,119,119,118,117,117,117,117,116,116,116,116,115,115,114,114,114,114,114,113,113,113,113,112,112,112,111,111,110,108,108,108,108,108,107,107,107,107,107,107,107,107,106,106,106,105,105,105,105,104,104,103,103,102,102,101,101,100,100,99,99,99,99,99,98,98,97,97,96,96,95,95,94,94,94,94,94,94,94,93,93,93,92,92,92,91,91,90,90,90,89,89,89,89,88,88,88,88,87,87,87,87,87,87,87,86,86,86,86,86,85,85,85,84,84,83,83,83,83,83,83,83,82,82,82,82,82,82,80,80,80,80,80,80,80,80,79,79,79,79,79,78,78,78,78,78,78,78,78,78,78,78,78,77,77,77,77,76,76,76,76,76,76,76,75,75,75,75,75,75,74,74,74,74,74,73,73,73,72,72,72,72,72,72,71,71,71,70,69,69,69,69,69,68,68,68,67,67,67,67,67,67,67,66,66,66,66,66,66,65,65,65,65,65,65,65,65,65,64,64,63,63,63,63,63,63,63,63,62,62,62,62,62,62,61,61,61,61,61,61,61,61,61,61,61,61,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,59,59,59,59,59,59,59,59,59,58,58,58,58,58,58,58,58,57,57,57,57,57,57,57,57,57,56],&#34;word&#34;:[&#34;i&#34;,&#34;you&#34;,&#34;be&#34;,&#34;the&#34;,&#34;a&#34;,&#34;to&#34;,&#34;and&#34;,&#34;that&#34;,&#34;it&#34;,&#34;oh&#34;,&#34;do&#34;,&#34;what&#34;,&#34;good&#34;,&#34;no&#34;,&#34;have&#34;,&#34;this&#34;,&#34;yes&#34;,&#34;okay&#34;,&#34;i&#39;m&#34;,&#34;just&#34;,&#34;me&#34;,&#34;so&#34;,&#34;my&#34;,&#34;get&#34;,&#34;of&#34;,&#34;know&#34;,&#34;in&#34;,&#34;go&#34;,&#34;it&#39;s&#34;,&#34;we&#34;,&#34;don&#39;t&#34;,&#34;not&#34;,&#34;hey&#34;,&#34;on&#34;,&#34;with&#34;,&#34;for&#34;,&#34;all&#34;,&#34;but&#34;,&#34;right&#34;,&#34;can&#34;,&#34;think&#34;,&#34;like&#34;,&#34;your&#34;,&#34;you&#39;re&#34;,&#34;gonna&#34;,&#34;about&#34;,&#34;that&#39;s&#34;,&#34;look&#34;,&#34;really&#34;,&#34;out&#34;,&#34;uh&#34;,&#34;guy&#34;,&#34;say&#34;,&#34;her&#34;,&#34;y&#39;know&#34;,&#34;want&#34;,&#34;here&#34;,&#34;see&#34;,&#34;come&#34;,&#34;if&#34;,&#34;up&#34;,&#34;mean&#34;,&#34;he&#34;,&#34;she&#34;,&#34;how&#34;,&#34;at&#34;,&#34;ross&#34;,&#34;one&#34;,&#34;there&#34;,&#34;now&#34;,&#34;tell&#34;,&#34;why&#34;,&#34;hello&#34;,&#34;joey&#34;,&#34;god&#34;,&#34;much&#34;,&#34;him&#34;,&#34;can&#39;t&#34;,&#34;make&#34;,&#34;would&#34;,&#34;sorry&#34;,&#34;they&#34;,&#34;thing&#34;,&#34;when&#34;,&#34;great&#34;,&#34;chandler&#34;,&#34;time&#34;,&#34;then&#34;,&#34;take&#34;,&#34;monica&#34;,&#34;rachel&#34;,&#34;little&#34;,&#34;thank&#34;,&#34;didn&#39;t&#34;,&#34;talk&#34;,&#34;love&#34;,&#34;i&#39;ll&#34;,&#34;because&#34;,&#34;wait&#34;,&#34;back&#34;,&#34;we&#39;re&#34;,&#34;phoebe&#34;,&#34;who&#34;,&#34;he&#39;s&#34;,&#34;too&#34;,&#34;some&#34;,&#34;give&#34;,&#34;something&#34;,&#34;from&#34;,&#34;she&#39;s&#34;,&#34;or&#34;,&#34;umm&#34;,&#34;maybe&#34;,&#34;them&#34;,&#34;as&#34;,&#34;call&#34;,&#34;should&#34;,&#34;us&#34;,&#34;work&#34;,&#34;way&#34;,&#34;what&#39;s&#34;,&#34;will&#34;,&#34;over&#34;,&#34;i&#39;ve&#34;,&#34;need&#34;,&#34;never&#34;,&#34;sure&#34;,&#34;huh&#34;,&#34;feel&#34;,&#34;his&#34;,&#34;man&#34;,&#34;big&#34;,&#34;wanna&#34;,&#34;wow&#34;,&#34;baby&#34;,&#34;there&#39;s&#34;,&#34;happen&#34;,&#34;our&#34;,&#34;two&#34;,&#34;believe&#34;,&#34;down&#34;,&#34;please&#34;,&#34;friend&#34;,&#34;let&#34;,&#34;where&#34;,&#34;ah&#34;,&#34;still&#34;,&#34;leave&#34;,&#34;even&#34;,&#34;people&#34;,&#34;first&#34;,&#34;mr&#34;,&#34;fine&#34;,&#34;actually&#34;,&#34;day&#34;,&#34;again&#34;,&#34;off&#34;,&#34;try&#34;,&#34;let&#39;s&#34;,&#34;very&#34;,&#34;listen&#34;,&#34;by&#34;,&#34;night&#34;,&#34;any&#34;,&#34;marry&#34;,&#34;nice&#34;,&#34;ask&#34;,&#34;anything&#34;,&#34;woman&#34;,&#34;bad&#34;,&#34;stuff&#34;,&#34;other&#34;,&#34;find&#34;,&#34;year&#34;,&#34;um&#34;,&#34;hear&#34;,&#34;start&#34;,&#34;play&#34;,&#34;they&#39;re&#34;,&#34;last&#34;,&#34;only&#34;,&#34;gotta&#34;,&#34;ever&#34;,&#34;girl&#34;,&#34;stop&#34;,&#34;doesn&#39;t&#34;,&#34;new&#34;,&#34;minute&#34;,&#34;ooh&#34;,&#34;guess&#34;,&#34;use&#34;,&#34;before&#34;,&#34;geller&#34;,&#34;put&#34;,&#34;help&#34;,&#34;may&#34;,&#34;name&#34;,&#34;date&#34;,&#34;nothing&#34;,&#34;together&#34;,&#34;honey&#34;,&#34;break&#34;,&#34;lot&#34;,&#34;move&#34;,&#34;kid&#34;,&#34;ohh&#34;,&#34;meet&#34;,&#34;whoa&#34;,&#34;long&#34;,&#34;place&#34;,&#34;cause&#34;,&#34;rach&#34;,&#34;keep&#34;,&#34;mrs&#34;,&#34;remember&#34;,&#34;room&#34;,&#34;pheebs&#34;,&#34;into&#34;,&#34;dr&#34;,&#34;life&#34;,&#34;pretty&#34;,&#34;someone&#34;,&#34;fun&#34;,&#34;kinda&#34;,&#34;i&#39;d&#34;,&#34;around&#34;,&#34;live&#34;,&#34;happy&#34;,&#34;bye&#34;,&#34;late&#34;,&#34;2&#34;,&#34;always&#34;,&#34;kind&#34;,&#34;alright&#34;,&#34;isn&#39;t&#34;,&#34;three&#34;,&#34;funny&#34;,&#34;hand&#34;,&#34;hard&#34;,&#34;pick&#34;,&#34;tonight&#34;,&#34;than&#34;,&#34;wrong&#34;,&#34;care&#34;,&#34;kiss&#34;,&#34;wed&#34;,&#34;after&#34;,&#34;eat&#34;,&#34;stay&#34;,&#34;else&#34;,&#34;which&#34;,&#34;everything&#34;,&#34;ready&#34;,&#34;job&#34;,&#34;watch&#34;,&#34;we&#39;ll&#34;,&#34;you&#39;ve&#34;,&#34;old&#34;,&#34;home&#34;,&#34;miss&#34;,&#34;sleep&#34;,&#34;show&#34;,&#34;weird&#34;,&#34;sex&#34;,&#34;forget&#34;,&#34;away&#34;,&#34;party&#34;,&#34;mom&#34;,&#34;another&#34;,&#34;bring&#34;,&#34;totally&#34;,&#34;today&#34;,&#34;wear&#34;,&#34;head&#34;,&#34;wouldn&#39;t&#34;,&#34;money&#34;,&#34;turn&#34;,&#34;apartment&#34;,&#34;check&#34;,&#34;next&#34;,&#34;sound&#34;,&#34;idea&#34;,&#34;stupid&#34;,&#34;whole&#34;,&#34;wasn&#39;t&#34;,&#34;who&#39;s&#34;,&#34;won&#39;t&#34;,&#34;crazy&#34;,&#34;open&#34;,&#34;game&#34;,&#34;probably&#34;,&#34;lose&#34;,&#34;sit&#34;,&#34;cool&#34;,&#34;course&#34;,&#34;hour&#34;,&#34;part&#34;,&#34;suppose&#34;,&#34;hate&#34;,&#34;green&#34;,&#34;phone&#34;,&#34;dad&#34;,&#34;drink&#34;,&#34;problem&#34;,&#34;their&#34;,&#34;through&#34;,&#34;tomorrow&#34;,&#34;anyway&#34;,&#34;deal&#34;,&#34;run&#34;,&#34;haven&#39;t&#34;,&#34;already&#34;,&#34;end&#34;,&#34;win&#34;,&#34;walk&#34;,&#34;yet&#34;,&#34;every&#34;,&#34;each&#34;,&#34;enough&#34;,&#34;dinner&#34;,&#34;matter&#34;,&#34;such&#34;,&#34;while&#34;,&#34;question&#34;,&#34;since&#34;,&#34;buy&#34;,&#34;excuse&#34;,&#34;hold&#34;,&#34;throw&#34;,&#34;couple&#34;,&#34;everybody&#34;,&#34;couldn&#39;t&#34;,&#34;worry&#34;,&#34;change&#34;,&#34;close&#34;,&#34;understand&#34;,&#34;hang&#34;,&#34;boy&#34;,&#34;same&#34;,&#34;you&#39;ll&#34;,&#34;face&#34;,&#34;week&#34;,&#34;doctor&#34;,&#34;five&#34;,&#34;both&#34;,&#34;everyone&#34;,&#34;hot&#34;,&#34;sweet&#34;,&#34;emma&#34;,&#34;real&#34;,&#34;seem&#34;,&#34;pay&#34;,&#34;anymore&#34;,&#34;hell&#34;,&#34;cute&#34;,&#34;sister&#34;,&#34;own&#34;,&#34;read&#34;,&#34;you&#39;d&#34;,&#34;bing&#34;,&#34;ha&#34;,&#34;house&#34;,&#34;myself&#34;,&#34;plan&#34;,&#34;high&#34;,&#34;somebody&#34;,&#34;movie&#34;,&#34;we&#39;ve&#34;,&#34;also&#34;,&#34;person&#34;,&#34;die&#34;,&#34;ring&#34;,&#34;coffee&#34;,&#34;morning&#34;,&#34;beautiful&#34;,&#34;bite&#34;,&#34;hope&#34;,&#34;mine&#34;,&#34;picture&#34;,&#34;food&#34;,&#34;write&#34;,&#34;ben&#34;,&#34;hmm&#34;,&#34;must&#34;,&#34;joey&#39;s&#34;,&#34;door&#34;,&#34;dress&#34;,&#34;eye&#34;,&#34;lie&#34;,&#34;mike&#34;,&#34;yourself&#34;,&#34;here&#39;s&#34;,&#34;kill&#34;,&#34;cry&#34;,&#34;parent&#34;,&#34;amaze&#34;,&#34;figure&#34;,&#34;spend&#34;,&#34;different&#34;,&#34;fire&#34;,&#34;mind&#34;,&#34;easy&#34;,&#34;four&#34;,&#34;joke&#34;,&#34;true&#34;,&#34;book&#34;,&#34;scene&#34;,&#34;story&#34;,&#34;aren&#39;t&#34;,&#34;ticket&#34;,&#34;father&#34;,&#34;lady&#34;,&#34;once&#34;,&#34;serious&#34;,&#34;table&#34;,&#34;cut&#34;,&#34;though&#34;,&#34;uhm&#34;,&#34;dude&#34;,&#34;ow&#34;,&#34;until&#34;,&#34;word&#34;,&#34;dance&#34;,&#34;naked&#34;,&#34;wife&#34;,&#34;ago&#34;,&#34;birthday&#34;,&#34;chance&#34;,&#34;wish&#34;,&#34;point&#34;,&#34;emily&#34;,&#34;school&#34;,&#34;hair&#34;,&#34;numb&#34;,&#34;alone&#34;,&#34;either&#34;,&#34;hurt&#34;,&#34;anyone&#34;,&#34;half&#34;,&#34;reason&#34;,&#34;stand&#34;,&#34;fall&#34;,&#34;office&#34;,&#34;dog&#34;,&#34;line&#34;,&#34;monica&#39;s&#34;,&#34;uhh&#34;,&#34;without&#34;,&#34;joe&#34;,&#34;shoot&#34;,&#34;world&#34;,&#34;relationship&#34;,&#34;bet&#34;,&#34;mon&#34;,&#34;director&#34;,&#34;important&#34;,&#34;month&#34;,&#34;six&#34;,&#34;fact&#34;,&#34;hit&#34;,&#34;promise&#34;,&#34;smell&#34;,&#34;freak&#34;,&#34;car&#34;,&#34;perfect&#34;,&#34;present&#34;,&#34;interest&#34;,&#34;mad&#34;,&#34;pant&#34;,&#34;stick&#34;,&#34;ahh&#34;,&#34;ass&#34;,&#34;bed&#34;,&#34;decide&#34;,&#34;pregnant&#34;,&#34;send&#34;,&#34;boyfriend&#34;,&#34;card&#34;,&#34;dollar&#34;,&#34;tribbiani&#34;,&#34;wh&#34;,&#34;absolutely&#34;,&#34;box&#34;,&#34;fight&#34;,&#34;smoke&#34;,&#34;anybody&#34;,&#34;child&#34;,&#34;glad&#34;,&#34;bathroom&#34;,&#34;seriously&#34;,&#34;answer&#34;,&#34;chandler&#39;s&#34;,&#34;excite&#34;,&#34;less&#34;,&#34;set&#34;,&#34;whatever&#34;,&#34;3&#34;,&#34;em&#34;,&#34;exactly&#34;,&#34;family&#34;,&#34;janice&#34;,&#34;rachel&#39;s&#34;,&#34;soon&#34;,&#34;wonder&#34;,&#34;cat&#34;,&#34;divorce&#34;,&#34;ugh&#34;,&#34;mother&#34;,&#34;surprise&#34;,&#34;sweetie&#34;,&#34;touch&#34;,&#34;frank&#34;,&#34;shouldn&#39;t&#34;,&#34;finish&#34;,&#34;ho&#34;,&#34;how&#39;s&#34;,&#34;it&#39;ll&#34;,&#34;almost&#34;,&#34;christmas&#34;,&#34;key&#34;,&#34;luck&#34;,&#34;fat&#34;,&#34;free&#34;,&#34;laugh&#34;,&#34;light&#34;,&#34;many&#34;,&#34;clean&#34;,&#34;thanksgiving&#34;,&#34;catch&#34;,&#34;suck&#34;,&#34;audition&#34;,&#34;young&#34;,&#34;class&#34;,&#34;girlfriend&#34;,&#34;between&#34;,&#34;cannot&#34;,&#34;invite&#34;,&#34;its&#34;,&#34;rest&#34;,&#34;terrible&#34;,&#34;tv&#34;,&#34;carol&#34;,&#34;marriage&#34;,&#34;ten&#34;,&#34;bag&#34;,&#34;la&#34;,&#34;under&#34;,&#34;push&#34;,&#34;scare&#34;,&#34;finally&#34;,&#34;huge&#34;,&#34;realize&#34;,&#34;chair&#34;,&#34;grow&#34;,&#34;sick&#34;,&#34;steal&#34;,&#34;fast&#34;,&#34;moment&#34;,&#34;save&#34;,&#34;weren&#39;t&#34;,&#34;cup&#34;,&#34;dead&#34;,&#34;lunch&#34;,&#34;message&#34;,&#34;order&#34;,&#34;seat&#34;,&#34;yours&#34;,&#34;definitely&#34;,&#34;eh&#34;,&#34;kick&#34;,&#34;relax&#34;,&#34;welcome&#34;,&#34;gift&#34;,&#34;news&#34;,&#34;pass&#34;,&#34;speak&#34;,&#34;would&#39;ve&#34;,&#34;learn&#34;,&#34;restaurant&#34;,&#34;richard&#34;,&#34;small&#34;,&#34;song&#34;,&#34;teach&#34;,&#34;unless&#34;,&#34;able&#34;,&#34;build&#34;,&#34;credit&#34;,&#34;shoe&#34;,&#34;sing&#34;,&#34;sometimes&#34;,&#34;1&#34;,&#34;actor&#34;,&#34;cook&#34;,&#34;drop&#34;,&#34;london&#34;,&#34;massage&#34;,&#34;roommate&#34;,&#34;special&#34;,&#34;pizza&#34;,&#34;sell&#34;,&#34;seven&#34;,&#34;side&#34;,&#34;yep&#34;,&#34;act&#34;,&#34;boss&#34;,&#34;c&#39;mon&#34;,&#34;few&#34;,&#34;front&#34;,&#34;heart&#34;,&#34;ice&#34;,&#34;quit&#34;,&#34;sr&#34;,&#34;star&#34;,&#34;street&#34;,&#34;tape&#34;,&#34;city&#34;,&#34;clothe&#34;,&#34;foot&#34;,&#34;mess&#34;,&#34;aunt&#34;,&#34;nobody&#34;,&#34;propose&#34;,&#34;ross&#39;s&#34;,&#34;shut&#34;,&#34;son&#34;,&#34;where&#39;s&#34;,&#34;brother&#34;,&#34;dream&#34;,&#34;fault&#34;,&#34;grab&#34;,&#34;smart&#34;,&#34;water&#34;,&#34;enjoy&#34;,&#34;gay&#34;,&#34;pull&#34;,&#34;rule&#34;,&#34;secret&#34;,&#34;couch&#34;,&#34;drive&#34;,&#34;store&#34;,&#34;along&#34;,&#34;mark&#34;,&#34;pack&#34;,&#34;shop&#34;,&#34;upset&#34;,&#34;we&#39;d&#34;,&#34;o&#34;,&#34;ohhh&#34;,&#34;outside&#34;,&#34;college&#34;,&#34;ball&#34;,&#34;early&#34;,&#34;fair&#34;,&#34;hundred&#34;,&#34;shower&#34;,&#34;list&#34;,&#34;lucky&#34;,&#34;teacher&#34;,&#34;7&#34;,&#34;although&#34;,&#34;cookie&#34;,&#34;damn&#34;,&#34;sad&#34;,&#34;sign&#34;,&#34;top&#34;,&#34;buck&#34;,&#34;machine&#34;,&#34;paper&#34;,&#34;plate&#34;,&#34;ruin&#34;,&#34;sir&#34;,&#34;afraid&#34;,&#34;duck&#34;,&#34;eight&#34;,&#34;except&#34;,&#34;far&#34;,&#34;plane&#34;,&#34;sense&#34;,&#34;waltham&#34;,&#34;weekend&#34;,&#34;asleep&#34;,&#34;mistake&#34;,&#34;cake&#34;,&#34;chicken&#34;,&#34;fake&#34;,&#34;feeling&#34;,&#34;leg&#34;,&#34;shirt&#34;,&#34;twin&#34;,&#34;yay&#34;,&#34;blow&#34;,&#34;cast&#34;,&#34;david&#34;,&#34;pee&#34;,&#34;roll&#34;,&#34;sandwich&#34;,&#34;apparently&#34;,&#34;bitch&#34;,&#34;borrow&#34;,&#34;candy&#34;,&#34;charlie&#34;,&#34;control&#34;,&#34;marcel&#34;,&#34;middle&#34;,&#34;monkey&#34;,&#34;porn&#34;,&#34;strong&#34;,&#34;treeger&#34;,&#34;5&#34;,&#34;aww&#34;,&#34;bedroom&#34;,&#34;behind&#34;,&#34;bird&#34;,&#34;buddy&#34;,&#34;doin&#34;,&#34;hug&#34;,&#34;husband&#34;,&#34;none&#34;,&#34;notice&#34;,&#34;obviously&#34;,&#34;plus&#34;,&#34;step&#34;,&#34;swear&#34;,&#34;30&#34;,&#34;breast&#34;,&#34;clear&#34;,&#34;cold&#34;,&#34;gunther&#34;,&#34;hat&#34;,&#34;owe&#34;,&#34;yell&#34;,&#34;york&#34;,&#34;across&#34;,&#34;become&#34;,&#34;cover&#34;,&#34;crap&#34;,&#34;floor&#34;,&#34;hide&#34;,&#34;horrible&#34;,&#34;ride&#34;,&#34;amy&#34;,&#34;boat&#34;,&#34;chef&#34;,&#34;extra&#34;,&#34;hospital&#34;,&#34;idiot&#34;,&#34;knock&#34;,&#34;note&#34;,&#34;wake&#34;,&#34;beer&#34;],&#34;cumulative proportion&#34;:[&#34;4.08%&#34;,&#34;7.99%&#34;,&#34;11.32%&#34;,&#34;13.65%&#34;,&#34;15.67%&#34;,&#34;17.69%&#34;,&#34;19.34%&#34;,&#34;20.74%&#34;,&#34;22.07%&#34;,&#34;23.34%&#34;,&#34;24.60%&#34;,&#34;25.71%&#34;,&#34;26.75%&#34;,&#34;27.78%&#34;,&#34;28.78%&#34;,&#34;29.77%&#34;,&#34;30.76%&#34;,&#34;31.71%&#34;,&#34;32.66%&#34;,&#34;33.54%&#34;,&#34;34.42%&#34;,&#34;35.27%&#34;,&#34;36.11%&#34;,&#34;36.93%&#34;,&#34;37.75%&#34;,&#34;38.54%&#34;,&#34;39.31%&#34;,&#34;40.03%&#34;,&#34;40.75%&#34;,&#34;41.45%&#34;,&#34;42.13%&#34;,&#34;42.80%&#34;,&#34;43.45%&#34;,&#34;44.06%&#34;,&#34;44.66%&#34;,&#34;45.26%&#34;,&#34;45.84%&#34;,&#34;46.41%&#34;,&#34;46.97%&#34;,&#34;47.52%&#34;,&#34;48.06%&#34;,&#34;48.57%&#34;,&#34;49.07%&#34;,&#34;49.52%&#34;,&#34;49.97%&#34;,&#34;50.40%&#34;,&#34;50.81%&#34;,&#34;51.23%&#34;,&#34;51.64%&#34;,&#34;52.04%&#34;,&#34;52.43%&#34;,&#34;52.82%&#34;,&#34;53.21%&#34;,&#34;53.59%&#34;,&#34;53.96%&#34;,&#34;54.34%&#34;,&#34;54.71%&#34;,&#34;55.08%&#34;,&#34;55.45%&#34;,&#34;55.82%&#34;,&#34;56.17%&#34;,&#34;56.51%&#34;,&#34;56.86%&#34;,&#34;57.20%&#34;,&#34;57.53%&#34;,&#34;57.85%&#34;,&#34;58.17%&#34;,&#34;58.48%&#34;,&#34;58.79%&#34;,&#34;59.09%&#34;,&#34;59.38%&#34;,&#34;59.66%&#34;,&#34;59.93%&#34;,&#34;60.19%&#34;,&#34;60.45%&#34;,&#34;60.70%&#34;,&#34;60.95%&#34;,&#34;61.20%&#34;,&#34;61.44%&#34;,&#34;61.68%&#34;,&#34;61.90%&#34;,&#34;62.12%&#34;,&#34;62.34%&#34;,&#34;62.56%&#34;,&#34;62.77%&#34;,&#34;62.98%&#34;,&#34;63.19%&#34;,&#34;63.39%&#34;,&#34;63.60%&#34;,&#34;63.79%&#34;,&#34;63.99%&#34;,&#34;64.19%&#34;,&#34;64.38%&#34;,&#34;64.57%&#34;,&#34;64.75%&#34;,&#34;64.93%&#34;,&#34;65.12%&#34;,&#34;65.30%&#34;,&#34;65.48%&#34;,&#34;65.65%&#34;,&#34;65.83%&#34;,&#34;66.00%&#34;,&#34;66.18%&#34;,&#34;66.34%&#34;,&#34;66.51%&#34;,&#34;66.67%&#34;,&#34;66.83%&#34;,&#34;66.98%&#34;,&#34;67.13%&#34;,&#34;67.28%&#34;,&#34;67.43%&#34;,&#34;67.58%&#34;,&#34;67.72%&#34;,&#34;67.86%&#34;,&#34;68.00%&#34;,&#34;68.14%&#34;,&#34;68.27%&#34;,&#34;68.41%&#34;,&#34;68.54%&#34;,&#34;68.67%&#34;,&#34;68.80%&#34;,&#34;68.93%&#34;,&#34;69.06%&#34;,&#34;69.19%&#34;,&#34;69.31%&#34;,&#34;69.44%&#34;,&#34;69.56%&#34;,&#34;69.68%&#34;,&#34;69.80%&#34;,&#34;69.92%&#34;,&#34;70.04%&#34;,&#34;70.16%&#34;,&#34;70.27%&#34;,&#34;70.39%&#34;,&#34;70.50%&#34;,&#34;70.61%&#34;,&#34;70.72%&#34;,&#34;70.83%&#34;,&#34;70.94%&#34;,&#34;71.05%&#34;,&#34;71.16%&#34;,&#34;71.27%&#34;,&#34;71.38%&#34;,&#34;71.49%&#34;,&#34;71.59%&#34;,&#34;71.70%&#34;,&#34;71.80%&#34;,&#34;71.91%&#34;,&#34;72.01%&#34;,&#34;72.12%&#34;,&#34;72.22%&#34;,&#34;72.32%&#34;,&#34;72.42%&#34;,&#34;72.52%&#34;,&#34;72.62%&#34;,&#34;72.72%&#34;,&#34;72.81%&#34;,&#34;72.91%&#34;,&#34;73.01%&#34;,&#34;73.10%&#34;,&#34;73.19%&#34;,&#34;73.29%&#34;,&#34;73.38%&#34;,&#34;73.47%&#34;,&#34;73.55%&#34;,&#34;73.64%&#34;,&#34;73.73%&#34;,&#34;73.82%&#34;,&#34;73.90%&#34;,&#34;73.99%&#34;,&#34;74.07%&#34;,&#34;74.16%&#34;,&#34;74.24%&#34;,&#34;74.32%&#34;,&#34;74.41%&#34;,&#34;74.49%&#34;,&#34;74.57%&#34;,&#34;74.65%&#34;,&#34;74.73%&#34;,&#34;74.81%&#34;,&#34;74.89%&#34;,&#34;74.97%&#34;,&#34;75.05%&#34;,&#34;75.12%&#34;,&#34;75.20%&#34;,&#34;75.28%&#34;,&#34;75.36%&#34;,&#34;75.43%&#34;,&#34;75.51%&#34;,&#34;75.58%&#34;,&#34;75.66%&#34;,&#34;75.73%&#34;,&#34;75.80%&#34;,&#34;75.88%&#34;,&#34;75.95%&#34;,&#34;76.02%&#34;,&#34;76.09%&#34;,&#34;76.16%&#34;,&#34;76.24%&#34;,&#34;76.31%&#34;,&#34;76.38%&#34;,&#34;76.45%&#34;,&#34;76.52%&#34;,&#34;76.59%&#34;,&#34;76.66%&#34;,&#34;76.73%&#34;,&#34;76.79%&#34;,&#34;76.86%&#34;,&#34;76.93%&#34;,&#34;77.00%&#34;,&#34;77.06%&#34;,&#34;77.13%&#34;,&#34;77.20%&#34;,&#34;77.26%&#34;,&#34;77.32%&#34;,&#34;77.39%&#34;,&#34;77.45%&#34;,&#34;77.51%&#34;,&#34;77.58%&#34;,&#34;77.64%&#34;,&#34;77.70%&#34;,&#34;77.76%&#34;,&#34;77.82%&#34;,&#34;77.88%&#34;,&#34;77.94%&#34;,&#34;78.00%&#34;,&#34;78.06%&#34;,&#34;78.12%&#34;,&#34;78.17%&#34;,&#34;78.23%&#34;,&#34;78.28%&#34;,&#34;78.34%&#34;,&#34;78.39%&#34;,&#34;78.45%&#34;,&#34;78.50%&#34;,&#34;78.55%&#34;,&#34;78.60%&#34;,&#34;78.66%&#34;,&#34;78.71%&#34;,&#34;78.76%&#34;,&#34;78.81%&#34;,&#34;78.86%&#34;,&#34;78.91%&#34;,&#34;78.96%&#34;,&#34;79.01%&#34;,&#34;79.07%&#34;,&#34;79.11%&#34;,&#34;79.16%&#34;,&#34;79.21%&#34;,&#34;79.26%&#34;,&#34;79.31%&#34;,&#34;79.36%&#34;,&#34;79.41%&#34;,&#34;79.46%&#34;,&#34;79.51%&#34;,&#34;79.55%&#34;,&#34;79.60%&#34;,&#34;79.65%&#34;,&#34;79.70%&#34;,&#34;79.74%&#34;,&#34;79.79%&#34;,&#34;79.84%&#34;,&#34;79.88%&#34;,&#34;79.93%&#34;,&#34;79.97%&#34;,&#34;80.02%&#34;,&#34;80.07%&#34;,&#34;80.11%&#34;,&#34;80.16%&#34;,&#34;80.20%&#34;,&#34;80.24%&#34;,&#34;80.29%&#34;,&#34;80.33%&#34;,&#34;80.37%&#34;,&#34;80.42%&#34;,&#34;80.46%&#34;,&#34;80.50%&#34;,&#34;80.55%&#34;,&#34;80.59%&#34;,&#34;80.63%&#34;,&#34;80.67%&#34;,&#34;80.72%&#34;,&#34;80.76%&#34;,&#34;80.80%&#34;,&#34;80.84%&#34;,&#34;80.88%&#34;,&#34;80.92%&#34;,&#34;80.96%&#34;,&#34;81.00%&#34;,&#34;81.04%&#34;,&#34;81.08%&#34;,&#34;81.12%&#34;,&#34;81.16%&#34;,&#34;81.20%&#34;,&#34;81.24%&#34;,&#34;81.28%&#34;,&#34;81.32%&#34;,&#34;81.36%&#34;,&#34;81.40%&#34;,&#34;81.43%&#34;,&#34;81.47%&#34;,&#34;81.51%&#34;,&#34;81.55%&#34;,&#34;81.58%&#34;,&#34;81.62%&#34;,&#34;81.66%&#34;,&#34;81.69%&#34;,&#34;81.73%&#34;,&#34;81.77%&#34;,&#34;81.80%&#34;,&#34;81.84%&#34;,&#34;81.88%&#34;,&#34;81.91%&#34;,&#34;81.95%&#34;,&#34;81.98%&#34;,&#34;82.02%&#34;,&#34;82.06%&#34;,&#34;82.09%&#34;,&#34;82.12%&#34;,&#34;82.16%&#34;,&#34;82.19%&#34;,&#34;82.23%&#34;,&#34;82.26%&#34;,&#34;82.30%&#34;,&#34;82.33%&#34;,&#34;82.37%&#34;,&#34;82.40%&#34;,&#34;82.43%&#34;,&#34;82.47%&#34;,&#34;82.50%&#34;,&#34;82.53%&#34;,&#34;82.56%&#34;,&#34;82.60%&#34;,&#34;82.63%&#34;,&#34;82.66%&#34;,&#34;82.69%&#34;,&#34;82.73%&#34;,&#34;82.76%&#34;,&#34;82.79%&#34;,&#34;82.82%&#34;,&#34;82.85%&#34;,&#34;82.88%&#34;,&#34;82.92%&#34;,&#34;82.95%&#34;,&#34;82.98%&#34;,&#34;83.01%&#34;,&#34;83.04%&#34;,&#34;83.07%&#34;,&#34;83.10%&#34;,&#34;83.13%&#34;,&#34;83.16%&#34;,&#34;83.19%&#34;,&#34;83.22%&#34;,&#34;83.25%&#34;,&#34;83.28%&#34;,&#34;83.31%&#34;,&#34;83.34%&#34;,&#34;83.37%&#34;,&#34;83.40%&#34;,&#34;83.43%&#34;,&#34;83.46%&#34;,&#34;83.49%&#34;,&#34;83.52%&#34;,&#34;83.54%&#34;,&#34;83.57%&#34;,&#34;83.60%&#34;,&#34;83.63%&#34;,&#34;83.66%&#34;,&#34;83.69%&#34;,&#34;83.72%&#34;,&#34;83.74%&#34;,&#34;83.77%&#34;,&#34;83.80%&#34;,&#34;83.83%&#34;,&#34;83.86%&#34;,&#34;83.89%&#34;,&#34;83.91%&#34;,&#34;83.94%&#34;,&#34;83.97%&#34;,&#34;84.00%&#34;,&#34;84.02%&#34;,&#34;84.05%&#34;,&#34;84.08%&#34;,&#34;84.11%&#34;,&#34;84.13%&#34;,&#34;84.16%&#34;,&#34;84.19%&#34;,&#34;84.22%&#34;,&#34;84.24%&#34;,&#34;84.27%&#34;,&#34;84.29%&#34;,&#34;84.32%&#34;,&#34;84.35%&#34;,&#34;84.37%&#34;,&#34;84.40%&#34;,&#34;84.43%&#34;,&#34;84.45%&#34;,&#34;84.48%&#34;,&#34;84.50%&#34;,&#34;84.53%&#34;,&#34;84.55%&#34;,&#34;84.58%&#34;,&#34;84.61%&#34;,&#34;84.63%&#34;,&#34;84.66%&#34;,&#34;84.68%&#34;,&#34;84.71%&#34;,&#34;84.73%&#34;,&#34;84.76%&#34;,&#34;84.78%&#34;,&#34;84.81%&#34;,&#34;84.83%&#34;,&#34;84.86%&#34;,&#34;84.88%&#34;,&#34;84.90%&#34;,&#34;84.93%&#34;,&#34;84.95%&#34;,&#34;84.98%&#34;,&#34;85.00%&#34;,&#34;85.03%&#34;,&#34;85.05%&#34;,&#34;85.07%&#34;,&#34;85.10%&#34;,&#34;85.12%&#34;,&#34;85.14%&#34;,&#34;85.17%&#34;,&#34;85.19%&#34;,&#34;85.22%&#34;,&#34;85.24%&#34;,&#34;85.26%&#34;,&#34;85.29%&#34;,&#34;85.31%&#34;,&#34;85.33%&#34;,&#34;85.36%&#34;,&#34;85.38%&#34;,&#34;85.40%&#34;,&#34;85.42%&#34;,&#34;85.45%&#34;,&#34;85.47%&#34;,&#34;85.49%&#34;,&#34;85.52%&#34;,&#34;85.54%&#34;,&#34;85.56%&#34;,&#34;85.58%&#34;,&#34;85.61%&#34;,&#34;85.63%&#34;,&#34;85.65%&#34;,&#34;85.67%&#34;,&#34;85.69%&#34;,&#34;85.72%&#34;,&#34;85.74%&#34;,&#34;85.76%&#34;,&#34;85.78%&#34;,&#34;85.80%&#34;,&#34;85.83%&#34;,&#34;85.85%&#34;,&#34;85.87%&#34;,&#34;85.89%&#34;,&#34;85.91%&#34;,&#34;85.93%&#34;,&#34;85.95%&#34;,&#34;85.98%&#34;,&#34;86.00%&#34;,&#34;86.02%&#34;,&#34;86.04%&#34;,&#34;86.06%&#34;,&#34;86.08%&#34;,&#34;86.10%&#34;,&#34;86.12%&#34;,&#34;86.14%&#34;,&#34;86.16%&#34;,&#34;86.18%&#34;,&#34;86.20%&#34;,&#34;86.22%&#34;,&#34;86.24%&#34;,&#34;86.26%&#34;,&#34;86.28%&#34;,&#34;86.30%&#34;,&#34;86.32%&#34;,&#34;86.34%&#34;,&#34;86.36%&#34;,&#34;86.38%&#34;,&#34;86.40%&#34;,&#34;86.42%&#34;,&#34;86.44%&#34;,&#34;86.46%&#34;,&#34;86.48%&#34;,&#34;86.50%&#34;,&#34;86.52%&#34;,&#34;86.54%&#34;,&#34;86.56%&#34;,&#34;86.58%&#34;,&#34;86.60%&#34;,&#34;86.61%&#34;,&#34;86.63%&#34;,&#34;86.65%&#34;,&#34;86.67%&#34;,&#34;86.69%&#34;,&#34;86.71%&#34;,&#34;86.73%&#34;,&#34;86.75%&#34;,&#34;86.76%&#34;,&#34;86.78%&#34;,&#34;86.80%&#34;,&#34;86.82%&#34;,&#34;86.84%&#34;,&#34;86.86%&#34;,&#34;86.88%&#34;,&#34;86.89%&#34;,&#34;86.91%&#34;,&#34;86.93%&#34;,&#34;86.95%&#34;,&#34;86.97%&#34;,&#34;86.99%&#34;,&#34;87.00%&#34;,&#34;87.02%&#34;,&#34;87.04%&#34;,&#34;87.06%&#34;,&#34;87.07%&#34;,&#34;87.09%&#34;,&#34;87.11%&#34;,&#34;87.13%&#34;,&#34;87.14%&#34;,&#34;87.16%&#34;,&#34;87.18%&#34;,&#34;87.20%&#34;,&#34;87.21%&#34;,&#34;87.23%&#34;,&#34;87.25%&#34;,&#34;87.26%&#34;,&#34;87.28%&#34;,&#34;87.30%&#34;,&#34;87.31%&#34;,&#34;87.33%&#34;,&#34;87.35%&#34;,&#34;87.36%&#34;,&#34;87.38%&#34;,&#34;87.40%&#34;,&#34;87.41%&#34;,&#34;87.43%&#34;,&#34;87.45%&#34;,&#34;87.46%&#34;,&#34;87.48%&#34;,&#34;87.50%&#34;,&#34;87.51%&#34;,&#34;87.53%&#34;,&#34;87.54%&#34;,&#34;87.56%&#34;,&#34;87.58%&#34;,&#34;87.59%&#34;,&#34;87.61%&#34;,&#34;87.62%&#34;,&#34;87.64%&#34;,&#34;87.65%&#34;,&#34;87.67%&#34;,&#34;87.68%&#34;,&#34;87.70%&#34;,&#34;87.72%&#34;,&#34;87.73%&#34;,&#34;87.75%&#34;,&#34;87.76%&#34;,&#34;87.78%&#34;,&#34;87.79%&#34;,&#34;87.81%&#34;,&#34;87.82%&#34;,&#34;87.84%&#34;,&#34;87.85%&#34;,&#34;87.87%&#34;,&#34;87.88%&#34;,&#34;87.90%&#34;,&#34;87.91%&#34;,&#34;87.93%&#34;,&#34;87.94%&#34;,&#34;87.96%&#34;,&#34;87.97%&#34;,&#34;87.99%&#34;,&#34;88.00%&#34;,&#34;88.02%&#34;,&#34;88.03%&#34;,&#34;88.05%&#34;,&#34;88.06%&#34;,&#34;88.07%&#34;,&#34;88.09%&#34;,&#34;88.10%&#34;,&#34;88.12%&#34;,&#34;88.13%&#34;,&#34;88.15%&#34;,&#34;88.16%&#34;,&#34;88.18%&#34;,&#34;88.19%&#34;,&#34;88.20%&#34;,&#34;88.22%&#34;,&#34;88.23%&#34;,&#34;88.25%&#34;,&#34;88.26%&#34;,&#34;88.27%&#34;,&#34;88.29%&#34;,&#34;88.30%&#34;,&#34;88.32%&#34;,&#34;88.33%&#34;,&#34;88.34%&#34;,&#34;88.36%&#34;,&#34;88.37%&#34;,&#34;88.38%&#34;,&#34;88.40%&#34;,&#34;88.41%&#34;,&#34;88.43%&#34;,&#34;88.44%&#34;,&#34;88.45%&#34;,&#34;88.47%&#34;,&#34;88.48%&#34;,&#34;88.49%&#34;,&#34;88.51%&#34;,&#34;88.52%&#34;,&#34;88.53%&#34;,&#34;88.55%&#34;,&#34;88.56%&#34;,&#34;88.58%&#34;,&#34;88.59%&#34;,&#34;88.60%&#34;,&#34;88.62%&#34;,&#34;88.63%&#34;,&#34;88.64%&#34;,&#34;88.66%&#34;,&#34;88.67%&#34;,&#34;88.68%&#34;,&#34;88.69%&#34;,&#34;88.71%&#34;,&#34;88.72%&#34;,&#34;88.73%&#34;,&#34;88.75%&#34;,&#34;88.76%&#34;,&#34;88.77%&#34;,&#34;88.79%&#34;,&#34;88.80%&#34;,&#34;88.81%&#34;,&#34;88.83%&#34;,&#34;88.84%&#34;,&#34;88.85%&#34;,&#34;88.86%&#34;,&#34;88.88%&#34;,&#34;88.89%&#34;,&#34;88.90%&#34;,&#34;88.91%&#34;,&#34;88.93%&#34;,&#34;88.94%&#34;,&#34;88.95%&#34;,&#34;88.96%&#34;,&#34;88.98%&#34;,&#34;88.99%&#34;,&#34;89.00%&#34;,&#34;89.01%&#34;,&#34;89.02%&#34;,&#34;89.04%&#34;,&#34;89.05%&#34;,&#34;89.06%&#34;,&#34;89.07%&#34;,&#34;89.08%&#34;,&#34;89.10%&#34;,&#34;89.11%&#34;,&#34;89.12%&#34;,&#34;89.13%&#34;,&#34;89.14%&#34;,&#34;89.16%&#34;,&#34;89.17%&#34;,&#34;89.18%&#34;,&#34;89.19%&#34;,&#34;89.20%&#34;,&#34;89.21%&#34;,&#34;89.22%&#34;,&#34;89.24%&#34;,&#34;89.25%&#34;,&#34;89.26%&#34;,&#34;89.27%&#34;,&#34;89.28%&#34;,&#34;89.29%&#34;,&#34;89.30%&#34;,&#34;89.32%&#34;,&#34;89.33%&#34;,&#34;89.34%&#34;,&#34;89.35%&#34;,&#34;89.36%&#34;,&#34;89.37%&#34;,&#34;89.38%&#34;,&#34;89.39%&#34;,&#34;89.40%&#34;,&#34;89.42%&#34;,&#34;89.43%&#34;,&#34;89.44%&#34;,&#34;89.45%&#34;,&#34;89.46%&#34;,&#34;89.47%&#34;,&#34;89.48%&#34;,&#34;89.49%&#34;,&#34;89.50%&#34;,&#34;89.51%&#34;,&#34;89.52%&#34;,&#34;89.54%&#34;,&#34;89.55%&#34;,&#34;89.56%&#34;,&#34;89.57%&#34;,&#34;89.58%&#34;,&#34;89.59%&#34;,&#34;89.60%&#34;,&#34;89.61%&#34;,&#34;89.62%&#34;,&#34;89.63%&#34;,&#34;89.64%&#34;,&#34;89.65%&#34;,&#34;89.66%&#34;,&#34;89.67%&#34;,&#34;89.68%&#34;,&#34;89.69%&#34;,&#34;89.71%&#34;,&#34;89.72%&#34;,&#34;89.73%&#34;,&#34;89.74%&#34;,&#34;89.75%&#34;,&#34;89.76%&#34;,&#34;89.77%&#34;,&#34;89.78%&#34;,&#34;89.79%&#34;,&#34;89.80%&#34;,&#34;89.81%&#34;,&#34;89.82%&#34;,&#34;89.83%&#34;,&#34;89.84%&#34;,&#34;89.85%&#34;,&#34;89.86%&#34;,&#34;89.87%&#34;,&#34;89.88%&#34;,&#34;89.89%&#34;,&#34;89.90%&#34;,&#34;89.91%&#34;,&#34;89.92%&#34;,&#34;89.93%&#34;,&#34;89.94%&#34;,&#34;89.95%&#34;,&#34;89.96%&#34;,&#34;89.97%&#34;,&#34;89.98%&#34;,&#34;89.99%&#34;,&#34;90.00%&#34;,&#34;90.01%&#34;,&#34;90.02%&#34;,&#34;90.03%&#34;,&#34;90.04%&#34;,&#34;90.05%&#34;,&#34;90.06%&#34;,&#34;90.07%&#34;]},&#34;columns&#34;:[{&#34;accessor&#34;:&#34;index&#34;,&#34;name&#34;:&#34;index&#34;,&#34;type&#34;:&#34;numeric&#34;},{&#34;accessor&#34;:&#34;n&#34;,&#34;name&#34;:&#34;n&#34;,&#34;type&#34;:&#34;numeric&#34;},{&#34;accessor&#34;:&#34;word&#34;,&#34;name&#34;:&#34;word&#34;,&#34;type&#34;:&#34;character&#34;},{&#34;accessor&#34;:&#34;cumulative proportion&#34;,&#34;name&#34;:&#34;cumulative proportion&#34;,&#34;type&#34;:&#34;character&#34;}],&#34;defaultPageSize&#34;:10,&#34;paginationType&#34;:&#34;numbers&#34;,&#34;showPageInfo&#34;:true,&#34;minRows&#34;:1,&#34;dataKey&#34;:&#34;7526a5e0f65fcaf6b6a12ce4556c30c2&#34;,&#34;key&#34;:&#34;7526a5e0f65fcaf6b6a12ce4556c30c2&#34;},&#34;children&#34;:[]},&#34;class&#34;:&#34;reactR_markup&#34;},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Derivatives of a Gaussian Process</title>
      <link>/2020/07/06/gaussian-process-derivatives/</link>
      <pubDate>Mon, 06 Jul 2020 00:00:00 +0000</pubDate>
      <guid>/2020/07/06/gaussian-process-derivatives/</guid>
      <description>


&lt;p&gt;A &lt;a href=&#34;https://en.wikipedia.org/wiki/Gaussian_process&#34;&gt;&lt;em&gt;Gaussian Process&lt;/em&gt;&lt;/a&gt; (GP) is a process for which any finite set of observations follows a multivariate normal distribution. GPs are defined by their mean and a kernel function that gives the covariance between any two observations. They are useful in Bayesian statistics as priors over function spaces.&lt;/p&gt;
&lt;p&gt;To denote a GP prior over a set of observations &lt;span class=&#34;math inline&#34;&gt;\(\bm{y} = \{y_1, y_2, \cdots, y_n\}\)&lt;/span&gt; at points &lt;span class=&#34;math inline&#34;&gt;\(\bm{x} = \{ x_1, x_2, \cdots, x_n\}\)&lt;/span&gt;, we write:
&lt;span class=&#34;math display&#34;&gt;\[
\bm{y} \sim \mathcal{GP}(\bm{\mu}, \bm{\Sigma})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\bm{\Sigma}\)&lt;/span&gt; is computed via a kernel function &lt;span class=&#34;math inline&#34;&gt;\(k(x, x^\prime)\)&lt;/span&gt;. A popular choice of kernel function that we will consider in this post is the &lt;a href=&#34;https://www.cs.toronto.edu/~duvenaud/cookbook&#34;&gt;&lt;em&gt;squared exponential kernel&lt;/em&gt;&lt;/a&gt;:
&lt;span class=&#34;math display&#34;&gt;\[
k(x_i, x_j) = \alpha^2 \exp\left(-
\frac{(x_i - x_j)^2}{2\ell^2} \right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; is a scale parameter and &lt;span class=&#34;math inline&#34;&gt;\(\ell\)&lt;/span&gt; is a length-scale parameter which controls the strength of the association between points as they become farther apart.&lt;/p&gt;
&lt;p&gt;Intuitively, in order to define a GP we need to be able to write down the covariance between any two points.&lt;/p&gt;
&lt;p&gt;Let’s write this kernel function in R, and use it to draw samples from a GP. For simplicity, we will fix &lt;span class=&#34;math inline&#34;&gt;\(\bm{\mu} = \bm{0}\)&lt;/span&gt; for all the GPs we work with in this post. First, define the kernel function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;k &amp;lt;- function(x_i, x_j, alpha, l) {
  alpha^2 * exp(-(x_i - x_j)^2 / (2 * l^2))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s write a helper function that, given a kernel and a set of points, generates a full covariance matrix:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;covariance_from_kernel &amp;lt;- function(x, kernel, ...) {
  outer(x, x, kernel, ...)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and a function to draw from a mean-zero GP with a given covariance kernel:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gp_draw &amp;lt;- function(draws, x, Sigma, ...) {
  mu &amp;lt;- rep(0, length(x))
  mvtnorm::rmvnorm(draws, mu, Sigma)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can draw 10 samples from a mean-zero GP with a squared-exponential kernel with parameters &lt;span class=&#34;math inline&#34;&gt;\(\alpha=1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(l=1\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 100 # number of points to draw
x &amp;lt;- seq(1, 10, length.out = n) # position of each point

# Kernel parameters
alpha &amp;lt;- 1
l &amp;lt;- 1

set.seed(1)

# Draw 10 samples
Sigma &amp;lt;- covariance_from_kernel(x, k, alpha = alpha, l = l)
y &amp;lt;- gp_draw(10, x, Sigma)

matplot(x, t(y), type = &amp;#39;l&amp;#39;, xlab = &amp;#39;x&amp;#39;, ylab = &amp;#39;y&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-07-06-gaussian-process-derivatives.en_files/figure-html/draws_from_gp-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;derivative-of-a-gp&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Derivative of a GP&lt;/h2&gt;
&lt;p&gt;The derivative of a GP is also a GP, with its existence determined by the differentiability of its mean and kernel functions. The squared exponential kernel is infinitely differentiable, so the associated Gaussian Process has infinitely many derivatives.&lt;/p&gt;
&lt;p&gt;This derivative is useful because we may have prior knowledge about likely values of the derivative. For example, monotonicity constraints can be encoded as prior knowledge that the derivative is always positive or negative. In other cases we may have direct observations of the derivative that we would like to incorporate into model fitting.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\bm{y}^\prime = \{ y^\prime_1, y^\prime_2, \cdots, y^\prime_{n^\prime} \}\)&lt;/span&gt; be a set of derivative observations. The derivative GP, which we’ll denote &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{GP}^\prime\)&lt;/span&gt;, is given by
&lt;span class=&#34;math display&#34;&gt;\[
\bm{y}^\prime \sim \mathcal{GP}^\prime(\frac{d}{dx}\bm{\mu}, \frac{d}{dx}\bm{\Sigma})
\]&lt;/span&gt;
where the derivative of the covariance matrix is defined by the derivative of the original kernel function with respect to both of its inputs, which we will denote &lt;span class=&#34;math inline&#34;&gt;\(k_{11}\)&lt;/span&gt; to indicate that both arguments are derivative observations:
&lt;span class=&#34;math display&#34;&gt;\[
  k_{11}(x_i, x_j) = \frac{\partial}{\partial x_i \ \partial x_j} k(x_i, x_j) = \frac{ \alpha^2 }{ \ell^4 }\left( l^2 - (x_i - x_j)^2 \right) \exp\left( -\frac{(x_i - x_j)^2}{2\ell^2} \right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is enough information to be able to draw random samples from the derivative of a GP. Let’s write the new kernel &lt;span class=&#34;math inline&#34;&gt;\(k_11\)&lt;/span&gt; in R, and sample from the derivative GP:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;k_11 &amp;lt;- function(x_i, x_j, alpha, l) {
  alpha^2 / l^4 * (l^2 - (x_i - x_j)^2) * exp(-(x_i - x_j)^2 / (2*l^2))
}

n_prime &amp;lt;- 100
x_prime &amp;lt;- seq(1, 10, length.out = n_prime)

# Draw 10 samples
Sigma_prime &amp;lt;- covariance_from_kernel(x_prime, k_11, alpha = alpha, l = l)
y_prime &amp;lt;- gp_draw(10, x_prime, Sigma_prime)

matplot(x_prime, t(y_prime), type = &amp;#39;l&amp;#39;, xlab = &amp;#39;x&amp;#39;, ylab = &amp;quot;y&amp;#39;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-07-06-gaussian-process-derivatives.en_files/figure-html/draws_from_derivative_gp-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It’s hard to interpret this plot because we can’t compare these derivative values to the corresponding normal GP. Fortunately, it’s possible to sample from the joint distributions of the observations and its derivatives, which is in fact also a GP. To define it, we need to know the covariance between an observation and a derivative observation.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(k_{01}(x_i, x_j)\)&lt;/span&gt; be the covariance between an observation at &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; and a derivative observation at &lt;span class=&#34;math inline&#34;&gt;\(x_j\)&lt;/span&gt;. Then
&lt;span class=&#34;math display&#34;&gt;\[
k_{01}(x_i, x_j) = \frac{\alpha^2}{\ell^2} (x_i - x_j) \exp\left( -\frac{(x_i - x_j)^2}{2\ell^2} \right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Similarly, &lt;span class=&#34;math inline&#34;&gt;\(k_{10}(x_i, x_j)\)&lt;/span&gt; is the covariance between a derivative observation at &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; and an observation at &lt;span class=&#34;math inline&#34;&gt;\(x_j\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[
k_{10}(x_i, x_j) = \frac{\alpha^2}{\ell^2} (x_j - x_i) \exp\left( -\frac{(x_i - x_j)^2}{2\ell^2} \right)
\]&lt;/span&gt;
As we would expect from the symmetry of covariance matrices, &lt;span class=&#34;math inline&#34;&gt;\(k_{01}(x_i, x_j) = k_{10}(x_j, x_i)\)&lt;/span&gt;. That means we can get away with just defining one of them in R:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;k_01 &amp;lt;- function(x_i, x_j, alpha, l) {
  alpha^2 / l^2 * (x_i - x_j) * exp(-(x_i - x_j)^2 / (2*l^2))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now construct a combined vector by concatenating the observations &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and derivative observations &lt;span class=&#34;math inline&#34;&gt;\(y^\prime\)&lt;/span&gt;, denoted &lt;span class=&#34;math inline&#34;&gt;\(y^\mathrm{all}\)&lt;/span&gt;, of length &lt;span class=&#34;math inline&#34;&gt;\(n + n^\prime\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\bm{x}^{\mathrm{all}}\)&lt;/span&gt; be the corresponding positions of each observation in &lt;span class=&#34;math inline&#34;&gt;\(\bm{y}^{\mathrm{all}}\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\bm{d}^{\mathrm{all}}\)&lt;/span&gt; be a &lt;span class=&#34;math inline&#34;&gt;\(n+n^\prime\)&lt;/span&gt; length vector where &lt;span class=&#34;math inline&#34;&gt;\(d^\mathrm{all}_i = 1\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(y^{\mathrm{all}}_i\)&lt;/span&gt; is a derivative observation, and &lt;span class=&#34;math inline&#34;&gt;\(d^{\mathrm{all}}_i = 0\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(y^{\mathrm{all}}_i\)&lt;/span&gt; is a normal observation. Then define a new kernel over the joint observations:
&lt;span class=&#34;math display&#34;&gt;\[
k^{\mathrm{all}}(x_i, x_j, d_i, d_j) = \begin{cases}
  k(x_i, x_j) &amp;amp; d_i = 0, d_j = 0 \text{ (both normal observations)} \\
  k_{01}(x_i, x_j) &amp;amp; d_i = 0, d_j = 0 \text{ (one derivative, one normal)} \\
  k_{10}(x_i, x_j) &amp;amp; d_i = 1, d_j = 0 \text{ (one derivative, one normal)} \\
  k_{11}(x_i, x_j) &amp;amp; d_i = 1, d_j = 0 \text{ (both derivatives)}
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In R:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;k_all &amp;lt;- function(x_i, x_j, d_i, d_j, ...) {
  dplyr::case_when(
    d_i == 0 &amp;amp; d_j == 0 ~ k(x_i, x_j, ...),
    d_i == 0 &amp;amp; d_j == 1 ~ k_01(x_i, x_j, ...),
    d_i == 1 &amp;amp; d_j == 0 ~ k_01(x_j, x_i, ...),
    d_i == 1 &amp;amp; d_j == 1 ~ k_11(x_i, x_j, ...),
  )
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We also need a new function to form the joint covariance matrix:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;joint_covariance_from_kernel &amp;lt;- function(x, d, kernel, ...) {
  outer(1:length(x), 1:length(x),
        function(i, j) kernel(x[i], x[j], d[i], d[j], ...))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, I’m going to split out the plotting code into a separate function as it gets more complicated than before:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_joint_gp &amp;lt;- function(x, y, d) {
  plot(x[d == 0], y[d == 0], type = &amp;#39;l&amp;#39;, ylim = range(y), 
       col = &amp;#39;black&amp;#39;, xlab = &amp;#39;x&amp;#39;, ylab = &amp;#39;y&amp;#39;)
  lines(x[d == 1], y[d == 1], type = &amp;#39;l&amp;#39;, col = &amp;#39;blue&amp;#39;, lty = 2)
  abline(h = 0, lty = 3, col = &amp;quot;gray&amp;quot;)
  legend(&amp;#39;topright&amp;#39;, legend = c(&amp;quot;GP&amp;quot;, &amp;quot;Derivative of GP&amp;quot;),
         col = c(&amp;quot;black&amp;quot;, &amp;quot;blue&amp;quot;), lty = 1:2)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can sample from the joint distribution of the observations and derivatives:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x_all &amp;lt;- c(x, x_prime)
d_all &amp;lt;- c(rep(0, length(x)), rep(1, length(x_prime)))

Sigma_all &amp;lt;- joint_covariance_from_kernel(x_all, d_all, k_all, alpha = alpha, l = l)
y_all &amp;lt;- gp_draw(1, x_all, Sigma_all)

plot_joint_gp(x_all, y_all[1,], d_all)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-07-06-gaussian-process-derivatives.en_files/figure-html/draw_from_joint_gp-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let’s plot one more:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y_all &amp;lt;- gp_draw(1, x_all, Sigma_all)
plot_joint_gp(x_all, y_all[1,], d_all)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-07-06-gaussian-process-derivatives.en_files/figure-html/draw_from_joint_gp2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So far we’ve seen how derivatives of GPs are defined, and how to draw from the joint distribution of a GP and its derivative. In future posts we’ll look at fitting GPs in Stan with derivative observations, and at shape-constrained GPs.&lt;/p&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Andrew McHutchon, &lt;a href=&#34;http://mlg.eng.cam.ac.uk/mchutchon/DifferentiatingGPs.pdf&#34;&gt;Differentiating Gaussian Processes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Rasmussen et al. 2006, &lt;em&gt;Gaussian Processes for Machine Learning&lt;/em&gt;, &lt;a href=&#34;http://www.gaussianprocess.org/gpml/chapters/RW9.pdf&#34;&gt;Chapter 9&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Firearm Background Check Timeseries Modeling</title>
      <link>/2020/02/01/firearm-background-check-timeseries-modeling/</link>
      <pubDate>Sat, 01 Feb 2020 00:00:00 +0000</pubDate>
      <guid>/2020/02/01/firearm-background-check-timeseries-modeling/</guid>
      <description>


&lt;p&gt;Gun dealers in the U.S. are required to conduct instant background checks before selling weapons to individuals. The FBI &lt;a href=&#34;https://www.fbi.gov/file-repository/nics_firearm_checks_-_month_year.pdf/view&#34;&gt;provides data&lt;/a&gt; for the number of these background checks performed by month/year, which serves as a proxy for the total number of gun sales in the U.S.&lt;/p&gt;
&lt;p&gt;I brought the data into R for a quick and dirty analysis, with the intent of finding spikes in background checks around major events.&lt;/p&gt;
&lt;p&gt;First, let’s take a look raw data. There are a few obvious spikes in the later years, which correspond to the Sandy Hook (December 2012) and San Bernadino (December 2015) shootings.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-01-firearm-background-check-timeseries-modeling/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Next, I fit a negative binomial generalized linear model that accounts for an overall trend using a 3rd order cubic spline and monthly seasonal variation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model &amp;lt;- MASS::glm.nb(value ~ bs(date) + month, dat)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Even such a simple model does a decent job fitting the data, although it gets much worse in later years as the variance in the data increases:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-01-firearm-background-check-timeseries-modeling/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;More interesting is the plot of the residuals from the model, which show spikes in background checks that aren’t accounted for by the model. This makes a couple of other peaks jump out that are correlated with notable events, like 9/11 and Obama’s election:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-01-firearm-background-check-timeseries-modeling/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are a few other peaks that I don’t have explanations for, like in late 1999 and in the beginning of 2014.&lt;/p&gt;
&lt;p&gt;I think it’s interesting how the model residuals let us see spikes in background checks that we couldn’t see in the raw data. The tradeoff is that the model residuals are conditional on the model choice; choosing a different model might lead to a different plot. If we want to answer the question “were there spikes in gun background checks”, we now have to condition our conclusions on that model choice, which complicates interpretation.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hello Estimating Equations</title>
      <link>/2019/11/18/hello-estimating-equations/</link>
      <pubDate>Mon, 18 Nov 2019 00:00:00 +0000</pubDate>
      <guid>/2019/11/18/hello-estimating-equations/</guid>
      <description>


&lt;p&gt;This post gives a “Hello world” example for estimating equations, illustrating with a small simulation study why it is a useful approach for inference.&lt;/p&gt;
&lt;p&gt;Suppose we have &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; observations of data, &lt;span class=&#34;math inline&#34;&gt;\(Y_1, \dots, Y_n\)&lt;/span&gt;. We assume that they are independent and identically distributed according to a normal distribution:
&lt;span class=&#34;math display&#34;&gt;\[
Y_i \sim \mathrm{N}(\mu, \sigma^2)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can derive maximum likelihood estimators for &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
  \hat\mu_{MLE}      &amp;amp;= \frac{1}{n} \sum_{i=1}^n Y_i \\
  \hat\sigma^2_{MLE} &amp;amp;= \frac{1}{n} \sum_{i=1}^n (Y_i - \hat\mu_{MLE})^2
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can also find that &lt;span class=&#34;math inline&#34;&gt;\(\hat\mu_{MLE}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat\sigma^2_{MLE}\)&lt;/span&gt; are asymptotically independent and normally distributed:
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
  \hat\mu_{MLE}      &amp;amp;\sim \mathrm{N}(\mu, \frac{\sigma^2}{n}) \\
  \hat\sigma^2_{MLE} &amp;amp;\sim \mathrm{N}(\sigma^2, \frac{2(\sigma^2)^2}{n})
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;as long as the observed data actually follows a normal distribution, these estimators will have the lowest variance of any consistent estimators of &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;But what if the data aren’t actually normally distributed? Estimating equations provide a more general way to estimate the asymptotic variance of estimators when distributional assumptions about the data do not hold.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Presidential Primary Polling Analysis in Stan</title>
      <link>/2019/08/10/presidential-primary-polling-analysis-in-stan/</link>
      <pubDate>Sat, 10 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/2019/08/10/presidential-primary-polling-analysis-in-stan/</guid>
      <description>


&lt;p&gt;I often use random walk/autoregressive models in my research as a component in time-series analysis, and I wanted to get some more experience fitting them to data. &lt;a href=&#34;https://fivethirtyeight.com&#34;&gt;FiveThirtyEight&lt;/a&gt; publishes several &lt;a href=&#34;https://projects.fivethirtyeight.com/polls&#34;&gt;polling datasets&lt;/a&gt;, including polling for the 2020 Democratic presidential primary. I used Stan to fit a Bayesian random walk model to the polling data, which I describe below. The Stan and R code used in this post is available as a &lt;a href=&#34;https://gist.github.com/herbps10/d274d3d9c579e4e9eb5c16a16949c315&#34;&gt;Github gist&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\delta_{c,t}\)&lt;/span&gt; be the true proportion of voters in favor of candidate &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; at time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;. Our modeling assumption is that the logit-transform of &lt;span class=&#34;math inline&#34;&gt;\(\delta_{c,t}\)&lt;/span&gt; follows a random walk; that is:
&lt;span class=&#34;math display&#34;&gt;\[
\mathrm{logit}(\delta_{c,t}) \sim \mathrm{N}\left(\mathrm{logit}(\delta_{c,t-1}), \tau^2\right)
\]&lt;/span&gt;
We can’t observe &lt;span class=&#34;math inline&#34;&gt;\(\delta_{c,t}\)&lt;/span&gt; directly; we have to infer it through the noisy observations we have from polls.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(s_{i}\)&lt;/span&gt; be the sample size of poll &lt;span class=&#34;math inline&#34;&gt;\(c[i]\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y_{i}\)&lt;/span&gt; the number of poll respondents in favor of candidate &lt;span class=&#34;math inline&#34;&gt;\(c[i]\)&lt;/span&gt; at time &lt;span class=&#34;math inline&#34;&gt;\(t[i]\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\phi_i\)&lt;/span&gt; be the proportion of poll respondents in favor of candidate &lt;span class=&#34;math inline&#34;&gt;\(c[i]\)&lt;/span&gt; at time &lt;span class=&#34;math inline&#34;&gt;\(t[i]\)&lt;/span&gt;. To incorporate sampling error, we model &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; as binomial:
&lt;span class=&#34;math display&#34;&gt;\[
y_i \sim \mathrm{Binomial}(s_i, \phi_i)
\]&lt;/span&gt;
We also allow for added variance in our observations by relating &lt;span class=&#34;math inline&#34;&gt;\(\phi_i\)&lt;/span&gt; to the true logit proportion &lt;span class=&#34;math inline&#34;&gt;\(\delta_{c[i], t[i]}\)&lt;/span&gt; with a normal distribution:
&lt;span class=&#34;math display&#34;&gt;\[
\mathrm{logit}(\phi_i) \sim \mathrm{N}(\mathrm{logit}(\delta_{c[i], t[i]}), \sigma^2)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To finish defining the model half-normal priors on the hyperparameters. The prior for &lt;span class=&#34;math inline&#34;&gt;\(\tau^2\)&lt;/span&gt; has a small variance to improve identification of the model (a vaguer prior can cause the MCMC chains to not mix well.)
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
  \tau^2 &amp;amp;\sim \mathrm{N}(0, 0.02)[0, \infty] \\
  \sigma^2 &amp;amp;\sim \mathrm{N}(0, 1)[0, \infty] 
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here is the Stan representation of the statistical model:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;S4 class stanmodel &amp;#39;random_walk&amp;#39; coded as follows:
data {
  int&amp;lt;lower=0&amp;gt; T; // Number of timepoints
  int&amp;lt;lower=0&amp;gt; C; // Number of candidates
  int&amp;lt;lower=0&amp;gt; N; // Number of poll observations
  
  int sample_size[N]; // Sample size of each poll
  int y[N]; // Number of respondents in poll for candidate (approximate)
  int&amp;lt;lower=1, upper=T&amp;gt; get_t_i[N]; // timepoint for ith observation
  int&amp;lt;lower=1, upper=C&amp;gt; get_c_i[N]; // candidate for ith observation
}
parameters {
  matrix[C, T] delta_logit; // Percent for candidate c at time t
  real&amp;lt;lower=0, upper=1&amp;gt; phi[N]; // Percent of participants in poll for candidate
  real&amp;lt;lower=0&amp;gt; tau; // Random walk variance
  real&amp;lt;lower=0,upper=0.5&amp;gt; sigma; // Overdispersion of observations
}
model {
  // Priors
  tau ~ normal(0, 0.2);
  sigma ~ normal(0, 1);
  
  // Random walk
  for(c in 1:C) {
    delta_logit[c, 2:T] ~ normal(delta_logit[c, 1:(T - 1)], tau);
  }
  
  // Observed data
  y ~ binomial(sample_size, phi);
  for(i in 1:N) {
    // Overdispersion
    delta_logit[get_c_i[i], get_t_i[i]] ~ normal(logit(phi[i]), sigma);
  }
}
generated quantities {
  matrix[C, T] delta = inv_logit(delta_logit);
} &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The raw dataset that we are going to fit:
&lt;img src=&#34;/post/2019-08-10-presidental-primary-polling-analysis-in-stan/index_files/figure-html/raw_data-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I fitted the Stan model to the data using the standard HMC-NUTS algorithm and 1000 MCMC iterations. The plot below shows the posterior median with 75% and 95% credible intervals.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-08-10-presidental-primary-polling-analysis-in-stan/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;One issue with this model is that it oversmooths large bumps in the polls. For example, Harris had a bump after the first debate, which the model smooths into an uptick leading into the debate that is not justified in the data. The model could be improved by allowing for these shocks, for example by restarting the random walk after key dates like the debates which we know are likely to cause discontinuities in the results.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Autoregressive Processes are Gaussian Processes</title>
      <link>/2019/08/09/autoregressive-processes-are-gaussian-processes/</link>
      <pubDate>Fri, 09 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/2019/08/09/autoregressive-processes-are-gaussian-processes/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Autoregressive_model&#34;&gt;Autoregressive (AR) processes&lt;/a&gt; are a popular choice for modeling time-varying processes. AR processes are typically written down as a set of conditional distributions, but if we do some algebra we can show how they can also be written as a &lt;a href=&#34;https://en.wikipedia.org/wiki/Gaussian_process&#34;&gt;Gaussian process&lt;/a&gt;. One reason having a Guassian process representation is useful is because it makes it more clear how an AR process can be incorporated into larger models, like a spatio-temporal model. In this post, we’ll start with defining an AR process and deriving its mean and variance, then we’ll derive its joint distribution, which is a Gaussian process.&lt;/p&gt;
&lt;div id=&#34;ar-processes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;AR processes&lt;/h2&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Y} = \left\{ Y_1, Y_2, \dots, Y_n \right\}\)&lt;/span&gt; be a set of random variables indexed by time. An aurogressive model assumes that &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Y}\)&lt;/span&gt; is correlated over time. An AR model is typically described by defining &lt;span class=&#34;math inline&#34;&gt;\(Y_t\)&lt;/span&gt; in terms of &lt;span class=&#34;math inline&#34;&gt;\(Y_{t-1}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y_{t} = \rho Y_{t-1} + \epsilon_{t}
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{t}\sim N\left(0,\sigma_{\epsilon}^{2}\right)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\rho \in \mathbb{R}\)&lt;/span&gt; is a parameter that controls the degree to which &lt;span class=&#34;math inline&#34;&gt;\(Y_t\)&lt;/span&gt; is correlated with &lt;span class=&#34;math inline&#34;&gt;\(Y_{t-1}\)&lt;/span&gt;. This model is called an AR process of order 1 because &lt;span class=&#34;math inline&#34;&gt;\(Y_t\)&lt;/span&gt; only depends on &lt;span class=&#34;math inline&#34;&gt;\(Y_{t-1}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We can also rearrange terms to emphasize that this representation defines the conditional distribution of &lt;span class=&#34;math inline&#34;&gt;\(Y_{t}\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(Y_{t-1}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
Y_{t} \vert Y_{t-1} \sim&amp;amp; N(\rho Y_{t-1}, \sigma_\epsilon^2) \\
Y_1 \sim&amp;amp; N(0, \frac{\sigma_\epsilon^2}{1-\rho^2})
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where the variance of &lt;span class=&#34;math inline&#34;&gt;\(Y_1\)&lt;/span&gt; comes from the unconditional variance, which is derived below. The stationarity condition of an AR process is that each &lt;span class=&#34;math inline&#34;&gt;\(Y_t\)&lt;/span&gt; has the same distribution; that is, &lt;span class=&#34;math inline&#34;&gt;\(\mu = \mathrm{E}(Y_i) = \mathrm{E}Y_j\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2 = \mathrm{Var}(Y_i) = \mathrm{Var}(Y_j)\)&lt;/span&gt; for all &lt;span class=&#34;math inline&#34;&gt;\(i, j\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Now we can derive the unconditional mean and variance of &lt;span class=&#34;math inline&#34;&gt;\(Y_t\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\mathrm{E}\left(Y_{t}\right) &amp;amp;= \mathrm{E}\left(\rho Y_{t - 1} + \epsilon_{t} \right)\\
                     &amp;amp;= \rho \mathrm{E}\left( Y_{t - 1 } \right)\\
                 \mu &amp;amp;= \rho\mu\ \text{(apply stationarity condition)} \\
                 \mu &amp;amp;= 0 \\
\mathrm{Var}\left(Y_{t}\right) &amp;amp;= \mathrm{Var}\left(\rho Y_{t-1} + \epsilon_{t}\right)\\
                                   &amp;amp;= \rho^{2}\mathrm{Var}(Y_{t-1}) + \mathrm{Var}\left(\epsilon_{t}\right)\\
                                   &amp;amp;= \rho^{2}\mathrm{Var}(Y_{t-1}) + \sigma_{\epsilon}^{2}\\
                        \sigma^{2} &amp;amp;= \rho^{2}\sigma^{2} + \sigma_{\epsilon}^{2}\ \text{(apply stationarity condition)}\\
 \sigma^{2}\left(1-\rho^{2}\right) &amp;amp;= \sigma_{\epsilon}^{2}\\
                        \sigma^{2} &amp;amp;= \frac{\sigma_{\epsilon}^{2}}{1 - \rho^{2}}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The plot below shows several examples of draws from an AR(1) process with differing values of &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_\epsilon = 1\)&lt;/span&gt;:
&lt;img src=&#34;/post/2019-08-06-ar-process_files/figure-html/ar_1_conditional_representation-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gaussian-processes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Gaussian processes&lt;/h2&gt;
&lt;p&gt;Gaussian processes model a set of variables as being multivariate normally distributed with mean &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\mu}\)&lt;/span&gt; and variance/covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\Sigma}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbf{Y} \sim MVN(\boldsymbol{\mu}, \boldsymbol{\Sigma})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Usually the mean vector is set to &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{0}\)&lt;/span&gt;, which means the Gaussian process is fully defined by its choice of variance/covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\Sigma}\)&lt;/span&gt;. The variance/covariance matrix is defined by a kernel function which defines the covariance between any two variables:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\Sigma_{i,j} = K(i, j)
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;an-ar1-process-is-a-gaussian-process&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;An AR(1) process is a Gaussian process&lt;/h2&gt;
&lt;p&gt;We want to show that an AR process can be represented as a Gaussian process. To do this, we need to show that &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Y}\)&lt;/span&gt; is jointly normally distributed with some mean vector and variance/covariance matrix.&lt;/p&gt;
&lt;p&gt;We already know that &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{E}(Y_t)=0\)&lt;/span&gt;, so the mean vector of its joint distribution will be &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{0}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;To find the variance/covariance matrix, we need to derive the covariance between &lt;span class=&#34;math inline&#34;&gt;\(Y_{t_1}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y_{t_2}\)&lt;/span&gt;. First, let’s consider the simpler case of the covariance between &lt;span class=&#34;math inline&#34;&gt;\(Y_t\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y_{t+1}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
    \mathrm{cov}(Y_{t}, Y_{t+1}) &amp;amp;= \mathrm{E} \left[ \left( Y_t - \mathrm{E}[Y_t] \right) \left( Y_{t+1} - \mathrm{E}[Y_{t+1}] \right) \right] \text{ (definition of covariance) } \\
                   &amp;amp;= \mathrm{E} \left[ Y_t Y_{t+1} \right] \text{ (because } \mathrm{E}[Y_t] = \mathrm{E}[Y_{t+1}] = 0 \text{)} \\
                   &amp;amp;= \mathrm{E} \left[ Y_t \left( \rho Y_{t} + \epsilon_{t+1} \right) \right] \\
                   &amp;amp;= \mathrm{E} \left[ \rho Y_t^2 + Y_t \epsilon_{t+1} \right] \\
                   &amp;amp;= \rho \mathrm{E}\left[ Y_t^2 \right] \\
                   &amp;amp;= \rho (\mathrm{Var}(Y_t) + \mathrm{E}[Y_t]^2) \\
                   &amp;amp;= \rho \frac{\sigma_\epsilon^2}{1 - \rho^2}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;for &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;s separated by more than one time point, iterating the above result yields the expression&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
  \mathrm{cov}(Y_{t_1}, Y_{t_2}) = \rho^{\vert t_1 - t_2 \vert} \frac{\sigma_\epsilon^2}{1 - \rho^2}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now we can fully define the joint distribution of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Y}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbf{Y} \sim MVN(\mathbf{0}, \boldsymbol{\Sigma})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_{i,j} = \rho^{\vert i - j \vert} \frac{\sigma_\epsilon^2}{1-\rho^2}\)&lt;/span&gt;. This is a Gaussian process!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-08-06-ar-process_files/figure-html/ar_process_gaussian_representation-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;combining-kernel-functions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Combining kernel functions&lt;/h2&gt;
&lt;p&gt;The nice thing about Gaussian processes is that we can combine multiple kernel functions to model processes with dependence from different sources. Two ways kernels can be combined are by multiplication and addition. Multiplying two kernels is like an “AND” operation: the correlation between points will be high if the correlation from both kernels is high. Adding two kernels together is like an “OR” operation: correlation is high if either kernel indicates high covariance.&lt;/p&gt;
&lt;p&gt;As an example, let’s build a Gaussian process that combines an AR process (for temporal correlation) and a spatial process (for spatial correlation) by combining two kernel functions. First, we need to define an outcome variable &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; that varies in time and space: let &lt;span class=&#34;math inline&#34;&gt;\(Y_{c,t}\)&lt;/span&gt; be a random variable indexed by spatial site &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; at timepoint &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;. We take the AR covariance as the first kernel function, to model temporal correlation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
K_1(i, j) = \rho^{\vert t_i - t_j \vert} \frac{\sigma_\epsilon^2}{1 - \rho^2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and a squared-exponential kernel function to model spatial dependence:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
K_2(i, j) = \alpha^2 \exp\left( -\frac{d(i, j)}{2\lambda^2} \right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(d(i, j)\)&lt;/span&gt; is the spatial distance between sites &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; is a length-scale parameter, and &lt;span class=&#34;math inline&#34;&gt;\(\alpha^2\)&lt;/span&gt; is a parameter controlling the magnitude of the covariance.&lt;/p&gt;
&lt;p&gt;Combine the two kernel functions so that two data points are correlated if they are close together in time and space:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
K(i, j) &amp;amp;= K_1(i, j) \times K_2(i, j) \\
        &amp;amp;= \rho^{\vert t_i - t_j \vert} \frac{\sigma_\epsilon^2}{1 - \rho^2} \alpha^2 \exp\left( -\frac{d(i, j)}{2\lambda^2} \right)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note the parameters &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_\epsilon\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\alpha^2\)&lt;/span&gt;, which are multipled together, would be unidentifiable in parameter estimation and should be replaced by a single parameter that controls the magnitude of the covariance.&lt;/p&gt;
&lt;p&gt;To illustrate this Gaussian process model, I started by generating a set of sites with random locations:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-08-06-ar-process_files/figure-html/spatial_locations-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;then I drew from the Gaussian process using the parameters temporal parameters &lt;span class=&#34;math inline&#34;&gt;\(\rho=0.9\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\epsilon^2=1\)&lt;/span&gt; and spatial parameters &lt;span class=&#34;math inline&#34;&gt;\(\alpha = 1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\lambda=2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The plot below shows the time trend in the first six sites:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-08-06-ar-process_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And the spatial distribution over time of &lt;span class=&#34;math inline&#34;&gt;\(Y_{c,t}\)&lt;/span&gt; is shown below:
&lt;img src=&#34;/post/2019-08-06-ar-process_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Visually we can see that the Gaussian process generates data that is correlated in both time and space.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;modeling-using-the-mean-and-the-covariance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Modeling using the mean and the covariance&lt;/h2&gt;
&lt;p&gt;The spatio-temporal Gaussian process we defined in the previous section does its modeling through the variance/covariance matrix, with its mean function set to zero. An alternative way to think about a spatio-temporal process is akin to the first AR representation we looked at, and define &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Y}_t\)&lt;/span&gt; (the set of all &lt;span class=&#34;math inline&#34;&gt;\(Y_{c,t}\)&lt;/span&gt; at time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;) relative to &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Y}_{t-1}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
  \mathbf{Y}_{t} = \rho \mathbf{Y}_{t-1} + \boldsymbol{\epsilon}_t
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\epsilon_t} \sim MVN(\mathbf{0}, \boldsymbol{\Sigma}_\epsilon)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;If we set &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\Sigma_\epsilon}\)&lt;/span&gt; to be the diagonal matrix &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\Sigma}_\epsilon = \sigma^2_\epsilon \mathbf{I}_n\)&lt;/span&gt; then we will have an independent AR(1) independent process for each spatial site. It gets more interesting if we define &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\Sigma}_\epsilon\)&lt;/span&gt; by a covariance function so we can include dependence between sites, for example dependence based on the distance between the sites. For now, let’s use the squared exponential kernel and define &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_{i,j} = \alpha^2 \exp\left(-\frac{d(i, j)}{2\lambda^2} \right)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Is this process also equivalent to a mean zero Gaussian process with some covariance kernel? We’ll answer this question by deriving the covariance between any two points.&lt;/p&gt;
&lt;p&gt;The mean of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Y_t}\)&lt;/span&gt; can be shown to be zero in the same way we showed a univariate AR process has mean 0. We also need to know the overall variance/covariance matrix of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Y}_t\)&lt;/span&gt;, which we’ll call &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\Phi}\)&lt;/span&gt;; the logic is imilar to the univariate case, and I’ll show it here for completeness:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
    \mathrm{Var}\left(\boldsymbol{Y}_{t}\right) &amp;amp; =\mathrm{Var}\left(\rho^{2}\mathbf{Y}_{t-1} + \boldsymbol{\epsilon}_{t}\right) \\
     &amp;amp;= \rho^{2}\mathrm{Var}\left(\boldsymbol{Y}_{t-1}\right)+\mathrm{Var}\left(\boldsymbol{\epsilon}_{t}\right) \\
    \boldsymbol{\Phi} &amp;amp; =\rho^{2}\boldsymbol{\Phi}+\boldsymbol{\Sigma}_\epsilon \\
    \boldsymbol{\Phi}-\rho^{2}\boldsymbol{\Sigma} &amp;amp;= \boldsymbol{\Sigma}_\epsilon \\
    \boldsymbol{\Phi}\left(\mathbf{I}-\rho^{2}\mathbf{I}\right) &amp;amp; =\boldsymbol{\Sigma}_\epsilon \\
    \boldsymbol{\Phi} &amp;amp;=\boldsymbol{\Sigma}_{\epsilon}\left(\mathbf{I}-\rho^{2}\mathbf{I}\right)^{-1}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If we pull out two sites at the same time point, their covariance is &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{cov}(Y_{t,c_1}, Y_{t,c_2}) = \frac{\Sigma_{\epsilon, c_1, c_2}}{1-\rho^2}\)&lt;/span&gt;, which looks very similar to the unidimensional AR(1) process variance.&lt;/p&gt;
&lt;p&gt;Now we derive the covariance between any two sites that are one time point apart:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\mathrm{cov}\left(y_{c_1,t},y_{c_2,t+1}\right) &amp;amp; =\mathrm{E}\left[\left(y_{c_1,t}-\mathrm{E}\left[y_{c_1,t}\right]\right)\left(y_{c_2,t}-\mathrm{E}\left[y_{c_2,t}\right]\right)\right]\\
 &amp;amp; =\mathrm{E}\left[y_{c_1,t}y_{c_2,t}\right]\\
 &amp;amp; =\mathrm{E}\left[y_{c_1,t}\left[\rho y_{c_2,t}+\epsilon_{c_2,t+1}\right]\right]\\
 &amp;amp; =\rho\mathrm{E}\left[y_{c_1,t}y_{c_2,t}\right]\\
 &amp;amp; =\rho\mathrm{cov}\left(y_{c_1,t}y_{c_2,t}\right)\\
 &amp;amp; =\rho\frac{\Sigma_{i,j}}{1-\rho^2} \\
 &amp;amp;= \rho \frac{1}{1-\rho^2} \Sigma_{i,j} \\
 &amp;amp;= \rho \frac{1}{1-\rho^2} \alpha^2 \exp\left(-\frac{d(i, j)}{2\lambda^2} \right)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;for sites more than one time point away from each other, we can iterate the above result to get a general expression of the covariance between any two points:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\mathrm{cov}\left(y_{c_1,t_1},y_{c_2,t_2}\right) &amp;amp;= \rho^{\vert t_1 - t_2 \vert}\frac{1}{1-\rho^2} \alpha^2 \exp\left(-\frac{d(i, j)}{2\lambda^2} \right)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;if we reparameterize &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; to be the product of two parameters &lt;span class=&#34;math inline&#34;&gt;\(\alpha = \sigma^2_\epsilon \alpha\)&lt;/span&gt;, we get&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\mathrm{cov}\left(y_{c_1,t_1},y_{c_2,t_2}\right) &amp;amp;= \rho^{\vert t_1 - t_2 \vert}\frac{\sigma^2_\epsilon}{1-\rho^2} \alpha^2 \exp\left(-\frac{d(i, j)}{2\lambda^2} \right) \\
&amp;amp;= K_1(i, j) \times K_2(i,j)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which is the product of an AR(1) and squared exponential kernel function as defined in the previous section. In practice we wouldn’t want to separate these parameters because both of them will not be identifiable given observed data, but I separated them here to show how the covariance structure is the product of two kernel functions.&lt;/p&gt;
&lt;p&gt;Therefore, we can write this process in the form of a Gaussian process with mean zero and covariance kernel given by the product of a temporal and spatial kernel:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\mathbf{Y} \sim&amp;amp; MVN(\mathbf{0}, \boldsymbol{\Sigma}) \\
\Sigma_{i,j} =&amp;amp; K_1(i, j) \times K_2(i, j) 
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The spatio-temporal processes defined as a set of conditional distributions and as a Gaussian process are equivalent.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;To summarize, AR processes can be written as a Gaussian process model, which is useful because a temporal process can then be easily combined with other sources of dependence. In general, we can build our models by defining conditional distributions with a given mean and covariance, or a joint distribution with mean zero where the model is fully defined by a variance/covariance kernel function. In a future post I will look at Bayesian parameter estimation in these models using Stan.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Using R formulas to pass data to Stan</title>
      <link>/2019/07/22/r-formulas-stan/</link>
      <pubDate>Mon, 22 Jul 2019 18:00:00 +0000</pubDate>
      <guid>/2019/07/22/r-formulas-stan/</guid>
      <description>


&lt;p&gt;Many statistical routines in R use &lt;a href=&#34;https://stat.ethz.ch/R-manual/R-devel/library/stats/html/formula.html&#34;&gt;R formulas&lt;/a&gt; as a flexible way to specify the terms of a model. With a little setup, we can use formulas to build inputs to &lt;a href=&#34;http://mc-stan.org&#34;&gt;Stan&lt;/a&gt; and avoid hard-coding any variables in the model.&lt;/p&gt;
&lt;p&gt;For example, say you are writing a Stan model for linear regression. You would like to regress a response variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; on two predictors, &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y_1\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// linear_regression.stan
data {
  int N; // Number of observations
  vector[N] y;
  vector[N] x1;
  vector[N] x2;
} 
parameters {
  real beta_0;
  real beta_1;
  real beta_2;
  real&amp;lt;lower=0&amp;gt; sigma;
} 
model {
  y ~ normal(beta_0 + beta_1 * x1 + beta_2 * x2, sigma);
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But what if you later decide to add more predictors? We can make the above model more flexible by &lt;a href=&#34;https://mc-stan.org/docs/2_19/stan-users-guide/linear-regression.html#vectorization.section&#34;&gt;allowing a matrix of predictors&lt;/a&gt; &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; of arbitrary size:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// linear_regression.stan
data {
  int N; // Number of observations
  int K; // Number of predictors
  vector[N] y;
  matrix[N, K] X;
} 
parameters {
  vector[K] beta;
  real&amp;lt;lower=0&amp;gt; sigma;
} 
model {
  y ~ normal(beta * X, sigma);
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we can use R formulas to build the predictor matrix &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and pass it to Stan:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_linear_regression &amp;lt;- function(formula, data, ...) {
  model &amp;lt;- stan_model(&amp;quot;./linear_regression.stan&amp;quot;)

  X &amp;lt;- model.matrix(formula, data)
  y &amp;lt;- model.extract(model.frame(formula, data), &amp;quot;response&amp;quot;)
  
  data &amp;lt;- list(
    N = nrow(X),
    K = ncol(X),
    X = X,
    y = y
  )

  sampling(model, data, ...)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now it’s easy to fit the model with different predictors:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;N &amp;lt;- 100
simulated_data &amp;lt;- tibble::tibble(
  x1 = rnorm(N, 0, 1),
  x2 = rnorm(N, 0, 1),
  y = x1 + 2*x2 + rnorm(N, 0, 0.1)
)


fit_linear_model(y ~ x1, simulated_data)
fit_linear_model(y ~ x1 + x2, simulated_data)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Writing a separate function for preparing the data for Stan based on a formula makes the model more usable and flexible.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Smartphone interface for reporting research results to study participants</title>
      <link>/2019/04/08/smartphone-interface-for-reporting-research-results-to-study-participants/</link>
      <pubDate>Mon, 08 Apr 2019 18:00:00 +0000</pubDate>
      <guid>/2019/04/08/smartphone-interface-for-reporting-research-results-to-study-participants/</guid>
      <description>&lt;p&gt;One of the themes that I worked on at &lt;a href=&#34;https://silentspring.org&#34; target=&#34;_blank&#34;&gt;Silent Spring Institute&lt;/a&gt; was on how to report complex personal data to our study participants. In the &lt;a href=&#34;https://web.northeastern.edu/protect/&#34; target=&#34;_blank&#34;&gt;PROTECT&lt;/a&gt; study, mothers in Puerto Rico were tested for a host of environmental chemicals. Our job was to design a tool to report-back individual results to the participants.&lt;/p&gt;

&lt;p&gt;While I was on the project, I designed and implemented a novel smartphone interface for communicating personal results to study participants. One aspect was designing a visualization that allowed participants to compare their results to other women in the study. Our approach uses a &lt;a href=&#34;https://www.biorxiv.org/content/10.1101/028191v1.article-info&#34; target=&#34;_blank&#34;&gt;SinaPlot&lt;/a&gt; where the participant&amp;rsquo;s personal results are represented by an avatar that they chose when they enter their report.&lt;/p&gt;

&lt;p&gt;Last May I left Silent Spring Institute to pursue graduate school, and I was happy to see that the tool was launched in October! To learn more, check out &lt;a href=&#34;https://web.northeastern.edu/protect/protects-community-engagement-core-and-silent-spring-institute-inaugurate-a-mobile-application-to-provide-protect-participants-their-research-results/&#34; target=&#34;_blank&#34;&gt;the article&lt;/a&gt; the PROTECT team wrote about the launch of the reports.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/phthalates-screenshots.jpg&#34; alt=&#34;PROTECT Smartphone Interface&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>What Poisons Are in Your Body? - Nick Kristof</title>
      <link>/2018/02/25/what-poisons-are-in-your-body-nick-kristof/</link>
      <pubDate>Sun, 25 Feb 2018 18:00:00 +0000</pubDate>
      <guid>/2018/02/25/what-poisons-are-in-your-body-nick-kristof/</guid>
      <description>&lt;p&gt;Nick Kristof, columnist for the New York Times, recently &lt;a href=&#34;https://www.nytimes.com/interactive/2018/02/23/opinion/columnists/poisons-in-our-bodies.html&#34; target=&#34;_blank&#34;&gt;wrote about Detox Me Action Kit&lt;/a&gt;, a study I manage and helped launch at Silent Spring Institute.&lt;/p&gt;

&lt;p&gt;The column includes a nice graphic summarizing his results:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#39;https://www.nytimes.com/interactive/2018/02/23/opinion/columnists/poisons-in-our-bodies.html&#39;&gt;&lt;img src=&#39;/img/kristof-results.png&#39; style=&#39;border: 2px solid #ccc&#39;&gt;&lt;/a&gt;
&lt;p style=&#39;text-align: right&#39;&gt;Source: &lt;a href=&#39;https://www.nytimes.com/interactive/2018/02/23/opinion/columnists/poisons-in-our-bodies.html&#39;&gt;The New York Times&lt;/a&gt;&lt;/p&gt;&lt;/p&gt;

&lt;p&gt;You can check out the study on the Silent Spring Institute website: &lt;a href=&#34;http://silentspring.org/detoxmeactionkit&#34; target=&#34;_blank&#34;&gt;Detox Me Action Kit&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>More animals from Twitter</title>
      <link>/2018/02/25/more-animals-from-twitter/</link>
      <pubDate>Sun, 25 Feb 2018 12:00:00 +0000</pubDate>
      <guid>/2018/02/25/more-animals-from-twitter/</guid>
      <description>&lt;p&gt;More from Twitter, to complete a very silly triptych:&lt;/p&gt;

&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;.&lt;a href=&#34;https://twitter.com/common_squirrel?ref_src=twsrc%5Etfw&#34;&gt;@common_squirrel&lt;/a&gt; spends most of its time running and blinking. One time, it hoped. &lt;a href=&#34;https://t.co/gO6y3bmSp3&#34;&gt;pic.twitter.com/gO6y3bmSp3&lt;/a&gt;&lt;/p&gt;&amp;mdash; Herb Susmann (@herbps10) &lt;a href=&#34;https://twitter.com/herbps10/status/962143396667232256?ref_src=twsrc%5Etfw&#34;&gt;February 10, 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;The answer to &lt;a href=&#34;https://twitter.com/isacatinthesink?ref_src=twsrc%5Etfw&#34;&gt;@isacatinthesink&lt;/a&gt; is more often &amp;quot;no&amp;quot; than &amp;quot;yes&amp;quot;. I&amp;#39;m surprised. Cats love sinks. &lt;a href=&#34;https://t.co/si1q7VoW1N&#34;&gt;pic.twitter.com/si1q7VoW1N&lt;/a&gt;&lt;/p&gt;&amp;mdash; Herb Susmann (@herbps10) &lt;a href=&#34;https://twitter.com/herbps10/status/961773203759747072?ref_src=twsrc%5Etfw&#34;&gt;February 9, 2018&lt;/a&gt;&lt;/blockquote&gt;

&lt;p&gt;Code for the &lt;a href=&#34;https://gist.github.com/herbps10/1908ebecba9ccec4a5e90d0c8970ec8d&#34; target=&#34;_blank&#34;&gt;@common_squirrel plot&lt;/a&gt; and the &lt;a href=&#34;https://gist.github.com/herbps10/10789f17eeccdd221ecea61cc343041c&#34; target=&#34;_blank&#34;&gt;@isacatinthesink plot&lt;/a&gt; are available as Github Gists.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>They&#39;re all good dogs</title>
      <link>/2018/02/04/theyre-all-good-dogs/</link>
      <pubDate>Sun, 04 Feb 2018 12:00:00 +0000</pubDate>
      <guid>/2018/02/04/theyre-all-good-dogs/</guid>
      <description>


&lt;p&gt;From Twitter:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
The most common rating for dogs from &lt;a href=&#34;https://twitter.com/dog_rates?ref_src=twsrc%5Etfw&#34;&gt;&lt;span class=&#34;citation&#34;&gt;@dog_rates&lt;/span&gt;&lt;/a&gt; is 13/10. A few are 15/10 but all dogs deserve that rating in my opinion. Pictured with Ellie (12/10) from &lt;a href=&#34;https://twitter.com/KatieNicoleF?ref_src=twsrc%5Etfw&#34;&gt;&lt;span class=&#34;citation&#34;&gt;@KatieNicoleF&lt;/span&gt;&lt;/a&gt;. &lt;a href=&#34;https://t.co/B46rRDSCRY&#34;&gt;pic.twitter.com/B46rRDSCRY&lt;/a&gt;
&lt;/p&gt;
— Herb Susmann (&lt;span class=&#34;citation&#34;&gt;@herbps10&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/herbps10/status/959923100468105219?ref_src=twsrc%5Etfw&#34;&gt;February 3, 2018&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;p&gt;Here’s the R code I used to generate the histogram:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rtweet)
library(tidyverse)
library(stringr)
library(cowplot)
library(grid)
library(jpeg)

g &amp;lt;- rasterGrob(readJPEG(&amp;quot;ellie.jpg&amp;quot;), interpolate = TRUE)

tmls &amp;lt;- get_timelines(&amp;quot;dog_rates&amp;quot;, n = 3200)

ratings &amp;lt;- tmls %&amp;gt;%
  filter(str_detect(text, &amp;quot;t.co&amp;quot;)) %&amp;gt;%
  filter(!str_detect(text, &amp;quot;^RT&amp;quot;)) %&amp;gt;%
  filter(!str_detect(text, &amp;quot;Here&amp;#39;s a little more info on Dew&amp;quot;)) %&amp;gt;%
  mutate(rating = map(text, str_extract_all, &amp;quot;1[0-5]/10&amp;quot;),
         rating = map(rating, `[[`, 1)) %&amp;gt;%
  unnest(rating) %&amp;gt;%
  count(rating)

ggplot(ratings, aes(x = rating, y = n)) +
  annotation_custom(g) +
  geom_col(fill = &amp;quot;white&amp;quot;, alpha = 0.8) +
  labs(caption = &amp;quot;Data: @dog_rates, photo: @KatieNicoleF&amp;quot;,
       title = &amp;quot;577 WeRateDogs™ Ratings&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The R code is also available &lt;a href=&#34;https://gist.github.com/herbps10/0d3396b27d4de5a843694737efc98e8a&#34;&gt;as a gist&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Fastest way to see 17 Boston breweries (and one cider house)</title>
      <link>/2017/10/09/fastest-way-to-see-17-boston-breweries-and-one-cider-house/</link>
      <pubDate>Mon, 09 Oct 2017 12:00:00 +0000</pubDate>
      <guid>/2017/10/09/fastest-way-to-see-17-boston-breweries-and-one-cider-house/</guid>
      <description>&lt;p&gt;There have been a few articles in the last couple years that have used traveling salesman problem solvers to find the fastest way to see &lt;a href=&#34;http://www.randalolson.com/2016/07/30/the-optimal-u-s-national-parks-centennial-road-trip/&#34; target=&#34;_blank&#34;&gt;all the national parks&lt;/a&gt; or &lt;a href=&#34;https://flowingdata.com/2015/10/26/top-brewery-road-trip-routed-algorithmically/&#34; target=&#34;_blank&#34;&gt;72 breweries in the US&lt;/a&gt;. I&amp;rsquo;m going to join the trend on a smaller geographic scale by plotting the fastest way to see 18 breweries (including one cider house) in the Boston area.&lt;/p&gt;

&lt;p&gt;The fastest route takes about 2.5 hours of driving (by your designated driver or using a ride service, of course) from start to finish:&lt;/p&gt;

&lt;div style=&#39;margin: 15px 0px&#39;&gt;
  &lt;iframe src=&#39;/maps/brewery.html&#39; width=&#39;700&#39; height=&#39;600&#39; style=&#39;border: none; position: relative; left: -25px&#39;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;If you take 15 minutes at each brewery to have a beer, and it takes 2.5 hours to drive to each one, you could do it all in only 7 hours. Not bad, that&amp;rsquo;s less than a full day at work!&lt;/p&gt;

&lt;p&gt;You won&amp;rsquo;t be able to drive this yourself if you have a beer at every stop, so you&amp;rsquo;ll either need to find a friend to drive you around for seven hours, or take something like a Lyft. I used the Lyft API to estimate how much it would cost, and it comes in at about $176. If you can split this with three friends, and you pay, say, &lt;span&gt;$&lt;/span&gt;8 per drink, your total cost would be &lt;span&gt;$&lt;/span&gt;188.&lt;/p&gt;

&lt;p&gt;Am I going to do this? Probably not. I really don&amp;rsquo;t think I could stomach 17 beers and a cider in one day. And think of all the fun I could have programming in R for seven hours, instead. Yeah. Easy choice.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s a list of all the breweries, in order. You could start your tour anywhere, but if I were doing this I&amp;rsquo;d start at Aeronaut, my favorite brewery around here. You&amp;rsquo;d also get to end at Bantam Cider Company to cap off very long night out on a different note.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Aeuronaut Brewery&lt;/li&gt;
&lt;li&gt;Winter Hill Brewing Company&lt;/li&gt;
&lt;li&gt;Idle Hands Craft Ales&lt;/li&gt;
&lt;li&gt;Night Shift Brewing&lt;/li&gt;
&lt;li&gt;Bone Up Brewing Company&lt;/li&gt;
&lt;li&gt;Mystic Brewery&lt;/li&gt;
&lt;li&gt;Downeast Cider House&lt;/li&gt;
&lt;li&gt;Boston Beer Works&lt;/li&gt;
&lt;li&gt;Trillium Brewing Company&lt;/li&gt;
&lt;li&gt;Harpoon Brewery&lt;/li&gt;
&lt;li&gt;Dorchester Brewing Company&lt;/li&gt;
&lt;li&gt;Sam Adams Brewery&lt;/li&gt;
&lt;li&gt;Turtle Swamp Brewing&lt;/li&gt;
&lt;li&gt;John Harvards Brewery&lt;/li&gt;
&lt;li&gt;Lamplighter Brewing Company&lt;/li&gt;
&lt;li&gt;Cambridge Brewing Company&lt;/li&gt;
&lt;li&gt;Somerville Brewing Company&lt;/li&gt;
&lt;li&gt;Bantam Cider Company&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If you want to want do this, here&amp;rsquo;s a &lt;a href=&#34;https://www.google.com/maps/dir/Aeronaut+Brewing+Company,+14+Tyler+St,+Somerville,+MA+02143/Winter+Hill+Brewing+Company,+328+Broadway,+Somerville,+MA+02145/Idle+Hands+Craft+Ales,+89+Commercial+St,+Malden,+MA+02148/Night+Shift+Brewing,+87+Santilli+Hwy,+Everett,+MA+02149/Bone+Up+Brewing+Co.,+38+Norman+St,+Everett,+MA+02149/Mystic+Brewery,+174+Williams+St,+Chelsea,+MA+02150/Downeast+Cider+House,+256+Marginal+St+%2332,+East+Boston,+MA+02128/BEERWORKS+Brewing+No.+3+Canal,+112+Canal+St,+Boston,+MA+02114/Trillium+Brewing+Company,+369+Congress+Street,+FL+1A,+Boston,+MA+02210/Harpoon+Brewery+And+Beer+Hall,+306+Northern+Ave,+Boston,+MA+02210/Dorchester+Brewing+Company,+1250+Massachusetts+Ave,+Dorchester,+MA+02125/Samuel+Adams,+30+Germania+St,+Boston,+MA+02130/Turtle+Swamp+Brewing,+3377+Washington+St,+Boston,+MA+02130/John+Harvard&#39;s+Brewery+%26+Ale+House,+33+Dunster+St,+Cambridge,+MA+02138/Lamplighter+Brewing+Co.,+284+Broadway,+Cambridge,+MA+02139/Cambridge+Brewing+Company,+1,+1+Kendall+Square,+Cambridge,+MA+02139/Somerville+Brewing+Company+-+Slumbrew,+15+Ward+St,+Somerville,+MA+02143/Bantam+Cider+Company,+40+Merriam+St,+Somerville,+MA+02143/@42.357596,-71.062877,12z/data=!4m109!4m108!1m5!1m1!1s0x89e37736621ba0e3:0xcd08ad34fe7dca73!2m2!1d-71.1062431!2d42.3819603!1m5!1m1!1s0x89e370d461337b9d:0x732c6faaf36fa55!2m2!1d-71.093984!2d42.392658!1m5!1m1!1s0x89e3711a973d8981:0x602f1fd391d76ed7!2m2!1d-71.0747463!2d42.4243919!1m5!1m1!1s0x89e3711a8dbb40d3:0xc68b7f0434ddb595!2m2!1d-71.067886!2d42.4059707!1m5!1m1!1s0x89e371116054c6df:0x30dd9e16e3a944cd!2m2!1d-71.0659377!2d42.4057422!1m5!1m1!1s0x89e371ab8c837f8f:0xc644198c4204f828!2m2!1d-71.044508!2d42.391628!1m5!1m1!1s0x89e370f8a1a1ba83:0x616aeee762a3390d!2m2!1d-71.0324945!2d42.3642794!1m5!1m1!1s0x89e3708e355828b1:0xb7e761efd7866524!2m2!1d-71.060386!2d42.364458!1m5!1m1!1s0x89e37a803cc64d71:0x84333022c52876d!2m2!1d-71.047858!2d42.3498299!1m5!1m1!1s0x89e37a9da5a2cfcb:0xc0be8b883c30f87e!2m2!1d-71.034439!2d42.346892!1m5!1m1!1s0x89e37a4fd2b56dcf:0x4556895a1d1fb907!2m2!1d-71.062317!2d42.322152!1m5!1m1!1s0x89e3797b7da31c79:0x608b218b28d63376!2m2!1d-71.1032285!2d42.3145455!1m5!1m1!1s0x89e379643bcaf91f:0x470aa6cde54a2b0d!2m2!1d-71.1052909!2d42.3088836!1m5!1m1!1s0x89e37742ceb8a909:0x7666471e31ddf163!2m2!1d-71.1192843!2d42.3726027!1m5!1m1!1s0x89e37752b0e81867:0x26811979965d9ac4!2m2!1d-71.0978948!2d42.36795!1m5!1m1!1s0x89e370ade1cbb413:0x3e315a440afb04f9!2m2!1d-71.0912995!2d42.3664092!1m5!1m1!1s0x89e370b65aae14c7:0xdc997b87e8c938a0!2m2!1d-71.0892538!2d42.3748138!1m5!1m1!1s0x89e370b5ff812441:0x94fbd00ab0ecf947!2m2!1d-71.0919018!2d42.3777733&#34; target=&#34;_blank&#34;&gt;Google Map with all the breweries entered in order&lt;/a&gt;. Good luck. Please drink responsibly.&lt;/p&gt;

&lt;p&gt;P.S. Let me know if I missed any breweries in the area!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Do it yourself:&lt;/strong&gt; All the code for this project is &lt;a href=&#34;https://github.com/herbps10/brewery-tsp&#34; target=&#34;_blank&#34;&gt;on Github&lt;/a&gt;. I used the Google Maps API for calculating a distance matrix between all the breweries and to get detailed directions between each point on the final tour. The optimal tour was calculated using the &lt;a href=&#34;https://en.wikipedia.org/wiki/Travelling_salesman_problem#Asymmetric_TSP&#34; target=&#34;_blank&#34;&gt;asymmetric traveling salesman problem&lt;/a&gt; solver from the &lt;a href=&#34;https://cran.r-project.org/web/packages/TSP/index.html&#34; target=&#34;_blank&#34;&gt;TSP R package&lt;/a&gt;. The Lyft price estimate came from it&amp;rsquo;s &lt;a href=&#34;https://www.lyft.com/developers&#34; target=&#34;_blank&#34;&gt;public API&lt;/a&gt;. If you want to run the code yourself, you&amp;rsquo;ll need to get a Google Maps API key and a Lyft API key.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Predicting Jeopardy! Winners</title>
      <link>/2017/10/07/predicting-jeopardy-winners/</link>
      <pubDate>Sat, 07 Oct 2017 12:00:00 +0000</pubDate>
      <guid>/2017/10/07/predicting-jeopardy-winners/</guid>
      <description>


&lt;p&gt;&lt;strong&gt;Goal:&lt;/strong&gt; Predict the winner of Jeopardy. As the game progresses, update the predictions to take into account the current score profile.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Background&lt;/strong&gt;: A quick search revealed work by &lt;a href=&#34;https://thejeopardyfan.com/2016/04/making-jeopardy-predictions-a-methodology.html&#34;&gt;The Jeopardy Fan&lt;/a&gt; on building a model to predict the Tournament of Champions contestants. He used player’s &lt;a href=&#34;http://j-archive.com/help.php#coryatscore&#34;&gt;Coryat scores&lt;/a&gt; to predict the length of their winning streak, and whether they would qualify for the tournament. I’m going for something slightly different than him by focusing on predicting the winner of a single game. I’m also going to use the real game score, instead of Coryat scores.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Data:&lt;/strong&gt; The &lt;a href=&#34;http://j-archive.com&#34;&gt;J-Archive&lt;/a&gt; is an incredible resource for Jeopardy! data, thanks to the work of their archivists. They have every game ever played, every question asked, along with which contestants answered and whether they were correct or not. I downloaded data from seasons 22-33 for fitting and testing the models.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Intuition:&lt;/strong&gt; Let’s explore the dataset a little bit to get a sense of how we might build a classification model to predict winners. You can easily make plots that show how a contestant’s score changes over the course of a game, like this one that shows Roger Craig setting a one-day earnings record:
&lt;img src=&#39;/img/game5977.png&#39; alt=&#39;Jeopardy! Game 5977 score trajectory&#39; class=&#39;large&#39;/&gt;&lt;/p&gt;
&lt;p&gt;Expanding this type of plot beyond a single game, here is a plot showing all of the games from season 27, with each trajectory colored by whether the contestant won the game:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#39;/img/season27-trajectories.png&#39; alt=&#39;Jeopardy! season 27 score trajectories&#39; class=&#39;large&#39; /&gt;&lt;/p&gt;
&lt;p&gt;The winners tend to have higher scores throughout the game, but you can still see a few people who had high scores and still lost. If we show more seasons at once we can see more of the overall trend, but we lose the ability to see individual trajectories clearly:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#39;/img/season22-33-trajectories.png&#39; alt=&#39;Jeopardy! season 22-33 score trajectories&#39; class=&#39;large&#39; /&gt;&lt;/p&gt;
&lt;p&gt;Another way to look at this is by plotting the median scores, with a ribbon showing the 5% and 95% quantiles:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#39;/img/season22-33-trajectories-ribbon.png&#39; class=&#39;large&#39; /&gt;&lt;/p&gt;
&lt;p&gt;It looks like there is some separation between the winners and losers just in terms of their score, and the separation becomes more pronounced as the game progresses, which a model should be able to pick up on and use for prediction.&lt;/p&gt;
&lt;p&gt;We should temper our expectations, though. Especially in the first graph, you can see how much of a randomizer the Final Jeopardy round is. Here’s a table showing how the contestant’s rank going in to Final Jeopardy corresponds to winning or losing (data from seasons 22-33):&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Rank&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Won&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Lost&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Third&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;171 (6%)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2554 (94%)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Tied for second&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8 (12%)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;60 (88%)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Second&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;591 (22%)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2116 (75%)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Tied for first&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17 (47%)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;19 (53%)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;First&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1991 (73%)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;strong&gt;750 (27%)&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;27% of contestants in first place going in to Final Jeopardy still lose. It’s going to be very difficult to accurately predict when an upset will happen, so this gives us a sense of the limits of any prediction model.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Modeling and Results&lt;/strong&gt;: One of the goals is to predict the winner as the game progresses. In each game of Jeopardy! there are up to 61 questions: 30 in Single Jeopardy, 30 in Double Jeopardy, and 1 in Final Jeopardy. I decided to fit a logistic regression model after every question, so we can see how the classification accuracy improves as the game gets closer to the end.&lt;/p&gt;
&lt;p&gt;I used data from seasons 22-32 for training, and held out season 33 for testing.&lt;/p&gt;
&lt;p&gt;There are 61 questions in each game; call them &lt;span class=&#34;math inline&#34;&gt;\(q_{1},q_{2},...,q_{i},...,q_{61}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(q_{1}\)&lt;/span&gt; is the first question asked in the game, &lt;span class=&#34;math inline&#34;&gt;\(q_{2}\)&lt;/span&gt; is the second, and so on. The actual point value of each question might be different, depending on the order they are chosen in the game (for example, &lt;span class=&#34;math inline&#34;&gt;\(q_{1}\)&lt;/span&gt; might be a $200 question in one game, and a $1,000 question in another.) I fitted 61 logistic regression models &lt;span class=&#34;math inline&#34;&gt;\({M_{1}, M_{2}, ...,M_{n}}\)&lt;/span&gt; that predict whether the player won the game based on their score at the end of question &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;. We would expect model &lt;span class=&#34;math inline&#34;&gt;\(M_{1}\)&lt;/span&gt; to do poorly, because the first question isn’t very informative of who is going to end up winning; and the accuracy to improve as the game progresses.&lt;/p&gt;
&lt;p&gt;For example, here is the fitted logistic model for question 30:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#39;/img/logistic-regression-curve-question-30.png&#39; /&gt;&lt;/p&gt;
&lt;p&gt;We can visualize the accuracy of the all models at once by plotting their &lt;a href=&#34;https://en.wikipedia.org/wiki/Receiver_operating_characteristic&#34;&gt;ROC curves&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#39;/img/roc1.png&#39; class=&#39;large&#39; /&gt;&lt;/p&gt;
&lt;p&gt;As we would expect, the model has lousy accuracy at the beginning of the game, but improves steadily as the game progresses. However, it is not perfect even after the game is over. This is because the score itself doesn’t determine the winner; what matters is who has the highest score.&lt;/p&gt;
&lt;p&gt;To address this, I added two new features to bring in information about the contestants compare to each other within the game:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;rank&lt;/em&gt; - categorical variable indicating the contestant’s current rank (first place, tied for first, second place, etc.)&lt;/li&gt;
&lt;li&gt;&lt;em&gt;distance from lead&lt;/em&gt; - the difference in points between the contestant and the leader. If the contestant is in the lead, it is a negative number indicating how far they are ahead.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I built two new sets of models using these features. I also added a very simple model for comparison: predict the winner of the game to be whoever is in the lead. This model doesn’t output probabilities, so it will show up on the ROC curves as a single point (I call this model “current leader” in the legend.) Here’s how they compare to the original model:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#39;/img/rocs1.png&#39; class=&#39;large&#39; /&gt;&lt;/p&gt;
&lt;p&gt;The new models aren’t that much better than the original model. The biggest difference I see is that they have perfect accuracy at the end of the game, as you would expect since they have access to the final ranking of the contestants.&lt;/p&gt;
&lt;p&gt;The “current leader” reference model falls right on the curves, indicating the logistic models don’t do better than it. They may be more useful, though, since they output probabilities rather than a dichotomous prediction.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Visualizing predictions&lt;/strong&gt;: Now that we have these models, let’s see a contestant’s probability of winning (conditioned on the model) evolves over the course of a game. I’m going to take the set of models that use the &lt;em&gt;distance from lead&lt;/em&gt; predictor and apply them to a game from season 33.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#39;/img/game7573.png&#39; class=&#39;large&#39; /&gt;&lt;/p&gt;
&lt;p&gt;Gavin takes the lead in the prediction model at the same time he takes the lead, in the middle of Double Jeopardy.&lt;/p&gt;
&lt;p&gt;Here’s another one from season 33 where the prediction flips towards the end of the game:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#39;/img/game7544.png&#39; class=&#39;large&#39; /&gt;&lt;/p&gt;
&lt;p&gt;The highest probability of winning is assigned to whoever is in the lead, which reflects the logic of the simple reference model.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Next steps:&lt;/strong&gt;
I think a significant shortcoming of the approach I took here is that I fit completely separate models for each question; they aren’t connected in any way, and so they don’t have any “memory” of how each contestant has performed previously in the game (except via their current score), or in prior games. Perhaps there’s a way to incorporate some ideas from partial pooling models to share strength between questions. Or maybe a model could be built that estimates a latent “ability” score for each participant, conditioned on their previous performance. A bonus of this would be that the winners ability score could be fed in to their next game, as a form of prior information about how well the contestant will do. Doing this in a Bayesian framework seems like a good choice.&lt;/p&gt;
&lt;p&gt;I think there are also a few things that could be done to improve performance in the endgame. Right now the models don’t take into account how much money is still available. Incorporating this should help, especially in the end game when there isn’t very much money still on the board. It could also be used to find runaway scores, where one contestant has more money than is possible for a competitor to gain. We could also build a model for predicting the Final Jeopardy bets, so the end game model could have a better understanding of how likely an upset will be.&lt;/p&gt;
&lt;p&gt;Finally, I’d also be interested in changing the goal slightly to predict winners in terms of Coryat score, which I’m sure would perform better since the uncertainty of daily doubles and the Final Jeopardy wager would be removed.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Source code:&lt;/strong&gt;
The source code for this analysis is on Github: &lt;a href=&#34;https://github.com/herbps10/jeopardy&#34;&gt;https://github.com/herbps10/jeopardy&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bananagrams Probabilities</title>
      <link>/2017/07/30/bananagrams-probabilities/</link>
      <pubDate>Sun, 30 Jul 2017 21:13:14 -0500</pubDate>
      <guid>/2017/07/30/bananagrams-probabilities/</guid>
      <description>


&lt;style type=&#39;text/css&#39;&gt;
.banana {
  text-align: center;
  margin-bottom: 20px;
}

.banana span {
  display: inline-block;
  background: #e3dbcf;
  color: black;
  border-radius: 5px;
  text-align: center;
  box-shadow: inset 10px 5px 5px 0px rgba(255, 255, 255, 0.1),
              inset -3px -3px 7px 0px rgba(0, 0, 0, 0.2),
              inset 0px 0px 5px 7px rgba(255, 255, 255, 0.2),
              0px 1px 2px 0px rgba(0, 0, 0, 0.3);

}

div.banana span {
  width: 50px;
  height: 50px;
    font-size: 18pt;
  line-height: 50px;
  font-weight: bold;
}

span.banana span {
    width: 30px;
    height: 30px;
}
&lt;/style&gt;
&lt;p&gt;There was a fabled game of Bananagrams in which my Dad drew his initial 11 tiles, and immediately spelled:&lt;/p&gt;
&lt;div class=&#34;banana&#34;&gt;
&lt;span&gt;R&lt;/span&gt;&lt;span&gt;A&lt;/span&gt;&lt;span&gt;S&lt;/span&gt;&lt;span&gt;T&lt;/span&gt;&lt;span&gt;A&lt;/span&gt;&lt;span&gt;F&lt;/span&gt;&lt;span&gt;A&lt;/span&gt;&lt;span&gt;R&lt;/span&gt;&lt;span&gt;I&lt;/span&gt;&lt;span&gt;A&lt;/span&gt;&lt;span&gt;N&lt;/span&gt;
&lt;/div&gt;
&lt;p&gt;If you haven’t played the game, it’s like a free-form version of Scrabble. You start by drawing a number of tiles (typically 11 or 21), and try to form a word or words out of them.&lt;/p&gt;
&lt;p&gt;The story made me wonder how likely it is to spell an 11 letter word on the first draw.&lt;/p&gt;
&lt;p&gt;The first step is to calculate the probability of drawing a particular word. Consider a bananagrams bag filled with only two letters, S for success and F for failure. Start pulling out tiles from the bag at random, without replacing each tile back in the bag after drawing it, and count how many S tiles you get. The &lt;a href=&#34;https://en.wikipedia.org/wiki/Hypergeometric_distribution&#34;&gt;hypergeometric distribution&lt;/a&gt; models the probability that you will get a certain number of S tiles for a given number of draws. The &lt;a href=&#34;https://en.wikipedia.org/wiki/Hypergeometric_distribution#Multivariate_hypergeometric_distribution&#34;&gt;multivariate hypergeometric distribution&lt;/a&gt; extends this to the multivariate case; that is, it models the probability you’ll draw a certain number of As, Bs, Cs, etc. after drawing a number of tiles from the bag.&lt;/p&gt;
&lt;p&gt;Fortunately, the R package &lt;code&gt;extraDistr&lt;/code&gt; provides an R version of the multivariate hypergeometric probability mass function. Here’s a function that, given a word of length &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; and the number of each letter tile in a bag, gives the probability of drawing that word in &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; draws:&lt;/p&gt;
&lt;p&gt;Using this function, we can find the probability of drawing&lt;br/&gt;&lt;span class=&#34;banana&#34;&gt;&lt;span&gt;R&lt;/span&gt;&lt;span&gt;A&lt;/span&gt;&lt;span&gt;S&lt;/span&gt;&lt;span&gt;T&lt;/span&gt;&lt;span&gt;A&lt;/span&gt;&lt;span&gt;F&lt;/span&gt;&lt;span&gt;A&lt;/span&gt;&lt;span&gt;R&lt;/span&gt;&lt;span&gt;I&lt;/span&gt;&lt;span&gt;A&lt;/span&gt;&lt;span&gt;N&lt;/span&gt;&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;And the result is &lt;span class=&#34;math inline&#34;&gt;\(4.28\times10^{-6}\%\)&lt;/span&gt;. Pretty lucky!&lt;/p&gt;
&lt;p&gt;Now, what is the probability of drawing any valid 11 letter word to start the game? Note that in most cases, spelling a word using all your 11 tiles excludes the possibility of spelling another word. This suggests the the probability of spelling word &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; OR word &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; is given by &lt;span class=&#34;math inline&#34;&gt;\(P(A \cap B)=P(A) + P(B)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;However, there is a special case: what if word &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and word &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; are spelled with the same letters? In order to avoid double counting, we need to only want to include words with the same letters once.&lt;/p&gt;
&lt;p&gt;I downloaded a list of words in the &lt;a href=&#34;https://en.wikipedia.org/wiki/Collins_Scrabble_Words&#34;&gt;SOWPODS&lt;/a&gt; scrabble dictionary from a &lt;a href=&#34;https://github.com/jmlewis/valett/blob/master/scrabble/sowpods.txt&#34;&gt;GitHub repository&lt;/a&gt; and loaded them into R. To deduplicate words with the same letters, I sorted the letters in each word and removed duplicates:&lt;/p&gt;
&lt;p&gt;I then used the &lt;code&gt;word_probability&lt;/code&gt; function to calculate the probability of drawing each 11 letter word, and then summed them all up:&lt;/p&gt;
&lt;p&gt;Which computes the probability of drawing a valid 11 letter word in the opening tiles to be &lt;span class=&#34;math inline&#34;&gt;\(~0.28\%\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Now, suppose you start the game by drawing a different number of tiles. We can compute the probability of starting with a valid word for a range of starting tile numbers:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-07-30-bananagrams_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Drawing 3 letters has the highest probability of forming a word, at &lt;span class=&#34;math inline&#34;&gt;\(53.7\%\)&lt;/span&gt;. This validates my strategy of dumping early in the game to get new tiles when I get stuck, because the new letters often help me get out of the rut.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Who was in each episode of Star Trek?</title>
      <link>/2016/12/29/who-was-in-each-episode-of-star-trek/</link>
      <pubDate>Thu, 29 Dec 2016 12:00:00 +0000</pubDate>
      <guid>/2016/12/29/who-was-in-each-episode-of-star-trek/</guid>
      <description>&lt;p&gt;There is a &lt;a href=&#34;http://www.chakoteya.net/&#34; target=&#34;_blank&#34;&gt;website&lt;/a&gt; with scripts for every episode of Star Trek, so for fun I downloaded them and generated a visualization of which characters were in each episode of Star Trek.&lt;/p&gt;

&lt;p&gt;Enjoy!&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/startrek-characters.png&#34; alt=&#34;Star Trek Characters&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Build a Crystal Radio</title>
      <link>/2016/11/28/build-a-crystal-radio/</link>
      <pubDate>Mon, 28 Nov 2016 12:00:00 +0000</pubDate>
      <guid>/2016/11/28/build-a-crystal-radio/</guid>
      <description>&lt;p&gt;I had fun leading a workshop a few weeks ago on building a crystal radio. We used a simple design that incorporates a loop antenna (doubling as an inductor), a variable capacitor, a germanium diode, and an earpiece.&lt;/p&gt;

&lt;p&gt;Two pieces of wood make a frame for the antenna. We nailed in picture hanger hooks to the ends to wind wire around to form an antenna. Then we used a few nails to hold the variable capacitor in place, and soldered all the components together.&lt;/p&gt;

&lt;p&gt;The walls of the library we were working in dampened outside radio waves a lot, but once we stepped outside we could all hear some nice strong AM stations.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/crystalradioworkshop.png&#34; alt=&#34;Crystal Radio Workshop&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tobacco Industry Concessions that Weren&#39;t Concessions</title>
      <link>/2016/10/21/tobacco-industry-concessions-that-werent-concessions/</link>
      <pubDate>Fri, 21 Oct 2016 12:00:00 +0000</pubDate>
      <guid>/2016/10/21/tobacco-industry-concessions-that-werent-concessions/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;/img/cigarette_century.jpg&#34; alt=&#34;The Cigarette Century by Allan M. Brandt&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Last year I read &lt;a href=&#34;http://www.cigarettecentury.com/&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;The Cigarette Century&lt;/em&gt;&lt;/a&gt; by Allan M. Brandt, a history of the tobacco industry and a finalist for a Pulitzer Prize. One of the points that has stuck with me is how regulations that were intended to curtail the tobacco industry ended up benefiting them.&lt;/p&gt;

&lt;p&gt;In what appeard to be a blow to the tobacco industry, the advertising of tobacco products on the radio and television was banned by the FCC in 1969. In 1967, a lawyer named &lt;a href=&#34;https://en.wikipedia.org/wiki/John_F._Banzhaf_III&#34; target=&#34;_blank&#34;&gt;John F. Banzhaf III&lt;/a&gt; successfully petitioned the FCC under the fairness doctrine to force radio and television to play anti-tobacco public service announcements if they played tobacco company ads. When the tobacco ads went off the air, so did the public service announcements. This was to the benefit of the tobacco industry who desperately wanted to suppress anti-tobacco information. Furthermore, the ban on advertising saved money being spent on expensive ad campaigns (the industry spent $230 million on television advertising in 1970 alone.)&lt;/p&gt;

&lt;p&gt;In 1972, the FTC was finally able to require warning labels on tobacco products. Again, this perceived concession to the tobacco industry provided them with alternative benefits. Now the tobacco companies could argue that smokers were clearly warned of the health risks of smoking, and that they made an informed decision to start smoking despite the risks. Because these risks were accepted, the companies argued, they should not be liable for resulting health issues.&lt;/p&gt;

&lt;p&gt;The tobacco companies negotiated regulations that, while appearing to be concessions, offered them benefits. That they were successful shows their savviness and the difficulty regulators have in reigning in companies that are determined to preserve their business model, and the profits that go along with it.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Jeopardy! Survival Analysis</title>
      <link>/2016/06/19/jeopardy-survival-analysis/</link>
      <pubDate>Sun, 19 Jun 2016 12:00:00 +0000</pubDate>
      <guid>/2016/06/19/jeopardy-survival-analysis/</guid>
      <description>&lt;p&gt;Long streaks are rare in Jeopardy. Most winners only win one game, and slightly less than 40% win two games in a row. In fact, only 6 contestants have won more than ten games in a row.&lt;/p&gt;

&lt;p&gt;This &lt;a href=&#34;https://en.wikipedia.org/wiki/Kaplan%E2%80%93Meier_estimator&#34; target=&#34;_blank&#34;&gt;Kaplan-Meier survival plot&lt;/a&gt; visualizes the survival function of Jeopardy winners:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#39;/img/jeopardy-survival-plot.png&#39; width=&#39;95%&#39; alt=&#39;Kaplain-Meier survival plot of Jeopardy! winning streaks&#39; /&gt;&lt;/p&gt;

&lt;p&gt;The data includes seasons 1-33 (aired 1984-2016), and does not include any championship or tournament games. The point on the extreme right is due, of course, to Ken Jennings and his 74 game winning streak.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s the &lt;a href=&#34;https://gist.github.com/herbps10/c3282c5b869d7a33d601079e66eba490&#34; target=&#34;_blank&#34;&gt;R code&lt;/a&gt; for the figure.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Performant R</title>
      <link>/2014/04/21/performant-r/</link>
      <pubDate>Mon, 21 Apr 2014 21:13:14 -0500</pubDate>
      <guid>/2014/04/21/performant-r/</guid>
      <description>&lt;p&gt;I was fortunate enough to be able to present at UP-Stat 2014 on some of the things I&amp;rsquo;ve learned about writing performant R code while I was working on speeding up an R package for fitting mixed effects nested models. The talk seemed to be a big hit - I was awarded &amp;ldquo;Best Student Presentation&amp;rdquo;!&lt;/p&gt;

&lt;iframe src=&#34;http://www.slideshare.net/slideshow/embed_code/33775697&#34; width=&#34;550&#34; height=&#34;462&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34;&gt;&lt;/iframe&gt;
</description>
    </item>
    
    <item>
      <title>Raytracing in Bash</title>
      <link>/2014/04/04/raytracing-in-bash/</link>
      <pubDate>Fri, 04 Apr 2014 21:13:14 -0500</pubDate>
      <guid>/2014/04/04/raytracing-in-bash/</guid>
      <description>&lt;p&gt;Last semester I took an introductory course in raytracing.&lt;/p&gt;

&lt;p&gt;We practiced an iterative development cycle in which we built up more and more complex ray tracers over the course of the semester. The very first ray tracer was pretty simple: it had to be able to intersect rays with simple geometric objects and display the results, but there didn’t have to be any lighting calculations or anything yet.&lt;/p&gt;

&lt;p&gt;Once I figured out the assignment in OCaml, I decided to give it a shot entirely in Bash!&lt;/p&gt;

&lt;p&gt;(Well, not ENTIRELY in Bash. I will admit I shelled out to bc for floating point operations. A friend pointed out you could do floating point in Bash by having seperate variables for the integer and decimal components, but that’ll have to wait for version two)&lt;/p&gt;

&lt;p&gt;It prints out the raytraced image directly to the console using special unicode characters and coloring through escape codes. Here’s what the result looks like:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/bash-ray-tracer.png&#34; alt=&#34;Bash ray tracer output&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Not exactly pretty, but it works! (if only there was a way to change the line spacing in gnome-terminal).&lt;/p&gt;

&lt;p&gt;The script is available as a gist: &lt;a href=&#34;https://gist.github.com/herbps10/5d606d52e6d18b8c0f34825f22f9c713/&#34; target=&#34;_blank&#34;&gt;raytracer.sh&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>/1/01/01/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/1/01/01/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Autoregressive_model&#34;&gt;Autoregressive (AR) processes&lt;/a&gt; are a popular choice for modeling time-varying processes. AR processes are typically written down as a set of conditional distributions, but if we do some algebra we can show how they can also be written as a &lt;a href=&#34;https://en.wikipedia.org/wiki/Gaussian_process&#34;&gt;Gaussian process&lt;/a&gt;. Having a Guassian process representation is useful because it is more clear how the AR process could be incorporated into larger models, like a spatio-temporal model. In this post, we’ll start with defining an AR process and deriving its mean and variance, then we’ll derive its joint distribution, which is a Gaussian process.&lt;/p&gt;
&lt;div id=&#34;ar-processes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;AR processes&lt;/h2&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Y} = \left\\{ Y_1, Y_2, \dots, Y_n \right\\}\)&lt;/span&gt; be a set of random variables indexed by time. An aurogressive model assumes that &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Y}\)&lt;/span&gt; is correlated over time. An AR model is typically described by defining &lt;span class=&#34;math inline&#34;&gt;\(Y_t\)&lt;/span&gt; in terms of &lt;span class=&#34;math inline&#34;&gt;\(Y_{t-1}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y_{t} = \rho Y_{t-1} + \epsilon_{t}
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{t}\sim N\left(0,\sigma_{\epsilon}^{2}\right)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\rho \in \mathbb{R}\)&lt;/span&gt; is a parameter that controls the degree to which &lt;span class=&#34;math inline&#34;&gt;\(Y_t\)&lt;/span&gt; is correlated with &lt;span class=&#34;math inline&#34;&gt;\(Y_{t-1}\)&lt;/span&gt;. This model is called an AR process of order 1 because &lt;span class=&#34;math inline&#34;&gt;\(Y_t\)&lt;/span&gt; only depends on &lt;span class=&#34;math inline&#34;&gt;\(Y_{t-1}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We can also rearrange terms to emphasize that this representation defines the conditional distribution of &lt;span class=&#34;math inline&#34;&gt;\(Y_{t}\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(Y_{t-1}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
Y_{t} \vert Y_{t-1} \sim&amp;amp; N(\rho Y_{t-1}, \sigma_\epsilon^2) \\
Y_1 \sim&amp;amp; N(0, \frac{\sigma_\epsilon^2}{1-\rho^2})
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where the variance of &lt;span class=&#34;math inline&#34;&gt;\(Y_1\)&lt;/span&gt; comes from the unconditional variance, which is derived below. The stationarity condition of an AR process is that each &lt;span class=&#34;math inline&#34;&gt;\(Y_t\)&lt;/span&gt; has the same distribution; that is, &lt;span class=&#34;math inline&#34;&gt;\(\mu = \mathrm{E}(Y_i) = \mathrm{E}Y_j\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2 = \mathrm{Var}(Y_i) = \mathrm{Var}(Y_j)\)&lt;/span&gt; for all &lt;span class=&#34;math inline&#34;&gt;\(i, j\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Now we can derive the unconditional mean and variance of &lt;span class=&#34;math inline&#34;&gt;\(Y_t\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
(Y_{t}) &amp;amp; = (Y_{t - 1} + &lt;em&gt;{t} )\
&amp;amp; = ( Y&lt;/em&gt;{t - 1 } )\
&amp;amp; =   \
&amp;amp; = 0 \&lt;/p&gt;
&lt;p&gt;(Y_{t}) &amp;amp; = (Y_{t-1} + &lt;em&gt;{t})\
&amp;amp; = ^{2}(Y&lt;/em&gt;{t-1}) + (&lt;em&gt;{t})\
&amp;amp; = ^{2}(Y&lt;/em&gt;{t-1}) + &lt;em&gt;{}&lt;sup&gt;{2}\
&lt;/sup&gt;{2} &amp;amp; = &lt;sup&gt;{2}&lt;/sup&gt;{2} + &lt;/em&gt;{}&lt;sup&gt;{2} \
&lt;/sup&gt;{2}(1-^{2}) &amp;amp; = _{}&lt;sup&gt;{2}\
&lt;/sup&gt;{2} &amp;amp; = 
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;The plot below shows several examples of draws from an AR(1) process with differing values of &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_\epsilon = 1\)&lt;/span&gt;:
&lt;img src=&#34;2019-08-_files/figure-html/ar_1_conditional_representation-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gaussian-processes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Gaussian processes&lt;/h2&gt;
&lt;p&gt;Gaussian processes model a set of variables as being multivariate normally distributed with mean &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\mu}\)&lt;/span&gt; and variance/covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\Sigma}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbf{Y} \sim MVN(\boldsymbol{\mu}, \boldsymbol{\Sigma})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Usually the mean vector is set to &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{0}\)&lt;/span&gt;, which means the Gaussian process is fully defined by its choice of variance/covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\Sigma}\)&lt;/span&gt;. The variance/covariance matrix is defined by a kernel function which defines the covariance between any two variables:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\Sigma_{i,j} = K(i, j)
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;an-ar1-process-is-a-gaussian-process&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;An AR(1) process is a Gaussian process&lt;/h2&gt;
&lt;p&gt;We want to show that an AR process can be represented as a Gaussian process. To do this, we need to show that &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Y}\)&lt;/span&gt; is jointly normally distributed with some mean vector and variance/covariance matrix.&lt;/p&gt;
&lt;p&gt;We already know that &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{E}(Y_t)=0\)&lt;/span&gt;, so the mean vector of its joint distribution will be &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{0}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;To find the variance/covariance matrix, we need to derive the covariance between &lt;span class=&#34;math inline&#34;&gt;\(Y_{t_1}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y_{t_2}\)&lt;/span&gt;. First, let’s consider the simpler case of the covariance between &lt;span class=&#34;math inline&#34;&gt;\(Y_t\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y_{t+1}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
    \operatorname{cov}(Y_{t}, Y_{t+1}) &amp;amp;= \operatorname{E} \left[ \left( Y_t - \operatorname{E}[Y_t] \right) \left( Y_{t+1} - \operatorname{E}[Y_{t+1}] \right) \right] \text{ (definition of covariance) } \\
                   &amp;amp;= \operatorname{E} \left[ Y_t Y_{t+1} \right] \text{ (because } \operatorname{E}[Y_t] = \operatorname{E}[Y_{t+1}] = 0 \text{)} \\
                   &amp;amp;= \operatorname{E} \left[ Y_t \left( \rho Y_{t} + \epsilon_{t+1} \right) \right] \\
                   &amp;amp;= \operatorname{E} \left[ \rho Y_t^2 + Y_t \epsilon_{t+1} \right] \\
                   &amp;amp;= \rho \operatorname{E}\left[ Y_t^2 \right] \\
                   &amp;amp;= \rho (\operatorname{Var}(Y_t) + \operatorname{E}[Y_t]^2) \\
                   &amp;amp;= \rho \frac{\sigma_\epsilon^2}{1 - \rho^2}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;for &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;s separated by more than one time point, iterating the above result yields the expression&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
  \operatorname{cov}(Y_{t_1}, Y_{t_2}) = \rho^{\vert t_1 - t_2 \vert} \frac{\sigma_\epsilon^2}{1 - \rho^2}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now we can fully define the joint distribution of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Y}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbf{Y} \sim MVN(\mathbf{0}, \boldsymbol{\Sigma})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_{i,j} = \rho^{\vert i - j \vert} \frac{\sigma_\epsilon^2}{1-\rho^2}\)&lt;/span&gt;. This is a Gaussian process!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;2019-08-_files/figure-html/ar_process_gaussian_representation-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;combining-kernel-functions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Combining kernel functions&lt;/h2&gt;
&lt;p&gt;The nice thing about Gaussian processes is that we can combine multiple kernel functions to model processes with dependence from different sources. Two ways kernels can be combined are by multiplication and addition. Multiplying two kernels is like an “AND” operation: the correlation between points will be high if the correlation from both kernels is high. Adding two kernels together is like an “OR” operation: correlation is high if either kernel indicates high covariance.&lt;/p&gt;
&lt;p&gt;As an example, let’s build a Gaussian process that combines an AR process (for temporal correlation) and a spatial process (for spatial correlation) by combining two kernel functions. First, we need to define an outcome variable &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; that varies in time and space: let &lt;span class=&#34;math inline&#34;&gt;\(Y_{c,t}\)&lt;/span&gt; be a random variable indexed by spatial site &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; at timepoint &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;. We take the AR covariance as the first kernel function, to model temporal correlation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
K_1(i, j) = \rho^{\vert t_i - t_j \vert} \frac{\sigma_\epsilon^2}{1 - \rho^2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and a squared-exponential kernel function to model spatial dependence:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
K_2(i, j) = \alpha^2 \exp\left( -\frac{d(i, j)}{2\lambda^2} \right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(d(i, j)\)&lt;/span&gt; is the spatial distance between sites &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; is a length-scale parameter, and &lt;span class=&#34;math inline&#34;&gt;\(\alpha^2\)&lt;/span&gt; is a parameter controlling the magnitude of the covariance.&lt;/p&gt;
&lt;p&gt;Combine the two kernel functions so that two data points are correlated if they are close together in time and space:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
K(i, j) &amp;amp;= K_1(i, j) \times K_2(i, j) \\
        &amp;amp;= \rho^{\vert t_i - t_j \vert} \frac{\sigma_\epsilon^2}{1 - \rho^2} \alpha^2 \exp\left( -\frac{d(i, j)}{2\lambda^2} \right)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note the parameters &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_\epsilon\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\alpha^2\)&lt;/span&gt;, which are multipled together, would be unidentifiable in parameter estimation and should be replaced by a single parameter that controls the magnitude of the covariance.&lt;/p&gt;
&lt;p&gt;To illustrate this Gaussian process model, I started by generating a set of sites with random locations:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;2019-08-_files/figure-html/spatial_locations-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;then I drew from the Gaussian process using the parameters temporal parameters &lt;span class=&#34;math inline&#34;&gt;\(\rho=0.9\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\epsilon^2=1\)&lt;/span&gt; and spatial parameters &lt;span class=&#34;math inline&#34;&gt;\(\alpha = 1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\lambda=2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The plot below shows the time trend in the first six sites:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;2019-08-_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And the spatial distribution over time of &lt;span class=&#34;math inline&#34;&gt;\(Y_{c,t}\)&lt;/span&gt; is shown below:
&lt;img src=&#34;2019-08-_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Visually we can see that the Gaussian process generates data that is correlated in both time and space.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;modeling-using-the-mean-and-the-covariance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Modeling using the mean and the covariance&lt;/h2&gt;
&lt;p&gt;The spatio-temporal Gaussian process we defined in the previous section does its modeling through the variance/covariance matrix, with its mean function set to zero. An alternative way to think about a spatio-temporal process is akin to the first AR representation we looked at, and define &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Y}_t\)&lt;/span&gt; (the set of all &lt;span class=&#34;math inline&#34;&gt;\(Y_{c,t}\)&lt;/span&gt; at time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;) relative to &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Y}_{t-1}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
  \mathbf{Y}_{t} = \rho \mathbf{Y}_{t-1} + \boldsymbol{\epsilon}_t
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\epsilon_t} \sim MVN(\mathbf{0}, \boldsymbol{\Sigma}_\epsilon)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;If we set &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\Sigma_\epsilon}\)&lt;/span&gt; to be the diagonal matrix &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\Sigma}_\epsilon = \sigma^2_\epsilon \mathbf{I}_n\)&lt;/span&gt; then we will have an independent AR(1) independent process for each spatial site. It gets more interesting if we define &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\Sigma}_\epsilon\)&lt;/span&gt; by a covariance function so we can include dependence between sites, for example dependence based on the distance between the sites. For now, let’s use the squared exponential kernel and define &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_{i,j} = \alpha^2 \exp\left(-\frac{d(i, j)}{2\lambda^2} \right)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Is this process also equivalent to a mean zero Gaussian process with some covariance kernel? We’ll answer this question by deriving the covariance between any two points.&lt;/p&gt;
&lt;p&gt;The mean of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Y_t}\)&lt;/span&gt; can be shown to be zero in the same way we showed a univariate AR process has mean 0. We also need to know the overall variance/covariance matrix of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Y}_t\)&lt;/span&gt;, which we’ll call &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\Phi}\)&lt;/span&gt;; the logic is imilar to the univariate case, and I’ll show it here for completeness:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
    \operatorname{Var}\left(\boldsymbol{Y}_{t}\right) &amp;amp; =\operatorname{Var}\left(\rho^{2}\mathbf{Y}_{t-1} + \boldsymbol{\epsilon}_{t}\right) \\
     &amp;amp;= \rho^{2}\operatorname{Var}\left(\boldsymbol{Y}_{t-1}\right)+\operatorname{Var}\left(\boldsymbol{\epsilon}_{t}\right) \\
    \boldsymbol{\Phi} &amp;amp; =\rho^{2}\boldsymbol{\Phi}+\boldsymbol{\Sigma}_\epsilon \\
    \boldsymbol{\Phi}-\rho^{2}\boldsymbol{\Sigma} &amp;amp;= \boldsymbol{\Sigma}_\epsilon \\
    \boldsymbol{\Phi}\left(\mathbf{I}-\rho^{2}\mathbf{I}\right) &amp;amp; =\boldsymbol{\Sigma}_\epsilon \\
    \boldsymbol{\Phi} &amp;amp;=\boldsymbol{\Sigma}_{\epsilon}\left(\mathbf{I}-\rho^{2}\mathbf{I}\right)^{-1}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If we pull out two sites at the same time point, their covariance is &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{cov}(Y_{t,c_1}, Y_{t,c_2}) = \frac{\Sigma_{\epsilon, c_1, c_2}}{1-\rho^2}\)&lt;/span&gt;, which looks very similar to the unidimensional AR(1) process variance.&lt;/p&gt;
&lt;p&gt;Now we derive the covariance between any two sites that are one time point apart:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\mathrm{cov}\left(y_{c_1,t},y_{c_2,t+1}\right) &amp;amp; =\mathrm{E}\left[\left(y_{c_1,t}-\mathrm{E}\left[y_{c_1,t}\right]\right)\left(y_{c_2,t}-\mathrm{E}\left[y_{c_2,t}\right]\right)\right]\\
 &amp;amp; =\mathrm{E}\left[y_{c_1,t}y_{c_2,t}\right]\\
 &amp;amp; =\mathrm{E}\left[y_{c_1,t}\left[\rho y_{c_2,t}+\epsilon_{c_2,t+1}\right]\right]\\
 &amp;amp; =\rho\mathrm{E}\left[y_{c_1,t}y_{c_2,t}\right]\\
 &amp;amp; =\rho\mathrm{cov}\left(y_{c_1,t}y_{c_2,t}\right)\\
 &amp;amp; =\rho\frac{\Sigma_{i,j}}{1-\rho^2} \\
 &amp;amp;= \rho \frac{1}{1-\rho^2} \Sigma_{i,j} \\
 &amp;amp;= \rho \frac{1}{1-\rho^2} \alpha^2 \exp\left(-\frac{d(i, j)}{2\lambda^2} \right)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;for sites more than one time point away from each other, we can iterate the above result to get a general expression of the covariance between any two points:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\mathrm{cov}\left(y_{c_1,t_1},y_{c_2,t_2}\right) &amp;amp;= \rho^{\vert t_1 - t_2 \vert}\frac{1}{1-\rho^2} \alpha^2 \exp\left(-\frac{d(i, j)}{2\lambda^2} \right)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;if we reparameterize &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; to be the product of two parameters &lt;span class=&#34;math inline&#34;&gt;\(\alpha = \sigma^2_\epsilon \alpha\)&lt;/span&gt;, we get&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\mathrm{cov}\left(y_{c_1,t_1},y_{c_2,t_2}\right) &amp;amp;= \rho^{\vert t_1 - t_2 \vert}\frac{\sigma^2_\epsilon}{1-\rho^2} \alpha^2 \exp\left(-\frac{d(i, j)}{2\lambda^2} \right) \\
&amp;amp;= K_1(i, j) \times K_2(i,j)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which is the product of an AR(1) and squared exponential kernel function as defined in the previous section. In practice we wouldn’t want to separate these parameters because both of them will not be identifiable given observed data, but I separated them here to show how the covariance structure is the product of two kernel functions.&lt;/p&gt;
&lt;p&gt;Therefore, we can write this process in the form of a Gaussian process with mean zero and covariance kernel given by the product of a temporal and spatial kernel:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\mathbf{Y} \sim&amp;amp; MVN(\mathbf{0}, \boldsymbol{\Sigma}) \\
\Sigma_{i,j} =&amp;amp; K_1(i, j) \times K_2(i, j) 
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The spatio-temporal processes defined as a set of conditional distributions and as a joint Gaussian process are equivalent.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;To summarize, AR processes can be written as a Gaussian process model, which is useful because a temporal process can then be easily combined with other sources of dependence. In general, we can build our models by defining conditional distributions with a given mean and covariance, or a joint distribution with mean zero where the model is fully defined by a variance/covariance kernel function. In a future post I will look at Bayesian parameter estimation in these models using Stan.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>

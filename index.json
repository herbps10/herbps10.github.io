[{"authors":["admin"],"categories":null,"content":"I grew up near Ithaca, New York, where I was home-schooled for elementary school, an experience that shaped my belief in the value of self-directed, intrinsically motivated learning. I went to college at SUNY Geneseo where I received a degree in Mathematics in 2014. I spent four years working as a software developer at Silent Spring Institute where I developed digital tools for communicating complex scientific data to a lay audience. I began a doctoral program in biostatistics at UMass Amherst in 2018, where I am working with Leontine Alkema on spatial Bayesian modeling.\n","date":1570579200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1570579200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I grew up near Ithaca, New York, where I was home-schooled for elementary school, an experience that shaped my belief in the value of self-directed, intrinsically motivated learning. I went to college at SUNY Geneseo where I received a degree in Mathematics in 2014. I spent four years working as a software developer at Silent Spring Institute where I developed digital tools for communicating complex scientific data to a lay audience.","tags":null,"title":"Herb Susmann","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":[],"categories":[],"content":"\rGun dealers in the U.S. are required to conduct instant background checks before selling weapons to individuals. The FBI provides data for the number of these background checks performed by month/year, which serves as a proxy for the total number of gun sales in the U.S.\nI brought the data into R for a quick and dirty analysis, with the intent of finding spikes in background checks around major events.\nFirst, let’s take a look raw data. There are a few obvious spikes in the later years, which correspond to the Sandy Hook (December 2012) and San Bernadino (December 2015) shootings.\nNext, I fit a negative binomial generalized linear model that accounts for an overall trend using a 3rd order cubic spline and monthly seasonal variation:\nmodel \u0026lt;- MASS::glm.nb(value ~ bs(date) + month, dat)\rEven such a simple model does a decent job fitting the data, although it gets much worse in later years as the variance in the data increases:\nMore interesting is the plot of the residuals from the model, which show spikes in background checks that aren’t accounted for by the model. This makes a couple of other peaks jump out that are correlated with notable events, like 9/11 and Obama’s election:\nThere are a few other peaks that I don’t have explanations for, like in late 1999 and in the beginning of 2014.\nI think it’s interesting how the model residuals let us see spikes in background checks that we couldn’t see in the raw data. The tradeoff is that the model residuals are conditional on the model choice; choosing a different model might lead to a different plot. If we want to answer the question “were there spikes in gun background checks”, we now have to condition our conclusions on that model choice, which complicates interpretation.\n","date":1580515200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580604526,"objectID":"709451c03a1a5ac9666b3e3d8a1ff1df","permalink":"/2020/02/01/firearm-background-check-timeseries-modeling/","publishdate":"2020-02-01T00:00:00Z","relpermalink":"/2020/02/01/firearm-background-check-timeseries-modeling/","section":"post","summary":"Gun dealers in the U.S. are required to conduct instant background checks before selling weapons to individuals. The FBI provides data for the number of these background checks performed by month/year, which serves as a proxy for the total number of gun sales in the U.S.\nI brought the data into R for a quick and dirty analysis, with the intent of finding spikes in background checks around major events.","tags":[],"title":"Firearm Background Check Timeseries Modeling","type":"post"},{"authors":["Herb Susmann","Laurel A. Schaider","Kathryn M. Rodgers","Ruthann A. Rudel"],"categories":null,"content":"","date":1570579200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570579200,"objectID":"0a8f393f4d0a2bbac4244b8e586078e7","permalink":"/publication/pfas/","publishdate":"2019-10-09T00:00:00Z","relpermalink":"/publication/pfas/","section":"publication","summary":"DERBI is a digital tool for reporting back personal results to participants in exposure studies.","tags":["Silent Spring Institute"],"title":"Dietary Habits Related to Food Packaging and Population Exposure to PFASs","type":"publication"},{"authors":["Herb Susmann"],"categories":[],"content":"\rI often use random walk/autoregressive models in my research as a component in time-series analysis, and I wanted to get some more experience fitting them to data. FiveThirtyEight publishes several polling datasets, including polling for the 2020 Democratic presidential primary. I used Stan to fit a Bayesian random walk model to the polling data, which I describe below. The Stan and R code used in this post is available as a Github gist.\nLet \\(\\delta_{c,t}\\) be the true proportion of voters in favor of candidate \\(c\\) at time \\(t\\). Our modeling assumption is that the logit-transform of \\(\\delta_{c,t}\\) follows a random walk; that is:\r\\[\r\\mathrm{logit}(\\delta_{c,t}) \\sim \\mathrm{N}\\left(\\mathrm{logit}(\\delta_{c,t-1}), \\tau^2\\right)\r\\]\rWe can’t observe \\(\\delta_{c,t}\\) directly; we have to infer it through the noisy observations we have from polls.\nLet \\(s_{i}\\) be the sample size of poll \\(c[i]\\) and \\(y_{i}\\) the number of poll respondents in favor of candidate \\(c[i]\\) at time \\(t[i]\\). Let \\(\\phi_i\\) be the proportion of poll respondents in favor of candidate \\(c[i]\\) at time \\(t[i]\\). To incorporate sampling error, we model \\(y_i\\) as binomial:\r\\[\ry_i \\sim \\mathrm{Binomial}(s_i, \\phi_i)\r\\]\rWe also allow for added variance in our observations by relating \\(\\phi_i\\) to the true logit proportion \\(\\delta_{c[i], t[i]}\\) with a normal distribution:\r\\[\r\\mathrm{logit}(\\phi_i) \\sim \\mathrm{N}(\\mathrm{logit}(\\delta_{c[i], t[i]}), \\sigma^2)\r\\]\nTo finish defining the model half-normal priors on the hyperparameters. The prior for \\(\\tau^2\\) has a small variance to improve identification of the model (a vaguer prior can cause the MCMC chains to not mix well.)\r\\[\r\\begin{aligned}\r\\tau^2 \u0026amp;\\sim \\mathrm{N}(0, 0.02)[0, \\infty] \\\\\r\\sigma^2 \u0026amp;\\sim \\mathrm{N}(0, 1)[0, \\infty] \\end{aligned}\r\\]\nHere is the Stan representation of the statistical model:\nS4 class stanmodel \u0026#39;random_walk\u0026#39; coded as follows:\rdata {\rint\u0026lt;lower=0\u0026gt; T; // Number of timepoints\rint\u0026lt;lower=0\u0026gt; C; // Number of candidates\rint\u0026lt;lower=0\u0026gt; N; // Number of poll observations\rint sample_size[N]; // Sample size of each poll\rint y[N]; // Number of respondents in poll for candidate (approximate)\rint\u0026lt;lower=1, upper=T\u0026gt; get_t_i[N]; // timepoint for ith observation\rint\u0026lt;lower=1, upper=C\u0026gt; get_c_i[N]; // candidate for ith observation\r}\rparameters {\rmatrix[C, T] delta_logit; // Percent for candidate c at time t\rreal\u0026lt;lower=0, upper=1\u0026gt; phi[N]; // Percent of participants in poll for candidate\rreal\u0026lt;lower=0\u0026gt; tau; // Random walk variance\rreal\u0026lt;lower=0,upper=0.5\u0026gt; sigma; // Overdispersion of observations\r}\rmodel {\r// Priors\rtau ~ normal(0, 0.2);\rsigma ~ normal(0, 1);\r// Random walk\rfor(c in 1:C) {\rdelta_logit[c, 2:T] ~ normal(delta_logit[c, 1:(T - 1)], tau);\r}\r// Observed data\ry ~ binomial(sample_size, phi);\rfor(i in 1:N) {\r// Overdispersion\rdelta_logit[get_c_i[i], get_t_i[i]] ~ normal(logit(phi[i]), sigma);\r}\r}\rgenerated quantities {\rmatrix[C, T] delta = inv_logit(delta_logit);\r} \rThe raw dataset that we are going to fit:\rI fitted the Stan model to the data using the standard HMC-NUTS algorithm and 1000 MCMC iterations. The plot below shows the posterior median with 75% and 95% credible intervals.\nOne issue with this model is that it oversmooths large bumps in the polls. For example, Harris had a bump after the first debate, which the model smooths into an uptick leading into the debate that is not justified in the data. The model could be improved by allowing for these shocks, for example by restarting the random walk after key dates like the debates which we know are likely to cause discontinuities in the results.\n","date":1565395200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565437002,"objectID":"1c3815dbbb0384f1a8d64e47e9cf8c24","permalink":"/2019/08/10/presidential-primary-polling-analysis-in-stan/","publishdate":"2019-08-10T00:00:00Z","relpermalink":"/2019/08/10/presidential-primary-polling-analysis-in-stan/","section":"post","summary":"I often use random walk/autoregressive models in my research as a component in time-series analysis, and I wanted to get some more experience fitting them to data. FiveThirtyEight publishes several polling datasets, including polling for the 2020 Democratic presidential primary. I used Stan to fit a Bayesian random walk model to the polling data, which I describe below. The Stan and R code used in this post is available as a Github gist.","tags":["r","stan"],"title":"Presidential Primary Polling Analysis in Stan","type":"post"},{"authors":null,"categories":null,"content":"","date":1565395200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565395200,"objectID":"2b3c8a25669ebedda573f1ac8c1265f0","permalink":"/project/reactor/","publishdate":"2019-08-10T00:00:00Z","relpermalink":"/project/reactor/","section":"project","summary":"Reactive notebooks for R","tags":["R, 📦"],"title":"Reactor","type":"project"},{"authors":null,"categories":null,"content":"\rAutoregressive (AR) processes are a popular choice for modeling time-varying processes. AR processes are typically written down as a set of conditional distributions, but if we do some algebra we can show how they can also be written as a Gaussian process. One reason having a Guassian process representation is useful is because it makes it more clear how an AR process can be incorporated into larger models, like a spatio-temporal model. In this post, we’ll start with defining an AR process and deriving its mean and variance, then we’ll derive its joint distribution, which is a Gaussian process.\nAR processes\rLet \\(\\mathbf{Y} = \\left\\{ Y_1, Y_2, \\dots, Y_n \\right\\}\\) be a set of random variables indexed by time. An aurogressive model assumes that \\(\\mathbf{Y}\\) is correlated over time. An AR model is typically described by defining \\(Y_t\\) in terms of \\(Y_{t-1}\\):\n\\[\rY_{t} = \\rho Y_{t-1} + \\epsilon_{t}\r\\]\rwhere \\(\\epsilon_{t}\\sim N\\left(0,\\sigma_{\\epsilon}^{2}\\right)\\) and \\(\\rho \\in \\mathbb{R}\\) is a parameter that controls the degree to which \\(Y_t\\) is correlated with \\(Y_{t-1}\\). This model is called an AR process of order 1 because \\(Y_t\\) only depends on \\(Y_{t-1}\\).\nWe can also rearrange terms to emphasize that this representation defines the conditional distribution of \\(Y_{t}\\) given \\(Y_{t-1}\\):\n\\[\r\\begin{aligned}\rY_{t} \\vert Y_{t-1} \\sim\u0026amp; N(\\rho Y_{t-1}, \\sigma_\\epsilon^2) \\\\\rY_1 \\sim\u0026amp; N(0, \\frac{\\sigma_\\epsilon^2}{1-\\rho^2})\r\\end{aligned}\r\\]\nWhere the variance of \\(Y_1\\) comes from the unconditional variance, which is derived below. The stationarity condition of an AR process is that each \\(Y_t\\) has the same distribution; that is, \\(\\mu = \\mathrm{E}(Y_i) = \\mathrm{E}Y_j\\) and \\(\\sigma^2 = \\mathrm{Var}(Y_i) = \\mathrm{Var}(Y_j)\\) for all \\(i, j\\).\nNow we can derive the unconditional mean and variance of \\(Y_t\\):\n\\[\r\\begin{aligned}\r\\mathrm{E}\\left(Y_{t}\\right) \u0026amp;= \\mathrm{E}\\left(\\rho Y_{t - 1} + \\epsilon_{t} \\right)\\\\\r\u0026amp;= \\rho \\mathrm{E}\\left( Y_{t - 1 } \\right)\\\\\r\\mu \u0026amp;= \\rho\\mu\\ \\text{(apply stationarity condition)} \\\\\r\\mu \u0026amp;= 0 \\\\\r\\mathrm{Var}\\left(Y_{t}\\right) \u0026amp;= \\mathrm{Var}\\left(\\rho Y_{t-1} + \\epsilon_{t}\\right)\\\\\r\u0026amp;= \\rho^{2}\\mathrm{Var}(Y_{t-1}) + \\mathrm{Var}\\left(\\epsilon_{t}\\right)\\\\\r\u0026amp;= \\rho^{2}\\mathrm{Var}(Y_{t-1}) + \\sigma_{\\epsilon}^{2}\\\\\r\\sigma^{2} \u0026amp;= \\rho^{2}\\sigma^{2} + \\sigma_{\\epsilon}^{2}\\ \\text{(apply stationarity condition)}\\\\\r\\sigma^{2}\\left(1-\\rho^{2}\\right) \u0026amp;= \\sigma_{\\epsilon}^{2}\\\\\r\\sigma^{2} \u0026amp;= \\frac{\\sigma_{\\epsilon}^{2}}{1 - \\rho^{2}}\r\\end{aligned}\r\\]\nThe plot below shows several examples of draws from an AR(1) process with differing values of \\(\\rho\\) and \\(\\sigma^2_\\epsilon = 1\\):\r\rGaussian processes\rGaussian processes model a set of variables as being multivariate normally distributed with mean \\(\\boldsymbol{\\mu}\\) and variance/covariance matrix \\(\\boldsymbol{\\Sigma}\\):\n\\[\r\\mathbf{Y} \\sim MVN(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\r\\]\nUsually the mean vector is set to \\(\\boldsymbol{0}\\), which means the Gaussian process is fully defined by its choice of variance/covariance matrix \\(\\boldsymbol{\\Sigma}\\). The variance/covariance matrix is defined by a kernel function which defines the covariance between any two variables:\n\\[\r\\Sigma_{i,j} = K(i, j)\r\\]\n\rAn AR(1) process is a Gaussian process\rWe want to show that an AR process can be represented as a Gaussian process. To do this, we need to show that \\(\\mathbf{Y}\\) is jointly normally distributed with some mean vector and variance/covariance matrix.\nWe already know that \\(\\mathrm{E}(Y_t)=0\\), so the mean vector of its joint distribution will be \\(\\mathbf{0}\\).\nTo find the variance/covariance matrix, we need to derive the covariance between \\(Y_{t_1}\\) and \\(Y_{t_2}\\). First, let’s consider the simpler case of the covariance between \\(Y_t\\) and \\(Y_{t+1}\\):\n\\[\r\\begin{aligned}\r\\mathrm{cov}(Y_{t}, Y_{t+1}) \u0026amp;= \\mathrm{E} \\left[ \\left( Y_t - \\mathrm{E}[Y_t] \\right) \\left( Y_{t+1} - \\mathrm{E}[Y_{t+1}] \\right) \\right] \\text{ (definition of covariance) } \\\\\r\u0026amp;= \\mathrm{E} \\left[ Y_t Y_{t+1} \\right] \\text{ (because } \\mathrm{E}[Y_t] = \\mathrm{E}[Y_{t+1}] = 0 \\text{)} \\\\\r\u0026amp;= \\mathrm{E} \\left[ Y_t \\left( \\rho Y_{t} + \\epsilon_{t+1} \\right) \\right] \\\\\r\u0026amp;= \\mathrm{E} \\left[ \\rho Y_t^2 + Y_t \\epsilon_{t+1} \\right] \\\\\r\u0026amp;= \\rho \\mathrm{E}\\left[ Y_t^2 \\right] \\\\\r\u0026amp;= \\rho (\\mathrm{Var}(Y_t) + \\mathrm{E}[Y_t]^2) \\\\\r\u0026amp;= \\rho \\frac{\\sigma_\\epsilon^2}{1 - \\rho^2}\r\\end{aligned}\r\\]\nfor \\(Y\\)s separated by more than one time point, iterating the above result yields the expression\n\\[\r\\begin{aligned}\r\\mathrm{cov}(Y_{t_1}, Y_{t_2}) = \\rho^{\\vert t_1 - t_2 \\vert} \\frac{\\sigma_\\epsilon^2}{1 - \\rho^2}\r\\end{aligned}\r\\]\nNow we can fully define the joint distribution of \\(\\mathbf{Y}\\):\n\\[\r\\mathbf{Y} \\sim MVN(\\mathbf{0}, \\boldsymbol{\\Sigma})\r\\]\nwhere \\(\\Sigma_{i,j} = \\rho^{\\vert i - j \\vert} \\frac{\\sigma_\\epsilon^2}{1-\\rho^2}\\). This is a Gaussian process!\n\rCombining kernel functions\rThe nice thing about Gaussian processes is that we can combine multiple kernel functions to model processes with dependence from different sources. Two ways kernels can be combined are by multiplication and addition. Multiplying two kernels is like an “AND” operation: the correlation between points will be high if the correlation from both kernels is high. Adding two kernels together is like an “OR” operation: correlation is high if either kernel indicates high covariance.\nAs an example, let’s build a Gaussian process that combines an AR process (for temporal correlation) and a spatial process (for spatial correlation) by combining two kernel functions. First, we need to define an outcome variable \\(Y\\) that varies in time and space: let \\(Y_{c,t}\\) be a random variable indexed by spatial site \\(c\\) at timepoint \\(t\\). We take the AR covariance as the first kernel function, to model temporal correlation:\n\\[\rK_1(i, j) = \\rho^{\\vert t_i - t_j \\vert} \\frac{\\sigma_\\epsilon^2}{1 - \\rho^2}\r\\]\nand a squared-exponential kernel function to model spatial dependence:\n\\[\rK_2(i, j) = \\alpha^2 \\exp\\left( -\\frac{d(i, j)}{2\\lambda^2} \\right)\r\\]\nwhere \\(d(i, j)\\) is the spatial distance between sites \\(i\\) and \\(j\\), \\(\\lambda\\) is a length-scale parameter, and \\(\\alpha^2\\) is a parameter controlling the magnitude of the covariance.\nCombine the two kernel functions so that two data points are correlated if they are close together in time and space:\n\\[\r\\begin{aligned}\rK(i, j) \u0026amp;= K_1(i, j) \\times K_2(i, j) \\\\\r\u0026amp;= \\rho^{\\vert t_i - t_j \\vert} \\frac{\\sigma_\\epsilon^2}{1 - \\rho^2} \\alpha^2 \\exp\\left( -\\frac{d(i, j)}{2\\lambda^2} \\right)\r\\end{aligned}\r\\]\nNote the parameters \\(\\sigma^2_\\epsilon\\) and \\(\\alpha^2\\), which are multipled together, would be unidentifiable in parameter estimation and should be replaced by a single parameter that controls the magnitude of the covariance.\nTo illustrate this Gaussian process model, I started by generating a set of sites with random locations:\nthen I drew from the Gaussian process using the parameters temporal parameters \\(\\rho=0.9\\), \\(\\sigma_\\epsilon^2=1\\) and spatial parameters \\(\\alpha = 1\\) and \\(\\lambda=2\\).\nThe plot below shows the time trend in the first six sites:\nAnd the spatial distribution over time of \\(Y_{c,t}\\) is shown below:\rVisually we can see that the Gaussian process generates data that is correlated in both time and space.\n\rModeling using the mean and the covariance\rThe spatio-temporal Gaussian process we defined in the previous section does its modeling through the variance/covariance matrix, with its mean function set to zero. An alternative way to think about a spatio-temporal process is akin to the first AR representation we looked at, and define \\(\\mathbf{Y}_t\\) (the set of all \\(Y_{c,t}\\) at time \\(t\\)) relative to \\(\\mathbf{Y}_{t-1}\\):\n\\[\r\\begin{aligned}\r\\mathbf{Y}_{t} = \\rho \\mathbf{Y}_{t-1} + \\boldsymbol{\\epsilon}_t\r\\end{aligned}\r\\]\nwhere \\(\\boldsymbol{\\epsilon_t} \\sim MVN(\\mathbf{0}, \\boldsymbol{\\Sigma}_\\epsilon)\\).\nIf we set \\(\\boldsymbol{\\Sigma_\\epsilon}\\) to be the diagonal matrix \\(\\boldsymbol{\\Sigma}_\\epsilon = \\sigma^2_\\epsilon \\mathbf{I}_n\\) then we will have an independent AR(1) independent process for each spatial site. It gets more interesting if we define \\(\\boldsymbol{\\Sigma}_\\epsilon\\) by a covariance function so we can include dependence between sites, for example dependence based on the distance between the sites. For now, let’s use the squared exponential kernel and define \\(\\Sigma_{i,j} = \\alpha^2 \\exp\\left(-\\frac{d(i, j)}{2\\lambda^2} \\right)\\).\nIs this process also equivalent to a mean zero Gaussian process with some covariance kernel? We’ll answer this question by deriving the covariance between any two points.\nThe mean of \\(\\mathbf{Y_t}\\) can be shown to be zero in the same way we showed a univariate AR process has mean 0. We also need to know the overall variance/covariance matrix of \\(\\mathbf{Y}_t\\), which we’ll call \\(\\boldsymbol{\\Phi}\\); the logic is imilar to the univariate case, and I’ll show it here for completeness:\n\\[\r\\begin{aligned}\r\\mathrm{Var}\\left(\\boldsymbol{Y}_{t}\\right) \u0026amp; =\\mathrm{Var}\\left(\\rho^{2}\\mathbf{Y}_{t-1} + \\boldsymbol{\\epsilon}_{t}\\right) \\\\\r\u0026amp;= \\rho^{2}\\mathrm{Var}\\left(\\boldsymbol{Y}_{t-1}\\right)+\\mathrm{Var}\\left(\\boldsymbol{\\epsilon}_{t}\\right) \\\\\r\\boldsymbol{\\Phi} \u0026amp; =\\rho^{2}\\boldsymbol{\\Phi}+\\boldsymbol{\\Sigma}_\\epsilon \\\\\r\\boldsymbol{\\Phi}-\\rho^{2}\\boldsymbol{\\Sigma} \u0026amp;= \\boldsymbol{\\Sigma}_\\epsilon \\\\\r\\boldsymbol{\\Phi}\\left(\\mathbf{I}-\\rho^{2}\\mathbf{I}\\right) \u0026amp; =\\boldsymbol{\\Sigma}_\\epsilon \\\\\r\\boldsymbol{\\Phi} \u0026amp;=\\boldsymbol{\\Sigma}_{\\epsilon}\\left(\\mathbf{I}-\\rho^{2}\\mathbf{I}\\right)^{-1}\r\\end{aligned}\r\\]\nIf we pull out two sites at the same time point, their covariance is \\(\\mathrm{cov}(Y_{t,c_1}, Y_{t,c_2}) = \\frac{\\Sigma_{\\epsilon, c_1, c_2}}{1-\\rho^2}\\), which looks very similar to the unidimensional AR(1) process variance.\nNow we derive the covariance between any two sites that are one time point apart:\n\\[\r\\begin{aligned}\r\\mathrm{cov}\\left(y_{c_1,t},y_{c_2,t+1}\\right) \u0026amp; =\\mathrm{E}\\left[\\left(y_{c_1,t}-\\mathrm{E}\\left[y_{c_1,t}\\right]\\right)\\left(y_{c_2,t}-\\mathrm{E}\\left[y_{c_2,t}\\right]\\right)\\right]\\\\\r\u0026amp; =\\mathrm{E}\\left[y_{c_1,t}y_{c_2,t}\\right]\\\\\r\u0026amp; =\\mathrm{E}\\left[y_{c_1,t}\\left[\\rho y_{c_2,t}+\\epsilon_{c_2,t+1}\\right]\\right]\\\\\r\u0026amp; =\\rho\\mathrm{E}\\left[y_{c_1,t}y_{c_2,t}\\right]\\\\\r\u0026amp; =\\rho\\mathrm{cov}\\left(y_{c_1,t}y_{c_2,t}\\right)\\\\\r\u0026amp; =\\rho\\frac{\\Sigma_{i,j}}{1-\\rho^2} \\\\\r\u0026amp;= \\rho \\frac{1}{1-\\rho^2} \\Sigma_{i,j} \\\\\r\u0026amp;= \\rho \\frac{1}{1-\\rho^2} \\alpha^2 \\exp\\left(-\\frac{d(i, j)}{2\\lambda^2} \\right)\r\\end{aligned}\r\\]\nfor sites more than one time point away from each other, we can iterate the above result to get a general expression of the covariance between any two points:\n\\[\r\\begin{aligned}\r\\mathrm{cov}\\left(y_{c_1,t_1},y_{c_2,t_2}\\right) \u0026amp;= \\rho^{\\vert t_1 - t_2 \\vert}\\frac{1}{1-\\rho^2} \\alpha^2 \\exp\\left(-\\frac{d(i, j)}{2\\lambda^2} \\right)\r\\end{aligned}\r\\]\nif we reparameterize \\(\\alpha\\) to be the product of two parameters \\(\\alpha = \\sigma^2_\\epsilon \\alpha\\), we get\n\\[\r\\begin{aligned}\r\\mathrm{cov}\\left(y_{c_1,t_1},y_{c_2,t_2}\\right) \u0026amp;= \\rho^{\\vert t_1 - t_2 \\vert}\\frac{\\sigma^2_\\epsilon}{1-\\rho^2} \\alpha^2 \\exp\\left(-\\frac{d(i, j)}{2\\lambda^2} \\right) \\\\\r\u0026amp;= K_1(i, j) \\times K_2(i,j)\r\\end{aligned}\r\\]\nwhich is the product of an AR(1) and squared exponential kernel function as defined in the previous section. In practice we wouldn’t want to separate these parameters because both of them will not be identifiable given observed data, but I separated them here to show how the covariance structure is the product of two kernel functions.\nTherefore, we can write this process in the form of a Gaussian process with mean zero and covariance kernel given by the product of a temporal and spatial kernel:\n\\[\r\\begin{aligned}\r\\mathbf{Y} \\sim\u0026amp; MVN(\\mathbf{0}, \\boldsymbol{\\Sigma}) \\\\\r\\Sigma_{i,j} =\u0026amp; K_1(i, j) \\times K_2(i, j) \\end{aligned}\r\\]\nThe spatio-temporal processes defined as a set of conditional distributions and as a Gaussian process are equivalent.\nTo summarize, AR processes can be written as a Gaussian process model, which is useful because a temporal process can then be easily combined with other sources of dependence. In general, we can build our models by defining conditional distributions with a given mean and covariance, or a joint distribution with mean zero where the model is fully defined by a variance/covariance kernel function. In a future post I will look at Bayesian parameter estimation in these models using Stan.\n\r","date":1565308800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565308800,"objectID":"2f0aa3449bee7bdf0f02ec3c442c0fcf","permalink":"/2019/08/09/autoregressive-processes-are-gaussian-processes/","publishdate":"2019-08-09T00:00:00Z","relpermalink":"/2019/08/09/autoregressive-processes-are-gaussian-processes/","section":"post","summary":"Autoregressive (AR) processes are a popular choice for modeling time-varying processes. AR processes are typically written down as a set of conditional distributions, but if we do some algebra we can show how they can also be written as a Gaussian process. One reason having a Guassian process representation is useful is because it makes it more clear how an AR process can be incorporated into larger models, like a spatio-temporal model.","tags":null,"title":"Autoregressive Processes are Gaussian Processes","type":"post"},{"authors":null,"categories":null,"content":"\rMany statistical routines in R use R formulas as a flexible way to specify the terms of a model. With a little setup, we can use formulas to build inputs to Stan and avoid hard-coding any variables in the model.\nFor example, say you are writing a Stan model for linear regression. You would like to regress a response variable \\(y\\) on two predictors, \\(x_1\\) and \\(y_1\\):\n// linear_regression.stan\rdata {\rint N; // Number of observations\rvector[N] y;\rvector[N] x1;\rvector[N] x2;\r} parameters {\rreal beta_0;\rreal beta_1;\rreal beta_2;\rreal\u0026lt;lower=0\u0026gt; sigma;\r} model {\ry ~ normal(beta_0 + beta_1 * x1 + beta_2 * x2, sigma);\r}\rBut what if you later decide to add more predictors? We can make the above model more flexible by allowing a matrix of predictors \\(X\\) of arbitrary size:\n// linear_regression.stan\rdata {\rint N; // Number of observations\rint K; // Number of predictors\rvector[N] y;\rmatrix[N, K] X;\r} parameters {\rvector[K] beta;\rreal\u0026lt;lower=0\u0026gt; sigma;\r} model {\ry ~ normal(beta * X, sigma);\r}\rThen we can use R formulas to build the predictor matrix \\(X\\) and pass it to Stan:\nfit_linear_regression \u0026lt;- function(formula, data, ...) {\rmodel \u0026lt;- stan_model(\u0026quot;./linear_regression.stan\u0026quot;)\rX \u0026lt;- model.matrix(formula, data)\ry \u0026lt;- model.extract(model.frame(formula, data), \u0026quot;response\u0026quot;)\rdata \u0026lt;- list(\rN = nrow(X),\rK = ncol(X),\rX = X,\ry = y\r)\rsampling(model, data, ...)\r}\rNow it’s easy to fit the model with different predictors:\nN \u0026lt;- 100\rsimulated_data \u0026lt;- tibble::tibble(\rx1 = rnorm(N, 0, 1),\rx2 = rnorm(N, 0, 1),\ry = x1 + 2*x2 + rnorm(N, 0, 0.1)\r)\rfit_linear_model(y ~ x1, simulated_data)\rfit_linear_model(y ~ x1 + x2, simulated_data)\rWriting a separate function for preparing the data for Stan based on a formula makes the model more usable and flexible.\n","date":1563818400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563818400,"objectID":"a7d78d13e918aa8adf2d4672e7b325dd","permalink":"/2019/07/22/r-formulas-stan/","publishdate":"2019-07-22T18:00:00Z","relpermalink":"/2019/07/22/r-formulas-stan/","section":"post","summary":"Write flexible Stan models by using the R formula interface.","tags":null,"title":"Using R formulas to pass data to Stan","type":"post"},{"authors":null,"categories":null,"content":"One of the themes that I worked on at Silent Spring Institute was on how to report complex personal data to our study participants. In the PROTECT study, mothers in Puerto Rico were tested for a host of environmental chemicals. Our job was to design a tool to report-back individual results to the participants.\nWhile I was on the project, I designed and implemented a novel smartphone interface for communicating personal results to study participants. One aspect was designing a visualization that allowed participants to compare their results to other women in the study. Our approach uses a SinaPlot where the participant\u0026rsquo;s personal results are represented by an avatar that they chose when they enter their report.\nLast May I left Silent Spring Institute to pursue graduate school, and I was happy to see that the tool was launched in October! To learn more, check out the article the PROTECT team wrote about the launch of the reports.\n","date":1554746400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554746400,"objectID":"f32df025b8f59e4a6a1c91aa0b45ef06","permalink":"/2019/04/08/smartphone-interface-for-reporting-research-results-to-study-participants/","publishdate":"2019-04-08T18:00:00Z","relpermalink":"/2019/04/08/smartphone-interface-for-reporting-research-results-to-study-participants/","section":"post","summary":"We developed a novel interface for reporting results to participants of exposure studies.","tags":null,"title":"Smartphone interface for reporting research results to study participants","type":"post"},{"authors":[],"categories":[],"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"Nick Kristof, columnist for the New York Times, recently wrote about Detox Me Action Kit, a study I manage and helped launch at Silent Spring Institute.\nThe column includes a nice graphic summarizing his results:\n Source: The New York Times\nYou can check out the study on the Silent Spring Institute website: Detox Me Action Kit.\n","date":1519581600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519581600,"objectID":"f90a1553478abd13ccd9727aaf42dd87","permalink":"/2018/02/25/what-poisons-are-in-your-body-nick-kristof/","publishdate":"2018-02-25T18:00:00Z","relpermalink":"/2018/02/25/what-poisons-are-in-your-body-nick-kristof/","section":"post","summary":"New York Times columnist Nick Kristof covers the Detox Me Action Kit project.","tags":null,"title":"What Poisons Are in Your Body? - Nick Kristof","type":"post"},{"authors":null,"categories":null,"content":"More from Twitter, to complete a very silly triptych:\n.@common_squirrel spends most of its time running and blinking. One time, it hoped. pic.twitter.com/gO6y3bmSp3\n\u0026mdash; Herb Susmann (@herbps10) February 10, 2018  The answer to @isacatinthesink is more often \u0026quot;no\u0026quot; than \u0026quot;yes\u0026quot;. I\u0026#39;m surprised. Cats love sinks. pic.twitter.com/si1q7VoW1N\n\u0026mdash; Herb Susmann (@herbps10) February 9, 2018 Code for the @common_squirrel plot and the @isacatinthesink plot are available as Github Gists.\n","date":1519560000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519560000,"objectID":"0695d523858037052a81c737b23df1e5","permalink":"/2018/02/25/more-animals-from-twitter/","publishdate":"2018-02-25T12:00:00Z","relpermalink":"/2018/02/25/more-animals-from-twitter/","section":"post","summary":"Analyzing tweets from @common_squirrel and @isacatinthesink.","tags":null,"title":"More animals from Twitter","type":"post"},{"authors":null,"categories":null,"content":"\rFrom Twitter:\nThe most common rating for dogs from @dog_rates is 13/10. A few are 15/10 but all dogs deserve that rating in my opinion. Pictured with Ellie (12/10) from @KatieNicoleF. pic.twitter.com/B46rRDSCRY\r— Herb Susmann (@herbps10) February 3, 2018\r\r\rHere’s the R code I used to generate the histogram:\nlibrary(rtweet)\rlibrary(tidyverse)\rlibrary(stringr)\rlibrary(cowplot)\rlibrary(grid)\rlibrary(jpeg)\rg \u0026lt;- rasterGrob(readJPEG(\u0026quot;ellie.jpg\u0026quot;), interpolate = TRUE)\rtmls \u0026lt;- get_timelines(\u0026quot;dog_rates\u0026quot;, n = 3200)\rratings \u0026lt;- tmls %\u0026gt;%\rfilter(str_detect(text, \u0026quot;t.co\u0026quot;)) %\u0026gt;%\rfilter(!str_detect(text, \u0026quot;^RT\u0026quot;)) %\u0026gt;%\rfilter(!str_detect(text, \u0026quot;Here\u0026#39;s a little more info on Dew\u0026quot;)) %\u0026gt;%\rmutate(rating = map(text, str_extract_all, \u0026quot;1[0-5]/10\u0026quot;),\rrating = map(rating, `[[`, 1)) %\u0026gt;%\runnest(rating) %\u0026gt;%\rcount(rating)\rggplot(ratings, aes(x = rating, y = n)) +\rannotation_custom(g) +\rgeom_col(fill = \u0026quot;white\u0026quot;, alpha = 0.8) +\rlabs(caption = \u0026quot;Data: @dog_rates, photo: @KatieNicoleF\u0026quot;,\rtitle = \u0026quot;577 WeRateDogs™ Ratings\u0026quot;)\rThe R code is also available as a gist.\n","date":1517745600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517745600,"objectID":"6c0b2c6f206715b3dcd042074b70d4db","permalink":"/2018/02/04/theyre-all-good-dogs/","publishdate":"2018-02-04T12:00:00Z","relpermalink":"/2018/02/04/theyre-all-good-dogs/","section":"post","summary":"Analyzing tweets from @dog_rates.","tags":null,"title":"They're all good dogs","type":"post"},{"authors":null,"categories":null,"content":"There have been a few articles in the last couple years that have used traveling salesman problem solvers to find the fastest way to see all the national parks or 72 breweries in the US. I\u0026rsquo;m going to join the trend on a smaller geographic scale by plotting the fastest way to see 18 breweries (including one cider house) in the Boston area.\nThe fastest route takes about 2.5 hours of driving (by your designated driver or using a ride service, of course) from start to finish:\n  If you take 15 minutes at each brewery to have a beer, and it takes 2.5 hours to drive to each one, you could do it all in only 7 hours. Not bad, that\u0026rsquo;s less than a full day at work!\nYou won\u0026rsquo;t be able to drive this yourself if you have a beer at every stop, so you\u0026rsquo;ll either need to find a friend to drive you around for seven hours, or take something like a Lyft. I used the Lyft API to estimate how much it would cost, and it comes in at about $176. If you can split this with three friends, and you pay, say, $8 per drink, your total cost would be $188.\nAm I going to do this? Probably not. I really don\u0026rsquo;t think I could stomach 17 beers and a cider in one day. And think of all the fun I could have programming in R for seven hours, instead. Yeah. Easy choice.\nHere\u0026rsquo;s a list of all the breweries, in order. You could start your tour anywhere, but if I were doing this I\u0026rsquo;d start at Aeronaut, my favorite brewery around here. You\u0026rsquo;d also get to end at Bantam Cider Company to cap off very long night out on a different note.\n Aeuronaut Brewery Winter Hill Brewing Company Idle Hands Craft Ales Night Shift Brewing Bone Up Brewing Company Mystic Brewery Downeast Cider House Boston Beer Works Trillium Brewing Company Harpoon Brewery Dorchester Brewing Company Sam Adams Brewery Turtle Swamp Brewing John Harvards Brewery Lamplighter Brewing Company Cambridge Brewing Company Somerville Brewing Company Bantam Cider Company  If you want to want do this, here\u0026rsquo;s a Google Map with all the breweries entered in order. Good luck. Please drink responsibly.\nP.S. Let me know if I missed any breweries in the area!\nDo it yourself: All the code for this project is on Github. I used the Google Maps API for calculating a distance matrix between all the breweries and to get detailed directions between each point on the final tour. The optimal tour was calculated using the asymmetric traveling salesman problem solver from the TSP R package. The Lyft price estimate came from it\u0026rsquo;s public API. If you want to run the code yourself, you\u0026rsquo;ll need to get a Google Maps API key and a Lyft API key.\n","date":1507550400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1507550400,"objectID":"140abb7283ecb524b96e5c8e1f18f80b","permalink":"/2017/10/09/fastest-way-to-see-17-boston-breweries-and-one-cider-house/","publishdate":"2017-10-09T12:00:00Z","relpermalink":"/2017/10/09/fastest-way-to-see-17-boston-breweries-and-one-cider-house/","section":"post","summary":"Calculating an optimal route between all the Boston area breweries using a Traveling Salesman Problem solver.","tags":null,"title":"Fastest way to see 17 Boston breweries (and one cider house)","type":"post"},{"authors":null,"categories":null,"content":"\rGoal: Predict the winner of Jeopardy. As the game progresses, update the predictions to take into account the current score profile.\nBackground: A quick search revealed work by The Jeopardy Fan on building a model to predict the Tournament of Champions contestants. He used player’s Coryat scores to predict the length of their winning streak, and whether they would qualify for the tournament. I’m going for something slightly different than him by focusing on predicting the winner of a single game. I’m also going to use the real game score, instead of Coryat scores.\nData: The J-Archive is an incredible resource for Jeopardy! data, thanks to the work of their archivists. They have every game ever played, every question asked, along with which contestants answered and whether they were correct or not. I downloaded data from seasons 22-33 for fitting and testing the models.\nIntuition: Let’s explore the dataset a little bit to get a sense of how we might build a classification model to predict winners. You can easily make plots that show how a contestant’s score changes over the course of a game, like this one that shows Roger Craig setting a one-day earnings record:\rExpanding this type of plot beyond a single game, here is a plot showing all of the games from season 27, with each trajectory colored by whether the contestant won the game:\nThe winners tend to have higher scores throughout the game, but you can still see a few people who had high scores and still lost. If we show more seasons at once we can see more of the overall trend, but we lose the ability to see individual trajectories clearly:\nAnother way to look at this is by plotting the median scores, with a ribbon showing the 5% and 95% quantiles:\nIt looks like there is some separation between the winners and losers just in terms of their score, and the separation becomes more pronounced as the game progresses, which a model should be able to pick up on and use for prediction.\nWe should temper our expectations, though. Especially in the first graph, you can see how much of a randomizer the Final Jeopardy round is. Here’s a table showing how the contestant’s rank going in to Final Jeopardy corresponds to winning or losing (data from seasons 22-33):\n\r\rRank\rWon\rLost\r\r\r\rThird\r171 (6%)\r2554 (94%)\r\rTied for second\r8 (12%)\r60 (88%)\r\rSecond\r591 (22%)\r2116 (75%)\r\rTied for first\r17 (47%)\r19 (53%)\r\rFirst\r1991 (73%)\r750 (27%)\r\r\r\r27% of contestants in first place going in to Final Jeopardy still lose. It’s going to be very difficult to accurately predict when an upset will happen, so this gives us a sense of the limits of any prediction model.\nModeling and Results: One of the goals is to predict the winner as the game progresses. In each game of Jeopardy! there are up to 61 questions: 30 in Single Jeopardy, 30 in Double Jeopardy, and 1 in Final Jeopardy. I decided to fit a logistic regression model after every question, so we can see how the classification accuracy improves as the game gets closer to the end.\nI used data from seasons 22-32 for training, and held out season 33 for testing.\nThere are 61 questions in each game; call them \\(q_{1},q_{2},...,q_{i},...,q_{61}\\), where \\(q_{1}\\) is the first question asked in the game, \\(q_{2}\\) is the second, and so on. The actual point value of each question might be different, depending on the order they are chosen in the game (for example, \\(q_{1}\\) might be a $200 question in one game, and a $1,000 question in another.) I fitted 61 logistic regression models \\({M_{1}, M_{2}, ...,M_{n}}\\) that predict whether the player won the game based on their score at the end of question \\(i\\). We would expect model \\(M_{1}\\) to do poorly, because the first question isn’t very informative of who is going to end up winning; and the accuracy to improve as the game progresses.\nFor example, here is the fitted logistic model for question 30:\nWe can visualize the accuracy of the all models at once by plotting their ROC curves.\nAs we would expect, the model has lousy accuracy at the beginning of the game, but improves steadily as the game progresses. However, it is not perfect even after the game is over. This is because the score itself doesn’t determine the winner; what matters is who has the highest score.\nTo address this, I added two new features to bring in information about the contestants compare to each other within the game:\n\rrank - categorical variable indicating the contestant’s current rank (first place, tied for first, second place, etc.)\rdistance from lead - the difference in points between the contestant and the leader. If the contestant is in the lead, it is a negative number indicating how far they are ahead.\r\rI built two new sets of models using these features. I also added a very simple model for comparison: predict the winner of the game to be whoever is in the lead. This model doesn’t output probabilities, so it will show up on the ROC curves as a single point (I call this model “current leader” in the legend.) Here’s how they compare to the original model:\nThe new models aren’t that much better than the original model. The biggest difference I see is that they have perfect accuracy at the end of the game, as you would expect since they have access to the final ranking of the contestants.\nThe “current leader” reference model falls right on the curves, indicating the logistic models don’t do better than it. They may be more useful, though, since they output probabilities rather than a dichotomous prediction.\nVisualizing predictions: Now that we have these models, let’s see a contestant’s probability of winning (conditioned on the model) evolves over the course of a game. I’m going to take the set of models that use the distance from lead predictor and apply them to a game from season 33.\nGavin takes the lead in the prediction model at the same time he takes the lead, in the middle of Double Jeopardy.\nHere’s another one from season 33 where the prediction flips towards the end of the game:\nThe highest probability of winning is assigned to whoever is in the lead, which reflects the logic of the simple reference model.\nNext steps:\rI think a significant shortcoming of the approach I took here is that I fit completely separate models for each question; they aren’t connected in any way, and so they don’t have any “memory” of how each contestant has performed previously in the game (except via their current score), or in prior games. Perhaps there’s a way to incorporate some ideas from partial pooling models to share strength between questions. Or maybe a model could be built that estimates a latent “ability” score for each participant, conditioned on their previous performance. A bonus of this would be that the winners ability score could be fed in to their next game, as a form of prior information about how well the contestant will do. Doing this in a Bayesian framework seems like a good choice.\nI think there are also a few things that could be done to improve performance in the endgame. Right now the models don’t take into account how much money is still available. Incorporating this should help, especially in the end game when there isn’t very much money still on the board. It could also be used to find runaway scores, where one contestant has more money than is possible for a competitor to gain. We could also build a model for predicting the Final Jeopardy bets, so the end game model could have a better understanding of how likely an upset will be.\nFinally, I’d also be interested in changing the goal slightly to predict winners in terms of Coryat score, which I’m sure would perform better since the uncertainty of daily doubles and the Final Jeopardy wager would be removed.\nSource code:\rThe source code for this analysis is on Github: https://github.com/herbps10/jeopardy\n","date":1507377600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1507377600,"objectID":"f50edd4e15c447b027c84186782fa01b","permalink":"/2017/10/07/predicting-jeopardy-winners/","publishdate":"2017-10-07T12:00:00Z","relpermalink":"/2017/10/07/predicting-jeopardy-winners/","section":"post","summary":"Using logistic regression to predict the winner of Jeopardy!","tags":null,"title":"Predicting Jeopardy! Winners","type":"post"},{"authors":null,"categories":null,"content":"\r.banana {\rtext-align: center;\rmargin-bottom: 20px;\r}\r.banana span {\rdisplay: inline-block;\rbackground: #e3dbcf;\rcolor: black;\rborder-radius: 5px;\rtext-align: center;\rbox-shadow: inset 10px 5px 5px 0px rgba(255, 255, 255, 0.1),\rinset -3px -3px 7px 0px rgba(0, 0, 0, 0.2),\rinset 0px 0px 5px 7px rgba(255, 255, 255, 0.2),\r0px 1px 2px 0px rgba(0, 0, 0, 0.3);\r}\rdiv.banana span {\rwidth: 50px;\rheight: 50px;\rfont-size: 18pt;\rline-height: 50px;\rfont-weight: bold;\r}\rspan.banana span {\rwidth: 30px;\rheight: 30px;\r}\r\rThere was a fabled game of Bananagrams in which my Dad drew his initial 11 tiles, and immediately spelled:\nRASTAFARIAN\r\rIf you haven’t played the game, it’s like a free-form version of Scrabble. You start by drawing a number of tiles (typically 11 or 21), and try to form a word or words out of them.\nThe story made me wonder how likely it is to spell an 11 letter word on the first draw.\nThe first step is to calculate the probability of drawing a particular word. Consider a bananagrams bag filled with only two letters, S for success and F for failure. Start pulling out tiles from the bag at random, without replacing each tile back in the bag after drawing it, and count how many S tiles you get. The hypergeometric distribution models the probability that you will get a certain number of S tiles for a given number of draws. The multivariate hypergeometric distribution extends this to the multivariate case; that is, it models the probability you’ll draw a certain number of As, Bs, Cs, etc. after drawing a number of tiles from the bag.\nFortunately, the R package extraDistr provides an R version of the multivariate hypergeometric probability mass function. Here’s a function that, given a word of length \\(N\\) and the number of each letter tile in a bag, gives the probability of drawing that word in \\(N\\) draws:\nUsing this function, we can find the probability of drawingRASTAFARIAN:\nAnd the result is \\(4.28\\times10^{-6}\\%\\). Pretty lucky!\nNow, what is the probability of drawing any valid 11 letter word to start the game? Note that in most cases, spelling a word using all your 11 tiles excludes the possibility of spelling another word. This suggests the the probability of spelling word \\(A\\) OR word \\(B\\) is given by \\(P(A \\cap B)=P(A) + P(B)\\).\nHowever, there is a special case: what if word \\(A\\) and word \\(B\\) are spelled with the same letters? In order to avoid double counting, we need to only want to include words with the same letters once.\nI downloaded a list of words in the SOWPODS scrabble dictionary from a GitHub repository and loaded them into R. To deduplicate words with the same letters, I sorted the letters in each word and removed duplicates:\nI then used the word_probability function to calculate the probability of drawing each 11 letter word, and then summed them all up:\nWhich computes the probability of drawing a valid 11 letter word in the opening tiles to be \\(~0.28\\%\\).\nNow, suppose you start the game by drawing a different number of tiles. We can compute the probability of starting with a valid word for a range of starting tile numbers:\nDrawing 3 letters has the highest probability of forming a word, at \\(53.7\\%\\). This validates my strategy of dumping early in the game to get new tiles when I get stuck, because the new letters often help me get out of the rut.\n","date":1501467194,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501467194,"objectID":"46ca0160a6e13de27c05de2908690562","permalink":"/2017/07/30/bananagrams-probabilities/","publishdate":"2017-07-30T21:13:14-05:00","relpermalink":"/2017/07/30/bananagrams-probabilities/","section":"post","summary":"Calculating the probability of starting with a complete word in Bananagrams.","tags":null,"title":"Bananagrams Probabilities","type":"post"},{"authors":["Katerine E. Boronow","Herb Susmann","Krzysztof Z. Gajos","Ruthann A. Rudel","Kenneth C. Arnold","Phil Brown","Rachel Morello-Frosch","Laurie Havas","Julia Green Brody"],"categories":null,"content":"","date":1485907200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1485907200,"objectID":"0e98638b9f7d4ed905505121c8db40cd","permalink":"/publication/derbi/","publishdate":"2017-02-01T00:00:00Z","relpermalink":"/publication/derbi/","section":"publication","summary":"DERBI is a digital tool for reporting back personal results to participants in exposure studies.","tags":["DERBI, Silent Spring Institute"],"title":"DERBI: A Digital Method to Help Researchers Offer “Right-to-Know” Personal Exposure Results","type":"publication"},{"authors":null,"categories":null,"content":"There is a website with scripts for every episode of Star Trek, so for fun I downloaded them and generated a visualization of which characters were in each episode of Star Trek.\nEnjoy!\n","date":1483012800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483012800,"objectID":"ada8e67090109d7700e79ec8bc7874e9","permalink":"/2016/12/29/who-was-in-each-episode-of-star-trek/","publishdate":"2016-12-29T12:00:00Z","relpermalink":"/2016/12/29/who-was-in-each-episode-of-star-trek/","section":"post","summary":"A visualization of which characters were in each episode of Star Trek.","tags":null,"title":"Who was in each episode of Star Trek?","type":"post"},{"authors":null,"categories":null,"content":"I had fun leading a workshop a few weeks ago on building a crystal radio. We used a simple design that incorporates a loop antenna (doubling as an inductor), a variable capacitor, a germanium diode, and an earpiece.\nTwo pieces of wood make a frame for the antenna. We nailed in picture hanger hooks to the ends to wind wire around to form an antenna. Then we used a few nails to hold the variable capacitor in place, and soldered all the components together.\nThe walls of the library we were working in dampened outside radio waves a lot, but once we stepped outside we could all hear some nice strong AM stations.\n","date":1480334400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480334400,"objectID":"0bea80aabc6b039962e9f4484f93102e","permalink":"/2016/11/28/build-a-crystal-radio/","publishdate":"2016-11-28T12:00:00Z","relpermalink":"/2016/11/28/build-a-crystal-radio/","section":"post","summary":"We built a crystal radio sets at a workshop for teens.","tags":null,"title":"Build a Crystal Radio","type":"post"},{"authors":null,"categories":null,"content":"Last year I read The Cigarette Century by Allan M. Brandt, a history of the tobacco industry and a finalist for a Pulitzer Prize. One of the points that has stuck with me is how regulations that were intended to curtail the tobacco industry ended up benefiting them.\nIn what appeard to be a blow to the tobacco industry, the advertising of tobacco products on the radio and television was banned by the FCC in 1969. In 1967, a lawyer named John F. Banzhaf III successfully petitioned the FCC under the fairness doctrine to force radio and television to play anti-tobacco public service announcements if they played tobacco company ads. When the tobacco ads went off the air, so did the public service announcements. This was to the benefit of the tobacco industry who desperately wanted to suppress anti-tobacco information. Furthermore, the ban on advertising saved money being spent on expensive ad campaigns (the industry spent $230 million on television advertising in 1970 alone.)\nIn 1972, the FTC was finally able to require warning labels on tobacco products. Again, this perceived concession to the tobacco industry provided them with alternative benefits. Now the tobacco companies could argue that smokers were clearly warned of the health risks of smoking, and that they made an informed decision to start smoking despite the risks. Because these risks were accepted, the companies argued, they should not be liable for resulting health issues.\nThe tobacco companies negotiated regulations that, while appearing to be concessions, offered them benefits. That they were successful shows their savviness and the difficulty regulators have in reigning in companies that are determined to preserve their business model, and the profits that go along with it.\n","date":1477051200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1477051200,"objectID":"c436e17b258a932c6627b6068f2f9eea","permalink":"/2016/10/21/tobacco-industry-concessions-that-werent-concessions/","publishdate":"2016-10-21T12:00:00Z","relpermalink":"/2016/10/21/tobacco-industry-concessions-that-werent-concessions/","section":"post","summary":"Regulations had positive side effects for the tobacco industry","tags":null,"title":"Tobacco Industry Concessions that Weren't Concessions","type":"post"},{"authors":null,"categories":null,"content":"Long streaks are rare in Jeopardy. Most winners only win one game, and slightly less than 40% win two games in a row. In fact, only 6 contestants have won more than ten games in a row.\nThis Kaplan-Meier survival plot visualizes the survival function of Jeopardy winners:\nThe data includes seasons 1-33 (aired 1984-2016), and does not include any championship or tournament games. The point on the extreme right is due, of course, to Ken Jennings and his 74 game winning streak.\nHere\u0026rsquo;s the R code for the figure.\n","date":1466337600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1466337600,"objectID":"2bbd44e5370cd39be15de25629201f2e","permalink":"/2016/06/19/jeopardy-survival-analysis/","publishdate":"2016-06-19T12:00:00Z","relpermalink":"/2016/06/19/jeopardy-survival-analysis/","section":"post","summary":"A Kaplan-Meier survival plot of Jeopardy winning streaks.","tags":null,"title":"Jeopardy! Survival Analysis","type":"post"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"7f2fdd86e7162859c3b70f337d2940b3","permalink":"/project/rnhanes/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/rnhanes/","section":"project","summary":"R package for accessing and analyzing CDC NHANES data","tags":["R, 📦"],"title":"RNHANES","type":"project"},{"authors":null,"categories":null,"content":"I was fortunate enough to be able to present at UP-Stat 2014 on some of the things I\u0026rsquo;ve learned about writing performant R code while I was working on speeding up an R package for fitting mixed effects nested models. The talk seemed to be a big hit - I was awarded \u0026ldquo;Best Student Presentation\u0026rdquo;!\n ","date":1398132794,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1398132794,"objectID":"d0d147501a0cd29995e4532d98d2238e","permalink":"/2014/04/21/performant-r/","publishdate":"2014-04-21T21:13:14-05:00","relpermalink":"/2014/04/21/performant-r/","section":"post","summary":"Presentation on writing performan R packages that won a best student presentation award.","tags":["r"],"title":"Performant R","type":"post"},{"authors":null,"categories":null,"content":"Last semester I took an introductory course in raytracing.\nWe practiced an iterative development cycle in which we built up more and more complex ray tracers over the course of the semester. The very first ray tracer was pretty simple: it had to be able to intersect rays with simple geometric objects and display the results, but there didn’t have to be any lighting calculations or anything yet.\nOnce I figured out the assignment in OCaml, I decided to give it a shot entirely in Bash!\n(Well, not ENTIRELY in Bash. I will admit I shelled out to bc for floating point operations. A friend pointed out you could do floating point in Bash by having seperate variables for the integer and decimal components, but that’ll have to wait for version two)\nIt prints out the raytraced image directly to the console using special unicode characters and coloring through escape codes. Here’s what the result looks like:\nNot exactly pretty, but it works! (if only there was a way to change the line spacing in gnome-terminal).\nThe script is available as a gist: raytracer.sh\n","date":1396663994,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1396663994,"objectID":"11d6ff2660d89556feb54418b2f23957","permalink":"/2014/04/04/raytracing-in-bash/","publishdate":"2014-04-04T21:13:14-05:00","relpermalink":"/2014/04/04/raytracing-in-bash/","section":"post","summary":"It turns out it is possible to write a minimal raytracer in Bash.","tags":["bash","raytracing"],"title":"Raytracing in Bash","type":"post"},{"authors":null,"categories":null,"content":"\rAutoregressive (AR) processes are a popular choice for modeling time-varying processes. AR processes are typically written down as a set of conditional distributions, but if we do some algebra we can show how they can also be written as a Gaussian process. Having a Guassian process representation is useful because it is more clear how the AR process could be incorporated into larger models, like a spatio-temporal model. In this post, we’ll start with defining an AR process and deriving its mean and variance, then we’ll derive its joint distribution, which is a Gaussian process.\nAR processes\rLet \\(\\mathbf{Y} = \\left\\\\{ Y_1, Y_2, \\dots, Y_n \\right\\\\}\\) be a set of random variables indexed by time. An aurogressive model assumes that \\(\\mathbf{Y}\\) is correlated over time. An AR model is typically described by defining \\(Y_t\\) in terms of \\(Y_{t-1}\\):\n\\[\rY_{t} = \\rho Y_{t-1} + \\epsilon_{t}\r\\]\rwhere \\(\\epsilon_{t}\\sim N\\left(0,\\sigma_{\\epsilon}^{2}\\right)\\) and \\(\\rho \\in \\mathbb{R}\\) is a parameter that controls the degree to which \\(Y_t\\) is correlated with \\(Y_{t-1}\\). This model is called an AR process of order 1 because \\(Y_t\\) only depends on \\(Y_{t-1}\\).\nWe can also rearrange terms to emphasize that this representation defines the conditional distribution of \\(Y_{t}\\) given \\(Y_{t-1}\\):\n\\[\r\\begin{aligned}\rY_{t} \\vert Y_{t-1} \\sim\u0026amp; N(\\rho Y_{t-1}, \\sigma_\\epsilon^2) \\\\\rY_1 \\sim\u0026amp; N(0, \\frac{\\sigma_\\epsilon^2}{1-\\rho^2})\r\\end{aligned}\r\\]\nWhere the variance of \\(Y_1\\) comes from the unconditional variance, which is derived below. The stationarity condition of an AR process is that each \\(Y_t\\) has the same distribution; that is, \\(\\mu = \\mathrm{E}(Y_i) = \\mathrm{E}Y_j\\) and \\(\\sigma^2 = \\mathrm{Var}(Y_i) = \\mathrm{Var}(Y_j)\\) for all \\(i, j\\).\nNow we can derive the unconditional mean and variance of \\(Y_t\\):\n$$\r\\begin{aligned}\r(Y_{t}) \u0026amp; = (Y_{t - 1} + {t} )\\\r\u0026amp; = ( Y{t - 1 } )\\\r\u0026amp; = \\\r\u0026amp; = 0 \\\n(Y_{t}) \u0026amp; = (Y_{t-1} + {t})\\\r\u0026amp; = ^{2}(Y{t-1}) + ({t})\\\r\u0026amp; = ^{2}(Y{t-1}) + {}{2}\\\r{2} \u0026amp; = {2}{2} + {}{2} \\\r{2}(1-^{2}) \u0026amp; = _{}{2}\\\r{2} \u0026amp; = \\end{aligned}\r$$\nThe plot below shows several examples of draws from an AR(1) process with differing values of \\(\\rho\\) and \\(\\sigma^2_\\epsilon = 1\\):\r\rGaussian processes\rGaussian processes model a set of variables as being multivariate normally distributed with mean \\(\\boldsymbol{\\mu}\\) and variance/covariance matrix \\(\\boldsymbol{\\Sigma}\\):\n\\[\r\\mathbf{Y} \\sim MVN(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\r\\]\nUsually the mean vector is set to \\(\\boldsymbol{0}\\), which means the Gaussian process is fully defined by its choice of variance/covariance matrix \\(\\boldsymbol{\\Sigma}\\). The variance/covariance matrix is defined by a kernel function which defines the covariance between any two variables:\n\\[\r\\Sigma_{i,j} = K(i, j)\r\\]\n\rAn AR(1) process is a Gaussian process\rWe want to show that an AR process can be represented as a Gaussian process. To do this, we need to show that \\(\\mathbf{Y}\\) is jointly normally distributed with some mean vector and variance/covariance matrix.\nWe already know that \\(\\mathrm{E}(Y_t)=0\\), so the mean vector of its joint distribution will be \\(\\mathbf{0}\\).\nTo find the variance/covariance matrix, we need to derive the covariance between \\(Y_{t_1}\\) and \\(Y_{t_2}\\). First, let’s consider the simpler case of the covariance between \\(Y_t\\) and \\(Y_{t+1}\\):\n\\[\r\\begin{aligned}\r\\operatorname{cov}(Y_{t}, Y_{t+1}) \u0026amp;= \\operatorname{E} \\left[ \\left( Y_t - \\operatorname{E}[Y_t] \\right) \\left( Y_{t+1} - \\operatorname{E}[Y_{t+1}] \\right) \\right] \\text{ (definition of covariance) } \\\\\r\u0026amp;= \\operatorname{E} \\left[ Y_t Y_{t+1} \\right] \\text{ (because } \\operatorname{E}[Y_t] = \\operatorname{E}[Y_{t+1}] = 0 \\text{)} \\\\\r\u0026amp;= \\operatorname{E} \\left[ Y_t \\left( \\rho Y_{t} + \\epsilon_{t+1} \\right) \\right] \\\\\r\u0026amp;= \\operatorname{E} \\left[ \\rho Y_t^2 + Y_t \\epsilon_{t+1} \\right] \\\\\r\u0026amp;= \\rho \\operatorname{E}\\left[ Y_t^2 \\right] \\\\\r\u0026amp;= \\rho (\\operatorname{Var}(Y_t) + \\operatorname{E}[Y_t]^2) \\\\\r\u0026amp;= \\rho \\frac{\\sigma_\\epsilon^2}{1 - \\rho^2}\r\\end{aligned}\r\\]\nfor \\(Y\\)s separated by more than one time point, iterating the above result yields the expression\n\\[\r\\begin{aligned}\r\\operatorname{cov}(Y_{t_1}, Y_{t_2}) = \\rho^{\\vert t_1 - t_2 \\vert} \\frac{\\sigma_\\epsilon^2}{1 - \\rho^2}\r\\end{aligned}\r\\]\nNow we can fully define the joint distribution of \\(\\mathbf{Y}\\):\n\\[\r\\mathbf{Y} \\sim MVN(\\mathbf{0}, \\boldsymbol{\\Sigma})\r\\]\nwhere \\(\\Sigma_{i,j} = \\rho^{\\vert i - j \\vert} \\frac{\\sigma_\\epsilon^2}{1-\\rho^2}\\). This is a Gaussian process!\n\rCombining kernel functions\rThe nice thing about Gaussian processes is that we can combine multiple kernel functions to model processes with dependence from different sources. Two ways kernels can be combined are by multiplication and addition. Multiplying two kernels is like an “AND” operation: the correlation between points will be high if the correlation from both kernels is high. Adding two kernels together is like an “OR” operation: correlation is high if either kernel indicates high covariance.\nAs an example, let’s build a Gaussian process that combines an AR process (for temporal correlation) and a spatial process (for spatial correlation) by combining two kernel functions. First, we need to define an outcome variable \\(Y\\) that varies in time and space: let \\(Y_{c,t}\\) be a random variable indexed by spatial site \\(c\\) at timepoint \\(t\\). We take the AR covariance as the first kernel function, to model temporal correlation:\n\\[\rK_1(i, j) = \\rho^{\\vert t_i - t_j \\vert} \\frac{\\sigma_\\epsilon^2}{1 - \\rho^2}\r\\]\nand a squared-exponential kernel function to model spatial dependence:\n\\[\rK_2(i, j) = \\alpha^2 \\exp\\left( -\\frac{d(i, j)}{2\\lambda^2} \\right)\r\\]\nwhere \\(d(i, j)\\) is the spatial distance between sites \\(i\\) and \\(j\\), \\(\\lambda\\) is a length-scale parameter, and \\(\\alpha^2\\) is a parameter controlling the magnitude of the covariance.\nCombine the two kernel functions so that two data points are correlated if they are close together in time and space:\n\\[\r\\begin{aligned}\rK(i, j) \u0026amp;= K_1(i, j) \\times K_2(i, j) \\\\\r\u0026amp;= \\rho^{\\vert t_i - t_j \\vert} \\frac{\\sigma_\\epsilon^2}{1 - \\rho^2} \\alpha^2 \\exp\\left( -\\frac{d(i, j)}{2\\lambda^2} \\right)\r\\end{aligned}\r\\]\nNote the parameters \\(\\sigma^2_\\epsilon\\) and \\(\\alpha^2\\), which are multipled together, would be unidentifiable in parameter estimation and should be replaced by a single parameter that controls the magnitude of the covariance.\nTo illustrate this Gaussian process model, I started by generating a set of sites with random locations:\nthen I drew from the Gaussian process using the parameters temporal parameters \\(\\rho=0.9\\), \\(\\sigma_\\epsilon^2=1\\) and spatial parameters \\(\\alpha = 1\\) and \\(\\lambda=2\\).\nThe plot below shows the time trend in the first six sites:\nAnd the spatial distribution over time of \\(Y_{c,t}\\) is shown below:\rVisually we can see that the Gaussian process generates data that is correlated in both time and space.\n\rModeling using the mean and the covariance\rThe spatio-temporal Gaussian process we defined in the previous section does its modeling through the variance/covariance matrix, with its mean function set to zero. An alternative way to think about a spatio-temporal process is akin to the first AR representation we looked at, and define \\(\\mathbf{Y}_t\\) (the set of all \\(Y_{c,t}\\) at time \\(t\\)) relative to \\(\\mathbf{Y}_{t-1}\\):\n\\[\r\\begin{aligned}\r\\mathbf{Y}_{t} = \\rho \\mathbf{Y}_{t-1} + \\boldsymbol{\\epsilon}_t\r\\end{aligned}\r\\]\nwhere \\(\\boldsymbol{\\epsilon_t} \\sim MVN(\\mathbf{0}, \\boldsymbol{\\Sigma}_\\epsilon)\\).\nIf we set \\(\\boldsymbol{\\Sigma_\\epsilon}\\) to be the diagonal matrix \\(\\boldsymbol{\\Sigma}_\\epsilon = \\sigma^2_\\epsilon \\mathbf{I}_n\\) then we will have an independent AR(1) independent process for each spatial site. It gets more interesting if we define \\(\\boldsymbol{\\Sigma}_\\epsilon\\) by a covariance function so we can include dependence between sites, for example dependence based on the distance between the sites. For now, let’s use the squared exponential kernel and define \\(\\Sigma_{i,j} = \\alpha^2 \\exp\\left(-\\frac{d(i, j)}{2\\lambda^2} \\right)\\).\nIs this process also equivalent to a mean zero Gaussian process with some covariance kernel? We’ll answer this question by deriving the covariance between any two points.\nThe mean of \\(\\mathbf{Y_t}\\) can be shown to be zero in the same way we showed a univariate AR process has mean 0. We also need to know the overall variance/covariance matrix of \\(\\mathbf{Y}_t\\), which we’ll call \\(\\boldsymbol{\\Phi}\\); the logic is imilar to the univariate case, and I’ll show it here for completeness:\n\\[\r\\begin{aligned}\r\\operatorname{Var}\\left(\\boldsymbol{Y}_{t}\\right) \u0026amp; =\\operatorname{Var}\\left(\\rho^{2}\\mathbf{Y}_{t-1} + \\boldsymbol{\\epsilon}_{t}\\right) \\\\\r\u0026amp;= \\rho^{2}\\operatorname{Var}\\left(\\boldsymbol{Y}_{t-1}\\right)+\\operatorname{Var}\\left(\\boldsymbol{\\epsilon}_{t}\\right) \\\\\r\\boldsymbol{\\Phi} \u0026amp; =\\rho^{2}\\boldsymbol{\\Phi}+\\boldsymbol{\\Sigma}_\\epsilon \\\\\r\\boldsymbol{\\Phi}-\\rho^{2}\\boldsymbol{\\Sigma} \u0026amp;= \\boldsymbol{\\Sigma}_\\epsilon \\\\\r\\boldsymbol{\\Phi}\\left(\\mathbf{I}-\\rho^{2}\\mathbf{I}\\right) \u0026amp; =\\boldsymbol{\\Sigma}_\\epsilon \\\\\r\\boldsymbol{\\Phi} \u0026amp;=\\boldsymbol{\\Sigma}_{\\epsilon}\\left(\\mathbf{I}-\\rho^{2}\\mathbf{I}\\right)^{-1}\r\\end{aligned}\r\\]\nIf we pull out two sites at the same time point, their covariance is \\(\\mathrm{cov}(Y_{t,c_1}, Y_{t,c_2}) = \\frac{\\Sigma_{\\epsilon, c_1, c_2}}{1-\\rho^2}\\), which looks very similar to the unidimensional AR(1) process variance.\nNow we derive the covariance between any two sites that are one time point apart:\n\\[\r\\begin{aligned}\r\\mathrm{cov}\\left(y_{c_1,t},y_{c_2,t+1}\\right) \u0026amp; =\\mathrm{E}\\left[\\left(y_{c_1,t}-\\mathrm{E}\\left[y_{c_1,t}\\right]\\right)\\left(y_{c_2,t}-\\mathrm{E}\\left[y_{c_2,t}\\right]\\right)\\right]\\\\\r\u0026amp; =\\mathrm{E}\\left[y_{c_1,t}y_{c_2,t}\\right]\\\\\r\u0026amp; =\\mathrm{E}\\left[y_{c_1,t}\\left[\\rho y_{c_2,t}+\\epsilon_{c_2,t+1}\\right]\\right]\\\\\r\u0026amp; =\\rho\\mathrm{E}\\left[y_{c_1,t}y_{c_2,t}\\right]\\\\\r\u0026amp; =\\rho\\mathrm{cov}\\left(y_{c_1,t}y_{c_2,t}\\right)\\\\\r\u0026amp; =\\rho\\frac{\\Sigma_{i,j}}{1-\\rho^2} \\\\\r\u0026amp;= \\rho \\frac{1}{1-\\rho^2} \\Sigma_{i,j} \\\\\r\u0026amp;= \\rho \\frac{1}{1-\\rho^2} \\alpha^2 \\exp\\left(-\\frac{d(i, j)}{2\\lambda^2} \\right)\r\\end{aligned}\r\\]\nfor sites more than one time point away from each other, we can iterate the above result to get a general expression of the covariance between any two points:\n\\[\r\\begin{aligned}\r\\mathrm{cov}\\left(y_{c_1,t_1},y_{c_2,t_2}\\right) \u0026amp;= \\rho^{\\vert t_1 - t_2 \\vert}\\frac{1}{1-\\rho^2} \\alpha^2 \\exp\\left(-\\frac{d(i, j)}{2\\lambda^2} \\right)\r\\end{aligned}\r\\]\nif we reparameterize \\(\\alpha\\) to be the product of two parameters \\(\\alpha = \\sigma^2_\\epsilon \\alpha\\), we get\n\\[\r\\begin{aligned}\r\\mathrm{cov}\\left(y_{c_1,t_1},y_{c_2,t_2}\\right) \u0026amp;= \\rho^{\\vert t_1 - t_2 \\vert}\\frac{\\sigma^2_\\epsilon}{1-\\rho^2} \\alpha^2 \\exp\\left(-\\frac{d(i, j)}{2\\lambda^2} \\right) \\\\\r\u0026amp;= K_1(i, j) \\times K_2(i,j)\r\\end{aligned}\r\\]\nwhich is the product of an AR(1) and squared exponential kernel function as defined in the previous section. In practice we wouldn’t want to separate these parameters because both of them will not be identifiable given observed data, but I separated them here to show how the covariance structure is the product of two kernel functions.\nTherefore, we can write this process in the form of a Gaussian process with mean zero and covariance kernel given by the product of a temporal and spatial kernel:\n\\[\r\\begin{aligned}\r\\mathbf{Y} \\sim\u0026amp; MVN(\\mathbf{0}, \\boldsymbol{\\Sigma}) \\\\\r\\Sigma_{i,j} =\u0026amp; K_1(i, j) \\times K_2(i, j) \\end{aligned}\r\\]\nThe spatio-temporal processes defined as a set of conditional distributions and as a joint Gaussian process are equivalent.\nTo summarize, AR processes can be written as a Gaussian process model, which is useful because a temporal process can then be easily combined with other sources of dependence. In general, we can build our models by defining conditional distributions with a given mean and covariance, or a joint distribution with mean zero where the model is fully defined by a variance/covariance kernel function. In a future post I will look at Bayesian parameter estimation in these models using Stan.\n\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"341c20a888a3edbdbc9eb7d70be5b24a","permalink":"/1/01/01/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/1/01/01/","section":"post","summary":"Autoregressive (AR) processes are a popular choice for modeling time-varying processes. AR processes are typically written down as a set of conditional distributions, but if we do some algebra we can show how they can also be written as a Gaussian process. Having a Guassian process representation is useful because it is more clear how the AR process could be incorporated into larger models, like a spatio-temporal model. In this post, we’ll start with defining an AR process and deriving its mean and variance, then we’ll derive its joint distribution, which is a Gaussian process.","tags":null,"title":"","type":"post"}]
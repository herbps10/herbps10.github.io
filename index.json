[{"authors":["admin"],"categories":null,"content":"I grew up near Ithaca, New York, where I was home-schooled for elementary school, an experience that shaped my belief in the value of self-directed, intrinsically motivated learning. I went to college at SUNY Geneseo where I received a degree in Mathematics in 2014. I spent four years working as a software developer at Silent Spring Institute where I developed digital tools for communicating complex scientific data to a lay audience. I began a doctoral program in biostatistics at UMass Amherst in 2018, where I am working with Leontine Alkema on spatial Bayesian modeling.\n","date":1570579200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1570579200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I grew up near Ithaca, New York, where I was home-schooled for elementary school, an experience that shaped my belief in the value of self-directed, intrinsically motivated learning. I went to college at SUNY Geneseo where I received a degree in Mathematics in 2014. I spent four years working as a software developer at Silent Spring Institute where I developed digital tools for communicating complex scientific data to a lay audience.","tags":null,"title":"Herb Susmann","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":[],"categories":[],"content":"\rIn the first post of this series, we looked at how to draw from the joint distribution of a Gaussian Process and its derivative. In this post, we will show how to condition a Gaussian Process on derivative observations. Overall this is pretty straightforward because the conditional distribution of the multivariate normal has a closed form, although applying it in this context requires some somewhat tedious notation and bookkeeping.\nI’m going to take an aside to introduce a slightly different notation than I used in the last post. The new notation looks at Gaussian Processes as priors over a function class. It takes a bit of setup, and we will end up re-writing some of what was already done in the first post, but overall it’s a useful (and more rigorous) way of talking about Gaussian Processes.\nLet’s say we we have some observations of a univariate function \\(f(x) : \\mathbb{R} \\mapsto \\mathbb{R}\\). We wish to use these observations to estimate the function \\(f\\) over its entire domain. Without any prior knowledge, it could be any of an infinity of possible functions. The set of all possible functions that \\(f\\) could be is called a function space. In this example, \\(f\\) is in the space of all functions that map numbers in \\(\\mathbb{R}\\) to \\(\\mathbb{R}\\).\nWe could stop here and say that \\(f\\) could truly be any function. However, in many real world settings we actually do know something about what the function can look like. Broadly, we probably know something about the smoothness of \\(f\\). In some cases we might also know that it should be periodic. Gaussian Processes provide a flexible way to encode these types of prior knowledge about \\(f\\) as a prior over an infinite dimensional function space.\nWhen thinking about Gaussian Processes as a prior over a function space, the notation is slightly different than in the previous post. We write:\r\\[\rf \\sim \\mathcal{GP}(\\mu(x), k(x_i, x_j))\r\\]\rwhere \\(\\mu\\) is a function that gives the mean of the GP at any point \\(x\\), and \\(k\\) is a kernel function that can be used to form a covariance matrix between any points \\(x_i\\) and \\(x_j\\).\nSetting this prior places a restriction on the functions \\(f\\): for any finite set of points \\(\\bm{x}\\) in the domain of \\(f\\), the corresponding function values \\(f(\\bm{x})\\) will be multivariate normally distributed:\r\\[\rf(\\bm{x}) \\sim MVN\\left( \\mu(\\bm{x}), k(\\bm{x}, \\bm{x}) \\right)\r\\]\rThis connects the idea of a Gaussian Process as a prior distribution over a function space to its practical implementation as a multivariate normal distribution over data points.\nIn practice, using GPs mostly involves thinking about and manipulating multivariate normal distributions. Looking at GPs theoretically as a prior over a function space, however, is a useful way to interpret them, especially when we start getting into using Bayesian inference to fit GPs.\nNow, back to the main subject of this post. Suppose we observe \\(N\\) observations \\(\\bm{y} = \\{y_1, y_2, \\cdots, y_n \\}\\) at points \\(\\bm{x} = \\{x_1, x_2, \\cdots, x_n \\}\\) of a function \\(f\\). In addition, we have \\(N^\\prime\\) observations \\(\\bm{y}^\\prime\\) at points \\(\\bm{x}^\\prime\\) of the derivative \\(f^\\prime\\) of \\(f\\).\nWe wish to use both sets of observations to estimate values of the function \\(f\\) at a new set of points, which we will call \\(\\bm{\\tilde{x}}\\).\nFirst, we place a mean-zero Gaussian Process prior with kernel function \\(k\\) on the function \\(f\\):\r\\[\rf \\sim \\mathcal{GP}(0, k_{00}(x_i, x_j))\r\\]\rwhere the \\(0\\) is a slight abuse of notation indicating that the mean function is always zero, and \\(k_{00}\\) is the kernel function that gives the covariance between function values at \\(x_i\\) and \\(x_j\\).\nAs we saw in the last post, the derivative of a Gaussian Process is also a Gaussian Process. Setting a GP prior on \\(f\\) therefore implies a GP prior on \\(f^\\prime\\):\r\\[\rf^\\prime \\sim \\mathcal{GP}(0, k_{11}(x_i, x_j))\r\\]\rwhere \\(k_{11}\\) is the derivative of the kernel function, giving the covariance between the function derivatives at \\(x_i\\) and \\(x_j\\).\nTaken together, this implies that the finite set of realizations of the function and of the function’s derivative that we have observed will have multivariate normal distributions:\r\\[\r\\begin{aligned}\rf(\\bm{x}) \u0026amp;\\sim MVN(\\bm{0}, k_{00}(\\bm{x}, \\bm{x})) \\\\\rf^\\prime(\\bm{x}^\\prime) \u0026amp;\\sim MVN(\\bm{0}, k_{11}(\\bm{x}, \\bm{x}))\r\\end{aligned}\r\\]\nWhat’s more, the joint distribution of \\(f(\\bm{x})\\) and \\(f^\\prime(\\bm{x}^\\prime)\\) is multivariate normally distributed:\r\\[\r\\begin{bmatrix}f\\left(\\bm{x}\\right)\\\\\rf^{\\prime}\\left(\\bm{x}^{\\prime}\\right)\r\\end{bmatrix}\\sim MVN\\left(\\begin{bmatrix}\r\\bm{0}\\\\\r\\bm{0}\r\\end{bmatrix}, \\begin{bmatrix}\rk_{00}(\\bm{x}, \\bm{x}) \u0026amp; k_{01}(\\bm{x}, \\bm{x}^\\prime)\\\\\rk_{10}(\\bm{x}^\\prime, \\bm{x}) \u0026amp; k_{11}(\\bm{x}^\\prime, \\bm{x}^\\prime)\r\\end{bmatrix}\\right)\r\\]\rThe functions \\(k_{01}\\) and \\(k_{10}\\) are partial derivatives of the kernel function; see the previous post for a full definition.\nNow, after all of this notation and setup, we are ready to actually do some prediction. We would like to condition on the observed values \\(\\bm{y}\\) (of the function) and \\(\\bm{y}^\\prime\\) (of the derivative) to predict the values of \\(f\\) at a new set of points \\(\\tilde{\\bm{x}}\\).\nTo make this easier, we’re going to concatenate our observations into one big vector. That is, we make a new vector \\(\\bm{y}^{all}\\) that combines \\(\\bm{y}\\) and \\(\\bm{y}^\\prime\\), \\(\\bm{x}^{all}\\) which combines \\(\\bm{x}\\) and \\(\\bm{x}^\\prime\\), and a vector \\(\\bm{d}^{all}\\) which indicates whether each element is a function or derivative observation.\nFirst, let’s generate some observed data. I’m going to take one draw from joint GP and its derivative, using the code from the last post, which we’ll use as the true value of \\(f\\):\nset.seed(9)\r# Set hyperparameters\ralpha \u0026lt;- 1\rl \u0026lt;- 1\r# Points at which to observe the function and its derivative\rx \u0026lt;- rep(seq(0, 10, 0.1), 2)\rd \u0026lt;- c(rep(0, length(x) / 2), rep(1, length(x) / 2))\r# Joint covariance matrix\rSigma \u0026lt;- joint_covariance_from_kernel(x, d, k_all, alpha = alpha, l = l)\r# Draw from joint GP\ry \u0026lt;- gp_draw(1, x, Sigma)[1, ]\rNow let’s choose a few function and derivative values which we’ll use as our observed data:\n# Pick a few function and derivative values to use as observed data, making\r# sure to pick an equal number of each type\rN \u0026lt;- 10\robserved_indices \u0026lt;- c(\rsample(which(d == 0), N / 2),\rsample(which(d == 1), N / 2)\r)\r# We\u0026#39;ll call the observed data y_all so that it matches with the math notation\rx_all \u0026lt;- x[observed_indices]\ry_all \u0026lt;- y[observed_indices]\rd_all \u0026lt;- d[observed_indices]\rLet’s plot the observed values, along with the true value of the function:\n# Create a data frame for plotting\rf \u0026lt;- tibble(\rx = x,\ry = y,\robserved = seq_along(x) %in% observed_indices,\rd = d\r)\rggplot(f, aes(x = x, y = y)) +\rgeom_line(aes(lty = \u0026quot;True value\u0026quot;)) +\rgeom_point(data = filter(f, observed), aes(color = observed), size = 2) +\rfacet_wrap(~d, ncol = 1, labeller = as_labeller(c(\u0026quot;0\u0026quot; = \u0026quot;Function\u0026quot;, \u0026quot;1\u0026quot; = \u0026quot;Derivative\u0026quot;)))\rWith the data combined into one long vector, the joint distribution of the observed data \\(\\bm{y}^{all}\\) is now given by\r\\[\r\\bm{y}^{all} \\sim MVN\\left(\\bm{0}, k^{all}(\\bm{x}^{all}, \\bm{x}^{all}, \\bm{d}^{all}, \\bm{d}^{all}) \\right)\r\\]\nwhere \\(k^{all}\\) is a function which computes the covariance between points, choosing the right covariance formula to use depending on if a point is a derivative or function value:\r\\[\rk^{\\mathrm{all}}(x_i, x_j, d_i, d_j) = \\begin{cases}\rk(x_i, x_j) \u0026amp; d_i = 0, d_j = 0 \\text{ (both normal observations)} \\\\\rk_{01}(x_i, x_j) \u0026amp; d_i = 0, d_j = 0 \\text{ (one derivative, one normal)} \\\\\rk_{10}(x_i, x_j) \u0026amp; d_i = 1, d_j = 0 \\text{ (one derivative, one normal)} \\\\\rk_{11}(x_i, x_j) \u0026amp; d_i = 1, d_j = 0 \\text{ (both derivatives)}\r\\end{cases}\r\\]\nWe want to predict both the function values and derivatives at a set of new points \\(\\tilde{\\bm{x}}\\). Define \\(\\tilde{\\bm{x}}^{all}\\) to be a vector formed by repeating \\(\\tilde{\\bm{x}}\\) twice, with \\(\\tilde{\\bm{d}}^{all} = \\left\\{0, 0, \\cdots, 0, 1, 1, \\cdots, 1\\right\\}\\) indicating whether each element of \\(\\tilde{\\bm{x}}\\) refers to a function or derivative prediction.\nx_tilde \u0026lt;- seq(0, 10, 0.1) # Prediction points\rx_tilde_all \u0026lt;- c(x_tilde, x_tilde)\rd_tilde_all \u0026lt;- c(rep(0, length(x_tilde)), rep(1, length(x_tilde)))\rFortunately, the conditional distribution of a multivariate normal distribution has a closed form. Applying the formula, we arrive at\r\\[\r\\tilde{\\bm{y}} \\mid \\tilde{\\bm{x}}^{all}, \\bm{y}^{all}, \\bm{x}^{all} \\sim MVN(\\bm{K}^\\top \\bm{\\Sigma}^{-1} \\bm{y}^{all}, \\bm{\\Omega} - \\bm{K}^\\top \\bm{\\Sigma}^{-1} \\bm{K})\r\\]\rwhere\n\r\\(\\bm{\\Sigma}\\) is the covariance matrix between observed points \\(\\bm{x}^{all}\\)\r\\(\\bm{\\Omega}\\) is the covariance matrix between the new points \\(\\bm{\\tilde{x}}\\)\r\\(\\bm{K}\\) is the covariance matrix between the observed points \\(\\bm{x}^{all}\\) and new points \\(\\bm{\\tilde{x}}\\)\r\rFirst, let’s compute each one of these matrices:\n# Covariance between the observed points\rSigma \u0026lt;- joint_covariance_from_kernel(x_all, d_all, k_all, alpha = alpha, l = l)\r# Due to computational floating point issues, it is sometimes necessary to\r# add a small constant to the diagonal of Sigma to make sure it\u0026#39;s not singular\rSigma \u0026lt;- Sigma + diag(1e-4, nrow(Sigma))\r# Covariance between the prediction points\rOmega \u0026lt;- joint_covariance_from_kernel(x_tilde_all, d_tilde_all, k_all, alpha = alpha, l = l)\r# Covariance between the observed and prediction points\r# We calculate K by computing the covariance between x_all and x_tilde_all\rK \u0026lt;- outer(1:length(x_all), 1:length(x_tilde_all),\rfunction(i, j) k_all(x_all[i], x_tilde_all[j], d_all[i], d_tilde_all[j], alpha = alpha, l = l))\rThen apply the conditioning formula to get the conditional mean and covariance matrix:\nmu_conditional \u0026lt;- t(K) %*% solve(Sigma) %*% y_all\rSigma_conditional \u0026lt;- Omega - t(K) %*% solve(Sigma) %*% K\rFor plotting, it’s helpful to extract the marginal variances of each prediction, which are the diagonal entries of the conditional covariance matrix. We can also use this to get a 95% prediction interval:\nvariance_conditional \u0026lt;- diag(Sigma_conditional)\r# Floating point errors sometimes causes the variances to be very close to zero, but negative.\r# if this happens, just set the variance to zero:\rvariance_conditional \u0026lt;- ifelse(variance_conditional \u0026lt; 0, 0, variance_conditional)\r# 95% intervals\rlower_bound \u0026lt;- mu_conditional - 1.96 * sqrt(variance_conditional)\rupper_bound \u0026lt;- mu_conditional + 1.96 * sqrt(variance_conditional)\rLet’s plot the result:\nf_conditional \u0026lt;- tibble(\rx = x_tilde_all,\rd = d_tilde_all,\ry = mu_conditional,\rlower = lower_bound,\rupper = upper_bound\r) ggplot(f_conditional, aes(x = x, y = y)) +\rgeom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2) +\rgeom_line(aes(lty = \u0026quot;Conditional mean\u0026quot;)) +\rgeom_line(aes(lty = \u0026quot;True value\u0026quot;), data = f) +\rgeom_point(aes(color = observed), data = filter(f, observed)) +\rscale_linetype_manual(values = c(2, 1)) +\rfacet_wrap(~d, ncol = 1, labeller = as_labeller(c(\u0026quot;0\u0026quot; = \u0026quot;Function\u0026quot;, \u0026quot;1\u0026quot; = \u0026quot;Derivative\u0026quot;)))\rThe inclusion of derivative information between \\(x=2.5\\) and \\(x=5.0\\) appears to have a big effect on the predictions. Even though there are no function values in this region, the derivatives are able to guide the predictions.\nIt would be nice to be able to compare these predictions to ones that don’t use any derivative information. To make this easier, let’s wrap up our code into a helper function which will handle the conditional multivariate normal calculations:\n# To make this more readable I\u0026#39;m omitting the \u0026quot;_all\u0026quot; suffixes on variable names\rcondition_joint_gp \u0026lt;- function(x_tilde, d_tilde, y, x, d, kernel, ...) {\rSigma \u0026lt;- joint_covariance_from_kernel(x, d, kernel, ...) +\rdiag(1e-4, length(x))\rOmega \u0026lt;- joint_covariance_from_kernel(x_tilde, d_tilde, kernel, ...)\rK \u0026lt;- outer(1:length(x), 1:length(x_tilde),\rfunction(i, j) kernel(x[i], x_tilde[j], d[i], d_tilde[j], ...))\rmu_conditional \u0026lt;- (t(K) %*% solve(Sigma) %*% y)[, 1]\rSigma_conditional \u0026lt;- Omega - t(K) %*% solve(Sigma) %*% K\rvar_conditional \u0026lt;- diag(Sigma_conditional)\rvar_conditional \u0026lt;- ifelse(var_conditional \u0026lt; 0, 0, var_conditional)\rtibble(\rx = x_tilde,\rd = d_tilde,\ry = mu_conditional,\rvar = var_conditional,\rlower = y - 1.96 * sqrt(var_conditional),\rupper = y + 1.96 * sqrt(var_conditional)\r)\r}\rAnd let’s also wrap up the plotting code into a function:\nplot_conditional \u0026lt;- function(f_conditional, f) {\rggplot(f_conditional, aes(x = x, y = y)) +\rgeom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2) +\rgeom_line(aes(lty = \u0026quot;Conditional mean\u0026quot;)) +\rgeom_line(aes(lty = \u0026quot;True value\u0026quot;), data = f) +\rgeom_point(aes(color = observed), data = filter(f, observed)) +\rscale_linetype_manual(values = c(2, 1)) +\rfacet_wrap(~d, ncol = 1, labeller = as_labeller(c(\u0026quot;0\u0026quot; = \u0026quot;Function\u0026quot;, \u0026quot;1\u0026quot; = \u0026quot;Derivative\u0026quot;)))\r}\rNow let’s condition on just the function observations, leaving out the derivative points, and plotting the results.\nnon_derivative_indices \u0026lt;- which(d_all == 0)\rf_conditional_no_derivatives \u0026lt;- condition_joint_gp(\rx_tilde_all, # Prediction points\rd_tilde_all, # Derivative indicator at prediction points\ry_all[non_derivative_indices], # Observed function values\rx_all[non_derivative_indices], # Position of observed values\rd_all[non_derivative_indices], # Derivative indicator of observed values\rk_all,\ralpha = alpha,\rl = l\r)\r# First, plot the predictions that don\u0026#39;t use derivative information\rplot_conditional(f_conditional_no_derivatives, mutate(f, observed = ifelse(d == 1, FALSE, observed))) +\rggtitle(\u0026quot;Predictions without derivative observations\u0026quot;)\r# Then plot again the predictions that use the derivatives\rplot_conditional(f_conditional, f) +\rggtitle(\u0026quot;Predictions using derivative observations\u0026quot;)\rAs we would expect, including more information by providing some derivative observations leads to much better predictions.\nUp until now, we have been fixing the hyperparmaters of the Gaussian Process to known values. However, in the real world it’s rare to know these parameters; we need to estimate them. In a future post I’ll show how to use Stan to estimate the hyperparameter values using full Bayesian inference.\nReferences\r\rStan Reference Manual, Fitting a Gaussian Process\r\r\r","date":1606780800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606852520,"objectID":"01fce16bfe91a457e99f1d1d6bcf9152","permalink":"/2020/12/01/conditioning-on-gaussian-process-derivative-observations/","publishdate":"2020-12-01T00:00:00Z","relpermalink":"/2020/12/01/conditioning-on-gaussian-process-derivative-observations/","section":"post","summary":"In the first post of this series, we looked at how to draw from the joint distribution of a Gaussian Process and its derivative. In this post, we will show how to condition a Gaussian Process on derivative observations. Overall this is pretty straightforward because the conditional distribution of the multivariate normal has a closed form, although applying it in this context requires some somewhat tedious notation and bookkeeping.","tags":[],"title":"Conditioning on Gaussian Process Derivative Observations","type":"post"},{"authors":[],"categories":[],"content":"\r\r\r\r\r\r\rA few years ago, The New York Times published an article about several Major League Baseball players who use the sitcom Friends to improve their English. Friends seems to be a very popular tool for learning English: there’s even an ESL program developed around it, and you can easily find advice on how to use Friends as a language learning tool.\nIn my own language learning I’ve relied heavily on frequency dictionaries, which order words by their popularity. Learning just the top 500-1,000 words in language goes surprisingly far in conversation.\nI wanted to know how many English words someone would need to recognize to be able to watch Friends and understand, say, 90% of all the words.\nHere the definition of “word” becomes important. For the purposes of this article, I’m going to consider most grammatical inflections of a root to be the same word. For example, “go”, “going”, and “went” will be considered one word, since they all relate back to the root word “to go”. Later on we’ll see how to operationalize this via lemmatization algorithms, which will allow us to map words back to their grammatical roots.\nFortunately, complete transcripts of Friends are available online. Below I go through the data collection and analysis. But first, here’s the final result:\nAround 750 words gets you to 90% coverage. Watching Friends where you don’t know every tenth word could be a frustrating experience, but hopefully context clues would help fill in gaps. The Appendix at the bottom of the post lists the top 750 words.\nThe numbers in this post should be considered as rough estimates because I did not filter out other words that we may not want to consider as words, like proper nouns (“Joey”, “Monica”, “New York”, etc.)\nData Collection\rThe script of every episode of Friends is available at https://fangj.github.io/friends/. I used rvest to download every file, and parsed the scripts into individual lines. I only wanted to have to do this once, so I saved the results to a CSV file so that subsequent analyses wouldn’t have to redownload all of the data.\nlibrary(tidyverse)\rlibrary(rvest)\rpages \u0026lt;- read_html(\u0026quot;https://fangj.github.io/friends/\u0026quot;) %\u0026gt;%\rhtml_nodes(\u0026quot;a\u0026quot;) %\u0026gt;%\rhtml_attr(\u0026quot;href\u0026quot;)\rpages \u0026lt;- str_c(\u0026quot;https://fangj.github.io/friends/\u0026quot;, pages)\rparse_episode \u0026lt;- function(url) {\rread_html(url) %\u0026gt;%\rhtml_nodes(\u0026quot;p:not([align=center])\u0026quot;) %\u0026gt;%\r.[-1] %\u0026gt;%\rhtml_text() %\u0026gt;%\renframe(name = NULL, value = \u0026quot;line\u0026quot;) %\u0026gt;%\rmutate(\rurl = url,\repisode = str_extract(url, \u0026quot;\\\\d{4}\u0026quot;), # extract episode number\rcharacter = str_extract(line, \u0026quot;^(\\\\w+):\u0026quot;), # extract character name\rcharacter = str_replace_all(character, \u0026quot;:$\u0026quot;, \u0026quot;\u0026quot;),\rline = str_replace_all(line, \u0026quot;^\\\\w+: \u0026quot;, \u0026quot;\u0026quot;),\r) %\u0026gt;%\rfilter(line != \u0026quot;\u0026quot;)\r}\rfriends_raw \u0026lt;- pages %\u0026gt;% map(parse_episode) %\u0026gt;%\rbind_rows()\rwrite_csv(friends_raw, \u0026quot;./friends.csv\u0026quot;)\rThis yields a raw dataset with the lines from every episode:\nhead(friends_raw, n = 10) %\u0026gt;%\rselect(episode, character, line) %\u0026gt;%\rgt::gt()\rhtml {\rfont-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\r}\r#nesysomsrl .gt_table {\rdisplay: table;\rborder-collapse: collapse;\rmargin-left: auto;\rmargin-right: auto;\rcolor: #333333;\rfont-size: 16px;\rbackground-color: #FFFFFF;\rwidth: auto;\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #A8A8A8;\rborder-right-style: none;\rborder-right-width: 2px;\rborder-right-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #A8A8A8;\rborder-left-style: none;\rborder-left-width: 2px;\rborder-left-color: #D3D3D3;\r}\r#nesysomsrl .gt_heading {\rbackground-color: #FFFFFF;\rtext-align: center;\rborder-bottom-color: #FFFFFF;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\r}\r#nesysomsrl .gt_title {\rcolor: #333333;\rfont-size: 125%;\rfont-weight: initial;\rpadding-top: 4px;\rpadding-bottom: 4px;\rborder-bottom-color: #FFFFFF;\rborder-bottom-width: 0;\r}\r#nesysomsrl .gt_subtitle {\rcolor: #333333;\rfont-size: 85%;\rfont-weight: initial;\rpadding-top: 0;\rpadding-bottom: 4px;\rborder-top-color: #FFFFFF;\rborder-top-width: 0;\r}\r#nesysomsrl .gt_bottom_border {\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\r}\r#nesysomsrl .gt_col_headings {\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\r}\r#nesysomsrl .gt_col_heading {\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: normal;\rtext-transform: inherit;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\rvertical-align: bottom;\rpadding-top: 5px;\rpadding-bottom: 6px;\rpadding-left: 5px;\rpadding-right: 5px;\roverflow-x: hidden;\r}\r#nesysomsrl .gt_column_spanner_outer {\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: normal;\rtext-transform: inherit;\rpadding-top: 0;\rpadding-bottom: 0;\rpadding-left: 4px;\rpadding-right: 4px;\r}\r#nesysomsrl .gt_column_spanner_outer:first-child {\rpadding-left: 0;\r}\r#nesysomsrl .gt_column_spanner_outer:last-child {\rpadding-right: 0;\r}\r#nesysomsrl .gt_column_spanner {\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rvertical-align: bottom;\rpadding-top: 5px;\rpadding-bottom: 6px;\roverflow-x: hidden;\rdisplay: inline-block;\rwidth: 100%;\r}\r#nesysomsrl .gt_group_heading {\rpadding: 8px;\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: initial;\rtext-transform: inherit;\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\rvertical-align: middle;\r}\r#nesysomsrl .gt_empty_group_heading {\rpadding: 0.5px;\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: initial;\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rvertical-align: middle;\r}\r#nesysomsrl .gt_striped {\rbackground-color: rgba(128, 128, 128, 0.05);\r}\r#nesysomsrl .gt_from_md  :first-child {\rmargin-top: 0;\r}\r#nesysomsrl .gt_from_md  :last-child {\rmargin-bottom: 0;\r}\r#nesysomsrl .gt_row {\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\rmargin: 10px;\rborder-top-style: solid;\rborder-top-width: 1px;\rborder-top-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\rvertical-align: middle;\roverflow-x: hidden;\r}\r#nesysomsrl .gt_stub {\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: initial;\rtext-transform: inherit;\rborder-right-style: solid;\rborder-right-width: 2px;\rborder-right-color: #D3D3D3;\rpadding-left: 12px;\r}\r#nesysomsrl .gt_summary_row {\rcolor: #333333;\rbackground-color: #FFFFFF;\rtext-transform: inherit;\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\r}\r#nesysomsrl .gt_first_summary_row {\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\r}\r#nesysomsrl .gt_grand_summary_row {\rcolor: #333333;\rbackground-color: #FFFFFF;\rtext-transform: inherit;\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\r}\r#nesysomsrl .gt_first_grand_summary_row {\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\rborder-top-style: double;\rborder-top-width: 6px;\rborder-top-color: #D3D3D3;\r}\r#nesysomsrl .gt_table_body {\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\r}\r#nesysomsrl .gt_footnotes {\rcolor: #333333;\rbackground-color: #FFFFFF;\rborder-bottom-style: none;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 2px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 2px;\rborder-right-color: #D3D3D3;\r}\r#nesysomsrl .gt_footnote {\rmargin: 0px;\rfont-size: 90%;\rpadding: 4px;\r}\r#nesysomsrl .gt_sourcenotes {\rcolor: #333333;\rbackground-color: #FFFFFF;\rborder-bottom-style: none;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 2px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 2px;\rborder-right-color: #D3D3D3;\r}\r#nesysomsrl .gt_sourcenote {\rfont-size: 90%;\rpadding: 4px;\r}\r#nesysomsrl .gt_left {\rtext-align: left;\r}\r#nesysomsrl .gt_center {\rtext-align: center;\r}\r#nesysomsrl .gt_right {\rtext-align: right;\rfont-variant-numeric: tabular-nums;\r}\r#nesysomsrl .gt_font_normal {\rfont-weight: normal;\r}\r#nesysomsrl .gt_font_bold {\rfont-weight: bold;\r}\r#nesysomsrl .gt_font_italic {\rfont-style: italic;\r}\r#nesysomsrl .gt_super {\rfont-size: 65%;\r}\r#nesysomsrl .gt_footnote_marks {\rfont-style: italic;\rfont-size: 65%;\r}\r\r\repisode\rcharacter\rline\r\r\r\r0101\rNA\r[Scene: Central Perk, Chandler, Joey, Phoebe, and Monica are there.]\r\r\r0101\rMonica\rThere's nothing to tell! He's just some guy\rI work with!\r\r\r0101\rJoey\rC'mon, you're going out with the guy! There's\rgotta be something wrong with him!\r\r\r0101\rChandler\rAll right Joey, be\rnice. So does he have a hump? A hump and a hairpiece?\r\r\r0101\rPhoebe\rWait, does he eat chalk?\r\r\r0101\rNA\r(They all stare, bemused.)\r\r\r0101\rPhoebe\rJust, 'cause, I don't want her to go through\rwhat I went through with Carl- oh!\r\r\r0101\rMonica\rOkay, everybody relax. This is not even a\rdate. It's just two people going out to dinner and- not having sex.\r\r\r0101\rChandler\rSounds like a date to me.\r\r\r0101\rNA\r[Time Lapse]\r\r\r\r\rCleaning\rFirst, there are some lines that give stage directions which we want to filter out of the dataset, since we are only interested in spoken words. There are also directions given sometimes within a character’s line, which we want to filter.\nfriends \u0026lt;- friends_raw %\u0026gt;%\rmutate(\rline = str_replace_all(line, \u0026quot;[\\u0091\\u0092]\u0026quot;, \u0026quot;\u0026#39;\u0026quot;),\rline = str_replace_all(line, \u0026quot;\\u0097\u0026quot;, \u0026quot;-\u0026quot;),\rline = str_replace_all(line, \u0026quot;\\\\([\\\\w\\n [[:punct:]]]+\\\\)\u0026quot;, \u0026quot;\u0026quot;),\rline = str_replace_all(line, \u0026quot;\\\\[[\\\\w\\n [[:punct:]]]+\\\\]\u0026quot;, \u0026quot;\u0026quot;),\rline = str_trim(line)\r) %\u0026gt;%\rfilter(line != \u0026quot;\u0026quot;)\rhead(friends, n = 10) %\u0026gt;%\rselect(episode, character, line) %\u0026gt;%\rgt::gt()\rhtml {\rfont-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\r}\r#vaqlnchokx .gt_table {\rdisplay: table;\rborder-collapse: collapse;\rmargin-left: auto;\rmargin-right: auto;\rcolor: #333333;\rfont-size: 16px;\rbackground-color: #FFFFFF;\rwidth: auto;\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #A8A8A8;\rborder-right-style: none;\rborder-right-width: 2px;\rborder-right-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #A8A8A8;\rborder-left-style: none;\rborder-left-width: 2px;\rborder-left-color: #D3D3D3;\r}\r#vaqlnchokx .gt_heading {\rbackground-color: #FFFFFF;\rtext-align: center;\rborder-bottom-color: #FFFFFF;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\r}\r#vaqlnchokx .gt_title {\rcolor: #333333;\rfont-size: 125%;\rfont-weight: initial;\rpadding-top: 4px;\rpadding-bottom: 4px;\rborder-bottom-color: #FFFFFF;\rborder-bottom-width: 0;\r}\r#vaqlnchokx .gt_subtitle {\rcolor: #333333;\rfont-size: 85%;\rfont-weight: initial;\rpadding-top: 0;\rpadding-bottom: 4px;\rborder-top-color: #FFFFFF;\rborder-top-width: 0;\r}\r#vaqlnchokx .gt_bottom_border {\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\r}\r#vaqlnchokx .gt_col_headings {\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\r}\r#vaqlnchokx .gt_col_heading {\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: normal;\rtext-transform: inherit;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\rvertical-align: bottom;\rpadding-top: 5px;\rpadding-bottom: 6px;\rpadding-left: 5px;\rpadding-right: 5px;\roverflow-x: hidden;\r}\r#vaqlnchokx .gt_column_spanner_outer {\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: normal;\rtext-transform: inherit;\rpadding-top: 0;\rpadding-bottom: 0;\rpadding-left: 4px;\rpadding-right: 4px;\r}\r#vaqlnchokx .gt_column_spanner_outer:first-child {\rpadding-left: 0;\r}\r#vaqlnchokx .gt_column_spanner_outer:last-child {\rpadding-right: 0;\r}\r#vaqlnchokx .gt_column_spanner {\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rvertical-align: bottom;\rpadding-top: 5px;\rpadding-bottom: 6px;\roverflow-x: hidden;\rdisplay: inline-block;\rwidth: 100%;\r}\r#vaqlnchokx .gt_group_heading {\rpadding: 8px;\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: initial;\rtext-transform: inherit;\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\rvertical-align: middle;\r}\r#vaqlnchokx .gt_empty_group_heading {\rpadding: 0.5px;\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: initial;\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rvertical-align: middle;\r}\r#vaqlnchokx .gt_striped {\rbackground-color: rgba(128, 128, 128, 0.05);\r}\r#vaqlnchokx .gt_from_md  :first-child {\rmargin-top: 0;\r}\r#vaqlnchokx .gt_from_md  :last-child {\rmargin-bottom: 0;\r}\r#vaqlnchokx .gt_row {\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\rmargin: 10px;\rborder-top-style: solid;\rborder-top-width: 1px;\rborder-top-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\rvertical-align: middle;\roverflow-x: hidden;\r}\r#vaqlnchokx .gt_stub {\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: initial;\rtext-transform: inherit;\rborder-right-style: solid;\rborder-right-width: 2px;\rborder-right-color: #D3D3D3;\rpadding-left: 12px;\r}\r#vaqlnchokx .gt_summary_row {\rcolor: #333333;\rbackground-color: #FFFFFF;\rtext-transform: inherit;\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\r}\r#vaqlnchokx .gt_first_summary_row {\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\r}\r#vaqlnchokx .gt_grand_summary_row {\rcolor: #333333;\rbackground-color: #FFFFFF;\rtext-transform: inherit;\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\r}\r#vaqlnchokx .gt_first_grand_summary_row {\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\rborder-top-style: double;\rborder-top-width: 6px;\rborder-top-color: #D3D3D3;\r}\r#vaqlnchokx .gt_table_body {\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\r}\r#vaqlnchokx .gt_footnotes {\rcolor: #333333;\rbackground-color: #FFFFFF;\rborder-bottom-style: none;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 2px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 2px;\rborder-right-color: #D3D3D3;\r}\r#vaqlnchokx .gt_footnote {\rmargin: 0px;\rfont-size: 90%;\rpadding: 4px;\r}\r#vaqlnchokx .gt_sourcenotes {\rcolor: #333333;\rbackground-color: #FFFFFF;\rborder-bottom-style: none;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 2px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 2px;\rborder-right-color: #D3D3D3;\r}\r#vaqlnchokx .gt_sourcenote {\rfont-size: 90%;\rpadding: 4px;\r}\r#vaqlnchokx .gt_left {\rtext-align: left;\r}\r#vaqlnchokx .gt_center {\rtext-align: center;\r}\r#vaqlnchokx .gt_right {\rtext-align: right;\rfont-variant-numeric: tabular-nums;\r}\r#vaqlnchokx .gt_font_normal {\rfont-weight: normal;\r}\r#vaqlnchokx .gt_font_bold {\rfont-weight: bold;\r}\r#vaqlnchokx .gt_font_italic {\rfont-style: italic;\r}\r#vaqlnchokx .gt_super {\rfont-size: 65%;\r}\r#vaqlnchokx .gt_footnote_marks {\rfont-style: italic;\rfont-size: 65%;\r}\r\r\repisode\rcharacter\rline\r\r\r\r0101\rMonica\rThere's nothing to tell! He's just some guy\rI work with!\r\r\r0101\rJoey\rC'mon, you're going out with the guy! There's\rgotta be something wrong with him!\r\r\r0101\rChandler\rAll right Joey, be\rnice. So does he have a hump? A hump and a hairpiece?\r\r\r0101\rPhoebe\rWait, does he eat chalk?\r\r\r0101\rPhoebe\rJust, 'cause, I don't want her to go through\rwhat I went through with Carl- oh!\r\r\r0101\rMonica\rOkay, everybody relax. This is not even a\rdate. It's just two people going out to dinner and- not having sex.\r\r\r0101\rChandler\rSounds like a date to me.\r\r\r0101\rChandler\rAlright, so I'm back in high school, I'm\rstanding in the middle of the cafeteria, and I realize I am totally naked.\r\r\r0101\rAll\rOh, yeah. Had that dream.\r\r\r0101\rChandler\rThen I look down, and I realize there's a\rphone... there.\r\r\r\rNow we transform our data from a set of lines from every script into a format that is more useful for analysis. Using tidytext, we generate a new table that has every word from the corpus and how many times it was used. We also calculate cumulative sums and proportions.\n# Counts how many times each word\r# appears, sorts by number of appearances,\r# and adds cumulative proportions\radd_cumulative_stats \u0026lt;- function(x) {\rx %\u0026gt;%\rcount(word, sort = TRUE) %\u0026gt;%\rmutate(cumsum = cumsum(n),\rcumprop = cumsum / sum(n),\rindex = 1:n())\r}\rlibrary(tidytext)\rwords \u0026lt;- friends %\u0026gt;%\runnest_tokens(word, line)\rword_counts \u0026lt;- add_cumulative_stats(words)\rword_counts %\u0026gt;%\rhead(n = 10) %\u0026gt;%\rgt::gt()\rhtml {\rfont-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\r}\r#nutqdpjnfa .gt_table {\rdisplay: table;\rborder-collapse: collapse;\rmargin-left: auto;\rmargin-right: auto;\rcolor: #333333;\rfont-size: 16px;\rbackground-color: #FFFFFF;\rwidth: auto;\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #A8A8A8;\rborder-right-style: none;\rborder-right-width: 2px;\rborder-right-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #A8A8A8;\rborder-left-style: none;\rborder-left-width: 2px;\rborder-left-color: #D3D3D3;\r}\r#nutqdpjnfa .gt_heading {\rbackground-color: #FFFFFF;\rtext-align: center;\rborder-bottom-color: #FFFFFF;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\r}\r#nutqdpjnfa .gt_title {\rcolor: #333333;\rfont-size: 125%;\rfont-weight: initial;\rpadding-top: 4px;\rpadding-bottom: 4px;\rborder-bottom-color: #FFFFFF;\rborder-bottom-width: 0;\r}\r#nutqdpjnfa .gt_subtitle {\rcolor: #333333;\rfont-size: 85%;\rfont-weight: initial;\rpadding-top: 0;\rpadding-bottom: 4px;\rborder-top-color: #FFFFFF;\rborder-top-width: 0;\r}\r#nutqdpjnfa .gt_bottom_border {\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\r}\r#nutqdpjnfa .gt_col_headings {\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\r}\r#nutqdpjnfa .gt_col_heading {\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: normal;\rtext-transform: inherit;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\rvertical-align: bottom;\rpadding-top: 5px;\rpadding-bottom: 6px;\rpadding-left: 5px;\rpadding-right: 5px;\roverflow-x: hidden;\r}\r#nutqdpjnfa .gt_column_spanner_outer {\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: normal;\rtext-transform: inherit;\rpadding-top: 0;\rpadding-bottom: 0;\rpadding-left: 4px;\rpadding-right: 4px;\r}\r#nutqdpjnfa .gt_column_spanner_outer:first-child {\rpadding-left: 0;\r}\r#nutqdpjnfa .gt_column_spanner_outer:last-child {\rpadding-right: 0;\r}\r#nutqdpjnfa .gt_column_spanner {\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rvertical-align: bottom;\rpadding-top: 5px;\rpadding-bottom: 6px;\roverflow-x: hidden;\rdisplay: inline-block;\rwidth: 100%;\r}\r#nutqdpjnfa .gt_group_heading {\rpadding: 8px;\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: initial;\rtext-transform: inherit;\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\rvertical-align: middle;\r}\r#nutqdpjnfa .gt_empty_group_heading {\rpadding: 0.5px;\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: initial;\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rvertical-align: middle;\r}\r#nutqdpjnfa .gt_striped {\rbackground-color: rgba(128, 128, 128, 0.05);\r}\r#nutqdpjnfa .gt_from_md  :first-child {\rmargin-top: 0;\r}\r#nutqdpjnfa .gt_from_md  :last-child {\rmargin-bottom: 0;\r}\r#nutqdpjnfa .gt_row {\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\rmargin: 10px;\rborder-top-style: solid;\rborder-top-width: 1px;\rborder-top-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\rvertical-align: middle;\roverflow-x: hidden;\r}\r#nutqdpjnfa .gt_stub {\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: initial;\rtext-transform: inherit;\rborder-right-style: solid;\rborder-right-width: 2px;\rborder-right-color: #D3D3D3;\rpadding-left: 12px;\r}\r#nutqdpjnfa .gt_summary_row {\rcolor: #333333;\rbackground-color: #FFFFFF;\rtext-transform: inherit;\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\r}\r#nutqdpjnfa .gt_first_summary_row {\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\r}\r#nutqdpjnfa .gt_grand_summary_row {\rcolor: #333333;\rbackground-color: #FFFFFF;\rtext-transform: inherit;\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\r}\r#nutqdpjnfa .gt_first_grand_summary_row {\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\rborder-top-style: double;\rborder-top-width: 6px;\rborder-top-color: #D3D3D3;\r}\r#nutqdpjnfa .gt_table_body {\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\r}\r#nutqdpjnfa .gt_footnotes {\rcolor: #333333;\rbackground-color: #FFFFFF;\rborder-bottom-style: none;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 2px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 2px;\rborder-right-color: #D3D3D3;\r}\r#nutqdpjnfa .gt_footnote {\rmargin: 0px;\rfont-size: 90%;\rpadding: 4px;\r}\r#nutqdpjnfa .gt_sourcenotes {\rcolor: #333333;\rbackground-color: #FFFFFF;\rborder-bottom-style: none;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 2px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 2px;\rborder-right-color: #D3D3D3;\r}\r#nutqdpjnfa .gt_sourcenote {\rfont-size: 90%;\rpadding: 4px;\r}\r#nutqdpjnfa .gt_left {\rtext-align: left;\r}\r#nutqdpjnfa .gt_center {\rtext-align: center;\r}\r#nutqdpjnfa .gt_right {\rtext-align: right;\rfont-variant-numeric: tabular-nums;\r}\r#nutqdpjnfa .gt_font_normal {\rfont-weight: normal;\r}\r#nutqdpjnfa .gt_font_bold {\rfont-weight: bold;\r}\r#nutqdpjnfa .gt_font_italic {\rfont-style: italic;\r}\r#nutqdpjnfa .gt_super {\rfont-size: 65%;\r}\r#nutqdpjnfa .gt_footnote_marks {\rfont-style: italic;\rfont-size: 65%;\r}\r\r\rword\rn\rcumsum\rcumprop\rindex\r\r\r\ri\r23298\r23298\r0.04076248\r1\r\r\ryou\r22017\r45315\r0.07928371\r2\r\r\rthe\r13292\r58607\r0.10253956\r3\r\r\rto\r11554\r70161\r0.12275459\r4\r\r\ra\r10749\r80910\r0.14156118\r5\r\r\rand\r9405\r90315\r0.15801629\r6\r\r\rit\r7631\r97946\r0.17136758\r7\r\r\rthat\r7564\r105510\r0.18460166\r8\r\r\roh\r7226\r112736\r0.19724436\r9\r\r\rwhat\r6355\r119091\r0.20836315\r10\r\r\r\rFrom the cumulative proportions in the table we can see that knowing the top 10 words covers around 20% of all the words in the dataset.\nThis is a good start, but it’s not quite what we want. Right now the data includes grammatical variations of the same word as different words. For example, “age” and “ages” really represent the same word, but the graph above includes them as two separate words. As such, the data we have so far overestimates the number of unique words that appear in Friends.\nLemmatizing algorithms try to map every grammatical variation of a word to a “stem”. For example, the words “go”, “going”, and “went” should map to the stem “go”. Stemming algorithms are related, but work by removing suffixes from the ends of words. A stemming algorithm wouldn’t know to map “went” to “go”; for that, we need lemmatization, which uses dictionaries to reverse the inflected forms of words to its grammatical root. The package textstem provides a lemmatize_words function in R.\nlibrary(textstem)\rword_lemmas \u0026lt;- words %\u0026gt;%\rmutate(word = lemmatize_words(word))\rword_lemma_counts \u0026lt;- add_cumulative_stats(word_lemmas)\rThere are 11,852 words after lemmatization, compared to 15,134 before.\nword_lemma_counts %\u0026gt;%\rhead(n = 10) %\u0026gt;%\rgt::gt()\rhtml {\rfont-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\r}\r#gxslgjtkij .gt_table {\rdisplay: table;\rborder-collapse: collapse;\rmargin-left: auto;\rmargin-right: auto;\rcolor: #333333;\rfont-size: 16px;\rbackground-color: #FFFFFF;\rwidth: auto;\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #A8A8A8;\rborder-right-style: none;\rborder-right-width: 2px;\rborder-right-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #A8A8A8;\rborder-left-style: none;\rborder-left-width: 2px;\rborder-left-color: #D3D3D3;\r}\r#gxslgjtkij .gt_heading {\rbackground-color: #FFFFFF;\rtext-align: center;\rborder-bottom-color: #FFFFFF;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\r}\r#gxslgjtkij .gt_title {\rcolor: #333333;\rfont-size: 125%;\rfont-weight: initial;\rpadding-top: 4px;\rpadding-bottom: 4px;\rborder-bottom-color: #FFFFFF;\rborder-bottom-width: 0;\r}\r#gxslgjtkij .gt_subtitle {\rcolor: #333333;\rfont-size: 85%;\rfont-weight: initial;\rpadding-top: 0;\rpadding-bottom: 4px;\rborder-top-color: #FFFFFF;\rborder-top-width: 0;\r}\r#gxslgjtkij .gt_bottom_border {\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\r}\r#gxslgjtkij .gt_col_headings {\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\r}\r#gxslgjtkij .gt_col_heading {\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: normal;\rtext-transform: inherit;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\rvertical-align: bottom;\rpadding-top: 5px;\rpadding-bottom: 6px;\rpadding-left: 5px;\rpadding-right: 5px;\roverflow-x: hidden;\r}\r#gxslgjtkij .gt_column_spanner_outer {\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: normal;\rtext-transform: inherit;\rpadding-top: 0;\rpadding-bottom: 0;\rpadding-left: 4px;\rpadding-right: 4px;\r}\r#gxslgjtkij .gt_column_spanner_outer:first-child {\rpadding-left: 0;\r}\r#gxslgjtkij .gt_column_spanner_outer:last-child {\rpadding-right: 0;\r}\r#gxslgjtkij .gt_column_spanner {\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rvertical-align: bottom;\rpadding-top: 5px;\rpadding-bottom: 6px;\roverflow-x: hidden;\rdisplay: inline-block;\rwidth: 100%;\r}\r#gxslgjtkij .gt_group_heading {\rpadding: 8px;\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: initial;\rtext-transform: inherit;\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\rvertical-align: middle;\r}\r#gxslgjtkij .gt_empty_group_heading {\rpadding: 0.5px;\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: initial;\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rvertical-align: middle;\r}\r#gxslgjtkij .gt_striped {\rbackground-color: rgba(128, 128, 128, 0.05);\r}\r#gxslgjtkij .gt_from_md  :first-child {\rmargin-top: 0;\r}\r#gxslgjtkij .gt_from_md  :last-child {\rmargin-bottom: 0;\r}\r#gxslgjtkij .gt_row {\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\rmargin: 10px;\rborder-top-style: solid;\rborder-top-width: 1px;\rborder-top-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\rvertical-align: middle;\roverflow-x: hidden;\r}\r#gxslgjtkij .gt_stub {\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: initial;\rtext-transform: inherit;\rborder-right-style: solid;\rborder-right-width: 2px;\rborder-right-color: #D3D3D3;\rpadding-left: 12px;\r}\r#gxslgjtkij .gt_summary_row {\rcolor: #333333;\rbackground-color: #FFFFFF;\rtext-transform: inherit;\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\r}\r#gxslgjtkij .gt_first_summary_row {\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\r}\r#gxslgjtkij .gt_grand_summary_row {\rcolor: #333333;\rbackground-color: #FFFFFF;\rtext-transform: inherit;\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\r}\r#gxslgjtkij .gt_first_grand_summary_row {\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\rborder-top-style: double;\rborder-top-width: 6px;\rborder-top-color: #D3D3D3;\r}\r#gxslgjtkij .gt_table_body {\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\r}\r#gxslgjtkij .gt_footnotes {\rcolor: #333333;\rbackground-color: #FFFFFF;\rborder-bottom-style: none;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 2px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 2px;\rborder-right-color: #D3D3D3;\r}\r#gxslgjtkij .gt_footnote {\rmargin: 0px;\rfont-size: 90%;\rpadding: 4px;\r}\r#gxslgjtkij .gt_sourcenotes {\rcolor: #333333;\rbackground-color: #FFFFFF;\rborder-bottom-style: none;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 2px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 2px;\rborder-right-color: #D3D3D3;\r}\r#gxslgjtkij .gt_sourcenote {\rfont-size: 90%;\rpadding: 4px;\r}\r#gxslgjtkij .gt_left {\rtext-align: left;\r}\r#gxslgjtkij .gt_center {\rtext-align: center;\r}\r#gxslgjtkij .gt_right {\rtext-align: right;\rfont-variant-numeric: tabular-nums;\r}\r#gxslgjtkij .gt_font_normal {\rfont-weight: normal;\r}\r#gxslgjtkij .gt_font_bold {\rfont-weight: bold;\r}\r#gxslgjtkij .gt_font_italic {\rfont-style: italic;\r}\r#gxslgjtkij .gt_super {\rfont-size: 65%;\r}\r#gxslgjtkij .gt_footnote_marks {\rfont-style: italic;\rfont-size: 65%;\r}\r\r\rword\rn\rcumsum\rcumprop\rindex\r\r\r\ri\r23298\r23298\r0.04076248\r1\r\r\ryou\r22364\r45662\r0.07989082\r2\r\r\rbe\r19053\r64715\r0.11322620\r3\r\r\rthe\r13292\r78007\r0.13648205\r4\r\r\ra\r11566\r89573\r0.15671808\r5\r\r\rto\r11554\r101127\r0.17693310\r6\r\r\rand\r9405\r110532\r0.19338821\r7\r\r\rthat\r7993\r118525\r0.20737287\r8\r\r\rit\r7631\r126156\r0.22072416\r9\r\r\roh\r7226\r133382\r0.23336687\r10\r\r\r\rThe top 10 words are the same, but the cumulative proportions have changed since there are fewer words overall. Now knowing the top 10 words covers about 23% of all words.\nA helper function helps compute how many words we need to know to cover a certain percentage of the total:\n# How many words do you need to know prop% of all words?\rwords_for_prop \u0026lt;- function(x, prop) {\rx %\u0026gt;% filter(cumprop \u0026gt; prop) %\u0026gt;% pull(index) %\u0026gt;% first()\r}\rwords_for_prop_result \u0026lt;- tibble(\rprop = c(0.98, 0.95, 0.9, 0.5),\rwords = map_int(prop, function(x) words_for_prop(word_lemma_counts, x))\r)\rwords_for_prop_result %\u0026gt;%\rgt() %\u0026gt;%\rfmt_percent(columns = vars(prop), decimals = 0) %\u0026gt;%\rfmt_number(columns = vars(words), use_seps = TRUE, decimals = 0) %\u0026gt;%\rcols_label(prop = \u0026quot;Cumulative percentage\u0026quot;, words = \u0026quot;Number of words\u0026quot;)\rhtml {\rfont-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\r}\r#ymdcyjcmgh .gt_table {\rdisplay: table;\rborder-collapse: collapse;\rmargin-left: auto;\rmargin-right: auto;\rcolor: #333333;\rfont-size: 16px;\rbackground-color: #FFFFFF;\rwidth: auto;\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #A8A8A8;\rborder-right-style: none;\rborder-right-width: 2px;\rborder-right-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #A8A8A8;\rborder-left-style: none;\rborder-left-width: 2px;\rborder-left-color: #D3D3D3;\r}\r#ymdcyjcmgh .gt_heading {\rbackground-color: #FFFFFF;\rtext-align: center;\rborder-bottom-color: #FFFFFF;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\r}\r#ymdcyjcmgh .gt_title {\rcolor: #333333;\rfont-size: 125%;\rfont-weight: initial;\rpadding-top: 4px;\rpadding-bottom: 4px;\rborder-bottom-color: #FFFFFF;\rborder-bottom-width: 0;\r}\r#ymdcyjcmgh .gt_subtitle {\rcolor: #333333;\rfont-size: 85%;\rfont-weight: initial;\rpadding-top: 0;\rpadding-bottom: 4px;\rborder-top-color: #FFFFFF;\rborder-top-width: 0;\r}\r#ymdcyjcmgh .gt_bottom_border {\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\r}\r#ymdcyjcmgh .gt_col_headings {\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\r}\r#ymdcyjcmgh .gt_col_heading {\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: normal;\rtext-transform: inherit;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\rvertical-align: bottom;\rpadding-top: 5px;\rpadding-bottom: 6px;\rpadding-left: 5px;\rpadding-right: 5px;\roverflow-x: hidden;\r}\r#ymdcyjcmgh .gt_column_spanner_outer {\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: normal;\rtext-transform: inherit;\rpadding-top: 0;\rpadding-bottom: 0;\rpadding-left: 4px;\rpadding-right: 4px;\r}\r#ymdcyjcmgh .gt_column_spanner_outer:first-child {\rpadding-left: 0;\r}\r#ymdcyjcmgh .gt_column_spanner_outer:last-child {\rpadding-right: 0;\r}\r#ymdcyjcmgh .gt_column_spanner {\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rvertical-align: bottom;\rpadding-top: 5px;\rpadding-bottom: 6px;\roverflow-x: hidden;\rdisplay: inline-block;\rwidth: 100%;\r}\r#ymdcyjcmgh .gt_group_heading {\rpadding: 8px;\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: initial;\rtext-transform: inherit;\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\rvertical-align: middle;\r}\r#ymdcyjcmgh .gt_empty_group_heading {\rpadding: 0.5px;\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: initial;\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rvertical-align: middle;\r}\r#ymdcyjcmgh .gt_striped {\rbackground-color: rgba(128, 128, 128, 0.05);\r}\r#ymdcyjcmgh .gt_from_md  :first-child {\rmargin-top: 0;\r}\r#ymdcyjcmgh .gt_from_md  :last-child {\rmargin-bottom: 0;\r}\r#ymdcyjcmgh .gt_row {\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\rmargin: 10px;\rborder-top-style: solid;\rborder-top-width: 1px;\rborder-top-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\rvertical-align: middle;\roverflow-x: hidden;\r}\r#ymdcyjcmgh .gt_stub {\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: initial;\rtext-transform: inherit;\rborder-right-style: solid;\rborder-right-width: 2px;\rborder-right-color: #D3D3D3;\rpadding-left: 12px;\r}\r#ymdcyjcmgh .gt_summary_row {\rcolor: #333333;\rbackground-color: #FFFFFF;\rtext-transform: inherit;\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\r}\r#ymdcyjcmgh .gt_first_summary_row {\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\r}\r#ymdcyjcmgh .gt_grand_summary_row {\rcolor: #333333;\rbackground-color: #FFFFFF;\rtext-transform: inherit;\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\r}\r#ymdcyjcmgh .gt_first_grand_summary_row {\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\rborder-top-style: double;\rborder-top-width: 6px;\rborder-top-color: #D3D3D3;\r}\r#ymdcyjcmgh .gt_table_body {\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\r}\r#ymdcyjcmgh .gt_footnotes {\rcolor: #333333;\rbackground-color: #FFFFFF;\rborder-bottom-style: none;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 2px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 2px;\rborder-right-color: #D3D3D3;\r}\r#ymdcyjcmgh .gt_footnote {\rmargin: 0px;\rfont-size: 90%;\rpadding: 4px;\r}\r#ymdcyjcmgh .gt_sourcenotes {\rcolor: #333333;\rbackground-color: #FFFFFF;\rborder-bottom-style: none;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 2px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 2px;\rborder-right-color: #D3D3D3;\r}\r#ymdcyjcmgh .gt_sourcenote {\rfont-size: 90%;\rpadding: 4px;\r}\r#ymdcyjcmgh .gt_left {\rtext-align: left;\r}\r#ymdcyjcmgh .gt_center {\rtext-align: center;\r}\r#ymdcyjcmgh .gt_right {\rtext-align: right;\rfont-variant-numeric: tabular-nums;\r}\r#ymdcyjcmgh .gt_font_normal {\rfont-weight: normal;\r}\r#ymdcyjcmgh .gt_font_bold {\rfont-weight: bold;\r}\r#ymdcyjcmgh .gt_font_italic {\rfont-style: italic;\r}\r#ymdcyjcmgh .gt_super {\rfont-size: 65%;\r}\r#ymdcyjcmgh .gt_footnote_marks {\rfont-style: italic;\rfont-size: 65%;\r}\r\r\rCumulative percentage\rNumber of words\r\r\r\r98\u0026percnt;\r4,148\r\r\r95\u0026percnt;\r1,758\r\r\r90\u0026percnt;\r743\r\r\r50\u0026percnt;\r46\r\r\r\rUsing the helper function, we can make a graph of the cumulative distribution of words with cutoffs at 50%, 90%, and 95%:\nword_lemma_counts %\u0026gt;%\rggplot(aes(x = index, y = cumprop * 100)) +\rgeom_line(color = \u0026quot;#2980b9\u0026quot;, size = 1) +\rgeom_segment(aes(x = words, xend = words, y = 0, yend = prop * 100), lty = 2, data = words_for_prop_result, color = \u0026quot;#2c3e50\u0026quot;) +\rgeom_segment(aes(x = 0, xend = words, y = prop * 100, yend = prop * 100), lty = 2, data = words_for_prop_result, color = \u0026quot;#2c3e50\u0026quot;) +\rgeom_point(aes(x = words, y = prop * 100), data = words_for_prop_result, color = \u0026quot;white\u0026quot;, size = 3) +\rgeom_point(aes(x = words, y = prop * 100), data = words_for_prop_result, color = \u0026quot;#2c3e50\u0026quot;, size = 2) +\rlabs(x = \u0026quot;Words ordered by frequency\u0026quot;, y = \u0026quot;Cumulative percentage\u0026quot;) +\rcowplot::theme_cowplot() +\rscale_y_continuous(labels = function(x) paste0(x, \u0026quot;%\u0026quot;), breaks = c(0, 25, 50, 75, words_for_prop_result$prop * 100), expand = c(0.01, 0.01)) +\rscale_x_continuous(breaks = c(words_for_prop_result$words, 7500, 10000), expand = c(0.01, 0.01)) +\rmy_theme +\rggtitle(\u0026quot;You need to know around 750 words to know 90%\\nof all the words used in Friends\u0026quot;) +\rlabs(caption = \u0026quot;Analysis: Herb Susmann, transcripts: https://fangj.github.io/friends/\u0026quot;)\r\rFigure 1: You need to know 750 words to know 90% of all the words used in Friends\r\r\rAppendix: top 750 words\rword_lemma_counts %\u0026gt;%\rmutate(`cumulative proportion` = scales::percent_format(0.01)(cumprop)) %\u0026gt;%\rselect(index, n, word, `cumulative proportion`) %\u0026gt;%\rhead(n = 750) %\u0026gt;%\rreactable()\r\r{\"x\":{\"tag\":{\"name\":\"Reactable\",\"attribs\":{\"data\":{\"index\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750],\"n\":[23298,22364,19053,13292,11566,11554,9405,7993,7631,7226,7213,6355,5950,5900,5702,5655,5633,5452,5443,5019,5010,4869,4793,4703,4672,4526,4415,4124,4065,4040,3870,3813,3711,3524,3431,3397,3320,3267,3181,3164,3079,2936,2831,2590,2560,2442,2396,2390,2317,2307,2226,2220,2213,2166,2159,2145,2136,2127,2114,2071,1995,1982,1969,1950,1881,1845,1804,1801,1784,1713,1665,1595,1528,1495,1461,1448,1426,1418,1407,1325,1262,1256,1256,1248,1235,1193,1191,1163,1161,1139,1138,1119,1108,1064,1048,1038,1037,1036,1035,1006,999,997,986,950,950,946,890,875,853,850,848,836,828,807,796,780,780,772,757,751,751,742,739,716,710,709,704,701,690,678,671,667,665,654,645,641,634,633,631,627,626,619,618,610,605,604,604,601,600,587,584,580,575,569,565,563,556,556,545,539,529,525,521,509,507,502,497,496,492,490,488,481,478,476,475,474,462,461,458,455,449,447,445,445,445,444,443,439,427,422,422,418,416,416,414,413,413,411,407,407,406,401,400,398,396,396,389,385,384,384,382,382,376,369,367,366,363,361,355,350,346,346,344,343,342,341,339,332,329,316,312,310,310,309,306,299,298,298,296,296,295,292,292,290,290,286,283,283,283,282,281,278,277,276,276,276,276,273,269,269,267,266,264,262,260,259,259,256,255,253,253,251,251,249,249,246,245,244,244,241,240,239,237,237,236,235,235,232,232,231,229,228,228,227,223,223,221,221,217,216,214,213,213,212,210,210,210,209,209,209,207,206,206,206,205,205,204,199,199,198,198,198,198,197,196,194,191,191,189,188,188,186,186,185,185,185,183,181,181,181,180,179,178,178,176,176,176,174,173,173,173,172,171,171,170,170,169,169,169,168,168,167,167,166,165,165,164,164,163,163,162,162,161,161,160,160,160,160,159,158,158,157,157,157,156,155,155,155,151,151,151,150,150,149,149,148,148,148,147,147,147,146,146,146,145,144,144,144,143,142,140,140,140,140,140,139,139,139,138,138,137,137,136,136,136,135,135,135,135,134,133,133,132,132,131,131,131,130,130,130,130,129,129,128,128,128,128,128,127,127,126,125,124,124,123,123,123,123,122,122,122,122,120,119,119,118,117,117,117,117,116,116,116,116,115,115,114,114,114,114,114,113,113,113,113,112,112,112,111,111,110,108,108,108,108,108,107,107,107,107,107,107,107,107,106,106,106,105,105,105,105,104,104,103,103,102,102,101,101,100,100,99,99,99,99,99,98,98,97,97,96,96,95,95,94,94,94,94,94,94,94,93,93,93,92,92,92,91,91,90,90,90,89,89,89,89,88,88,88,88,87,87,87,87,87,87,87,86,86,86,86,86,85,85,85,84,84,83,83,83,83,83,83,83,82,82,82,82,82,82,80,80,80,80,80,80,80,80,79,79,79,79,79,78,78,78,78,78,78,78,78,78,78,78,78,77,77,77,77,76,76,76,76,76,76,76,75,75,75,75,75,75,74,74,74,74,74,73,73,73,72,72,72,72,72,72,71,71,71,70,69,69,69,69,69,68,68,68,67,67,67,67,67,67,67,66,66,66,66,66,66,65,65,65,65,65,65,65,65,65,64,64,63,63,63,63,63,63,63,63,62,62,62,62,62,62,61,61,61,61,61,61,61,61,61,61,61,61,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,59,59,59,59,59,59,59,59,59,58,58,58,58,58,58,58,58,57,57,57,57,57,57,57,57,57,56],\"word\":[\"i\",\"you\",\"be\",\"the\",\"a\",\"to\",\"and\",\"that\",\"it\",\"oh\",\"do\",\"what\",\"good\",\"no\",\"have\",\"this\",\"yes\",\"okay\",\"i'm\",\"just\",\"me\",\"so\",\"my\",\"get\",\"of\",\"know\",\"in\",\"go\",\"it's\",\"we\",\"don't\",\"not\",\"hey\",\"on\",\"with\",\"for\",\"all\",\"but\",\"right\",\"can\",\"think\",\"like\",\"your\",\"you're\",\"gonna\",\"about\",\"that's\",\"look\",\"really\",\"out\",\"uh\",\"guy\",\"say\",\"her\",\"y'know\",\"want\",\"here\",\"see\",\"come\",\"if\",\"up\",\"mean\",\"he\",\"she\",\"how\",\"at\",\"ross\",\"one\",\"there\",\"now\",\"tell\",\"why\",\"hello\",\"joey\",\"god\",\"much\",\"him\",\"can't\",\"make\",\"would\",\"sorry\",\"they\",\"thing\",\"when\",\"great\",\"chandler\",\"time\",\"then\",\"take\",\"monica\",\"rachel\",\"little\",\"thank\",\"didn't\",\"talk\",\"love\",\"i'll\",\"because\",\"wait\",\"back\",\"we're\",\"phoebe\",\"who\",\"he's\",\"too\",\"some\",\"give\",\"something\",\"from\",\"she's\",\"or\",\"umm\",\"maybe\",\"them\",\"as\",\"call\",\"should\",\"us\",\"work\",\"way\",\"what's\",\"will\",\"over\",\"i've\",\"need\",\"never\",\"sure\",\"huh\",\"feel\",\"his\",\"man\",\"big\",\"wanna\",\"wow\",\"baby\",\"there's\",\"happen\",\"our\",\"two\",\"believe\",\"down\",\"please\",\"friend\",\"let\",\"where\",\"ah\",\"still\",\"leave\",\"even\",\"people\",\"first\",\"mr\",\"fine\",\"actually\",\"day\",\"again\",\"off\",\"try\",\"let's\",\"very\",\"listen\",\"by\",\"night\",\"any\",\"marry\",\"nice\",\"ask\",\"anything\",\"woman\",\"bad\",\"stuff\",\"other\",\"find\",\"year\",\"um\",\"hear\",\"start\",\"play\",\"they're\",\"last\",\"only\",\"gotta\",\"ever\",\"girl\",\"stop\",\"doesn't\",\"new\",\"minute\",\"ooh\",\"guess\",\"use\",\"before\",\"geller\",\"put\",\"help\",\"may\",\"name\",\"date\",\"nothing\",\"together\",\"honey\",\"break\",\"lot\",\"move\",\"kid\",\"ohh\",\"meet\",\"whoa\",\"long\",\"place\",\"cause\",\"rach\",\"keep\",\"mrs\",\"remember\",\"room\",\"pheebs\",\"into\",\"dr\",\"life\",\"pretty\",\"someone\",\"fun\",\"kinda\",\"i'd\",\"around\",\"live\",\"happy\",\"bye\",\"late\",\"2\",\"always\",\"kind\",\"alright\",\"isn't\",\"three\",\"funny\",\"hand\",\"hard\",\"pick\",\"tonight\",\"than\",\"wrong\",\"care\",\"kiss\",\"wed\",\"after\",\"eat\",\"stay\",\"else\",\"which\",\"everything\",\"ready\",\"job\",\"watch\",\"we'll\",\"you've\",\"old\",\"home\",\"miss\",\"sleep\",\"show\",\"weird\",\"sex\",\"forget\",\"away\",\"party\",\"mom\",\"another\",\"bring\",\"totally\",\"today\",\"wear\",\"head\",\"wouldn't\",\"money\",\"turn\",\"apartment\",\"check\",\"next\",\"sound\",\"idea\",\"stupid\",\"whole\",\"wasn't\",\"who's\",\"won't\",\"crazy\",\"open\",\"game\",\"probably\",\"lose\",\"sit\",\"cool\",\"course\",\"hour\",\"part\",\"suppose\",\"hate\",\"green\",\"phone\",\"dad\",\"drink\",\"problem\",\"their\",\"through\",\"tomorrow\",\"anyway\",\"deal\",\"run\",\"haven't\",\"already\",\"end\",\"win\",\"walk\",\"yet\",\"every\",\"each\",\"enough\",\"dinner\",\"matter\",\"such\",\"while\",\"question\",\"since\",\"buy\",\"excuse\",\"hold\",\"throw\",\"couple\",\"everybody\",\"couldn't\",\"worry\",\"change\",\"close\",\"understand\",\"hang\",\"boy\",\"same\",\"you'll\",\"face\",\"week\",\"doctor\",\"five\",\"both\",\"everyone\",\"hot\",\"sweet\",\"emma\",\"real\",\"seem\",\"pay\",\"anymore\",\"hell\",\"cute\",\"sister\",\"own\",\"read\",\"you'd\",\"bing\",\"ha\",\"house\",\"myself\",\"plan\",\"high\",\"somebody\",\"movie\",\"we've\",\"also\",\"person\",\"die\",\"ring\",\"coffee\",\"morning\",\"beautiful\",\"bite\",\"hope\",\"mine\",\"picture\",\"food\",\"write\",\"ben\",\"hmm\",\"must\",\"joey's\",\"door\",\"dress\",\"eye\",\"lie\",\"mike\",\"yourself\",\"here's\",\"kill\",\"cry\",\"parent\",\"amaze\",\"figure\",\"spend\",\"different\",\"fire\",\"mind\",\"easy\",\"four\",\"joke\",\"true\",\"book\",\"scene\",\"story\",\"aren't\",\"ticket\",\"father\",\"lady\",\"once\",\"serious\",\"table\",\"cut\",\"though\",\"uhm\",\"dude\",\"ow\",\"until\",\"word\",\"dance\",\"naked\",\"wife\",\"ago\",\"birthday\",\"chance\",\"wish\",\"point\",\"emily\",\"school\",\"hair\",\"numb\",\"alone\",\"either\",\"hurt\",\"anyone\",\"half\",\"reason\",\"stand\",\"fall\",\"office\",\"dog\",\"line\",\"monica's\",\"uhh\",\"without\",\"joe\",\"shoot\",\"world\",\"relationship\",\"bet\",\"mon\",\"director\",\"important\",\"month\",\"six\",\"fact\",\"hit\",\"promise\",\"smell\",\"freak\",\"car\",\"perfect\",\"present\",\"interest\",\"mad\",\"pant\",\"stick\",\"ahh\",\"ass\",\"bed\",\"decide\",\"pregnant\",\"send\",\"boyfriend\",\"card\",\"dollar\",\"tribbiani\",\"wh\",\"absolutely\",\"box\",\"fight\",\"smoke\",\"anybody\",\"child\",\"glad\",\"bathroom\",\"seriously\",\"answer\",\"chandler's\",\"excite\",\"less\",\"set\",\"whatever\",\"3\",\"em\",\"exactly\",\"family\",\"janice\",\"rachel's\",\"soon\",\"wonder\",\"cat\",\"divorce\",\"ugh\",\"mother\",\"surprise\",\"sweetie\",\"touch\",\"frank\",\"shouldn't\",\"finish\",\"ho\",\"how's\",\"it'll\",\"almost\",\"christmas\",\"key\",\"luck\",\"fat\",\"free\",\"laugh\",\"light\",\"many\",\"clean\",\"thanksgiving\",\"catch\",\"suck\",\"audition\",\"young\",\"class\",\"girlfriend\",\"between\",\"cannot\",\"invite\",\"its\",\"rest\",\"terrible\",\"tv\",\"carol\",\"marriage\",\"ten\",\"bag\",\"la\",\"under\",\"push\",\"scare\",\"finally\",\"huge\",\"realize\",\"chair\",\"grow\",\"sick\",\"steal\",\"fast\",\"moment\",\"save\",\"weren't\",\"cup\",\"dead\",\"lunch\",\"message\",\"order\",\"seat\",\"yours\",\"definitely\",\"eh\",\"kick\",\"relax\",\"welcome\",\"gift\",\"news\",\"pass\",\"speak\",\"would've\",\"learn\",\"restaurant\",\"richard\",\"small\",\"song\",\"teach\",\"unless\",\"able\",\"build\",\"credit\",\"shoe\",\"sing\",\"sometimes\",\"1\",\"actor\",\"cook\",\"drop\",\"london\",\"massage\",\"roommate\",\"special\",\"pizza\",\"sell\",\"seven\",\"side\",\"yep\",\"act\",\"boss\",\"c'mon\",\"few\",\"front\",\"heart\",\"ice\",\"quit\",\"sr\",\"star\",\"street\",\"tape\",\"city\",\"clothe\",\"foot\",\"mess\",\"aunt\",\"nobody\",\"propose\",\"ross's\",\"shut\",\"son\",\"where's\",\"brother\",\"dream\",\"fault\",\"grab\",\"smart\",\"water\",\"enjoy\",\"gay\",\"pull\",\"rule\",\"secret\",\"couch\",\"drive\",\"store\",\"along\",\"mark\",\"pack\",\"shop\",\"upset\",\"we'd\",\"o\",\"ohhh\",\"outside\",\"college\",\"ball\",\"early\",\"fair\",\"hundred\",\"shower\",\"list\",\"lucky\",\"teacher\",\"7\",\"although\",\"cookie\",\"damn\",\"sad\",\"sign\",\"top\",\"buck\",\"machine\",\"paper\",\"plate\",\"ruin\",\"sir\",\"afraid\",\"duck\",\"eight\",\"except\",\"far\",\"plane\",\"sense\",\"waltham\",\"weekend\",\"asleep\",\"mistake\",\"cake\",\"chicken\",\"fake\",\"feeling\",\"leg\",\"shirt\",\"twin\",\"yay\",\"blow\",\"cast\",\"david\",\"pee\",\"roll\",\"sandwich\",\"apparently\",\"bitch\",\"borrow\",\"candy\",\"charlie\",\"control\",\"marcel\",\"middle\",\"monkey\",\"porn\",\"strong\",\"treeger\",\"5\",\"aww\",\"bedroom\",\"behind\",\"bird\",\"buddy\",\"doin\",\"hug\",\"husband\",\"none\",\"notice\",\"obviously\",\"plus\",\"step\",\"swear\",\"30\",\"breast\",\"clear\",\"cold\",\"gunther\",\"hat\",\"owe\",\"yell\",\"york\",\"across\",\"become\",\"cover\",\"crap\",\"floor\",\"hide\",\"horrible\",\"ride\",\"amy\",\"boat\",\"chef\",\"extra\",\"hospital\",\"idiot\",\"knock\",\"note\",\"wake\",\"beer\"],\"cumulative proportion\":[\"4.08%\",\"7.99%\",\"11.32%\",\"13.65%\",\"15.67%\",\"17.69%\",\"19.34%\",\"20.74%\",\"22.07%\",\"23.34%\",\"24.60%\",\"25.71%\",\"26.75%\",\"27.78%\",\"28.78%\",\"29.77%\",\"30.76%\",\"31.71%\",\"32.66%\",\"33.54%\",\"34.42%\",\"35.27%\",\"36.11%\",\"36.93%\",\"37.75%\",\"38.54%\",\"39.31%\",\"40.03%\",\"40.75%\",\"41.45%\",\"42.13%\",\"42.80%\",\"43.45%\",\"44.06%\",\"44.66%\",\"45.26%\",\"45.84%\",\"46.41%\",\"46.97%\",\"47.52%\",\"48.06%\",\"48.57%\",\"49.07%\",\"49.52%\",\"49.97%\",\"50.40%\",\"50.81%\",\"51.23%\",\"51.64%\",\"52.04%\",\"52.43%\",\"52.82%\",\"53.21%\",\"53.59%\",\"53.96%\",\"54.34%\",\"54.71%\",\"55.08%\",\"55.45%\",\"55.82%\",\"56.17%\",\"56.51%\",\"56.86%\",\"57.20%\",\"57.53%\",\"57.85%\",\"58.17%\",\"58.48%\",\"58.79%\",\"59.09%\",\"59.38%\",\"59.66%\",\"59.93%\",\"60.19%\",\"60.45%\",\"60.70%\",\"60.95%\",\"61.20%\",\"61.44%\",\"61.68%\",\"61.90%\",\"62.12%\",\"62.34%\",\"62.56%\",\"62.77%\",\"62.98%\",\"63.19%\",\"63.39%\",\"63.60%\",\"63.79%\",\"63.99%\",\"64.19%\",\"64.38%\",\"64.57%\",\"64.75%\",\"64.93%\",\"65.12%\",\"65.30%\",\"65.48%\",\"65.65%\",\"65.83%\",\"66.00%\",\"66.18%\",\"66.34%\",\"66.51%\",\"66.67%\",\"66.83%\",\"66.98%\",\"67.13%\",\"67.28%\",\"67.43%\",\"67.58%\",\"67.72%\",\"67.86%\",\"68.00%\",\"68.14%\",\"68.27%\",\"68.41%\",\"68.54%\",\"68.67%\",\"68.80%\",\"68.93%\",\"69.06%\",\"69.19%\",\"69.31%\",\"69.44%\",\"69.56%\",\"69.68%\",\"69.80%\",\"69.92%\",\"70.04%\",\"70.16%\",\"70.27%\",\"70.39%\",\"70.50%\",\"70.61%\",\"70.72%\",\"70.83%\",\"70.94%\",\"71.05%\",\"71.16%\",\"71.27%\",\"71.38%\",\"71.49%\",\"71.59%\",\"71.70%\",\"71.80%\",\"71.91%\",\"72.01%\",\"72.12%\",\"72.22%\",\"72.32%\",\"72.42%\",\"72.52%\",\"72.62%\",\"72.72%\",\"72.81%\",\"72.91%\",\"73.01%\",\"73.10%\",\"73.19%\",\"73.29%\",\"73.38%\",\"73.47%\",\"73.55%\",\"73.64%\",\"73.73%\",\"73.82%\",\"73.90%\",\"73.99%\",\"74.07%\",\"74.16%\",\"74.24%\",\"74.32%\",\"74.41%\",\"74.49%\",\"74.57%\",\"74.65%\",\"74.73%\",\"74.81%\",\"74.89%\",\"74.97%\",\"75.05%\",\"75.12%\",\"75.20%\",\"75.28%\",\"75.36%\",\"75.43%\",\"75.51%\",\"75.58%\",\"75.66%\",\"75.73%\",\"75.80%\",\"75.88%\",\"75.95%\",\"76.02%\",\"76.09%\",\"76.16%\",\"76.24%\",\"76.31%\",\"76.38%\",\"76.45%\",\"76.52%\",\"76.59%\",\"76.66%\",\"76.73%\",\"76.79%\",\"76.86%\",\"76.93%\",\"77.00%\",\"77.06%\",\"77.13%\",\"77.20%\",\"77.26%\",\"77.32%\",\"77.39%\",\"77.45%\",\"77.51%\",\"77.58%\",\"77.64%\",\"77.70%\",\"77.76%\",\"77.82%\",\"77.88%\",\"77.94%\",\"78.00%\",\"78.06%\",\"78.12%\",\"78.17%\",\"78.23%\",\"78.28%\",\"78.34%\",\"78.39%\",\"78.45%\",\"78.50%\",\"78.55%\",\"78.60%\",\"78.66%\",\"78.71%\",\"78.76%\",\"78.81%\",\"78.86%\",\"78.91%\",\"78.96%\",\"79.01%\",\"79.07%\",\"79.11%\",\"79.16%\",\"79.21%\",\"79.26%\",\"79.31%\",\"79.36%\",\"79.41%\",\"79.46%\",\"79.51%\",\"79.55%\",\"79.60%\",\"79.65%\",\"79.70%\",\"79.74%\",\"79.79%\",\"79.84%\",\"79.88%\",\"79.93%\",\"79.97%\",\"80.02%\",\"80.07%\",\"80.11%\",\"80.16%\",\"80.20%\",\"80.24%\",\"80.29%\",\"80.33%\",\"80.37%\",\"80.42%\",\"80.46%\",\"80.50%\",\"80.55%\",\"80.59%\",\"80.63%\",\"80.67%\",\"80.72%\",\"80.76%\",\"80.80%\",\"80.84%\",\"80.88%\",\"80.92%\",\"80.96%\",\"81.00%\",\"81.04%\",\"81.08%\",\"81.12%\",\"81.16%\",\"81.20%\",\"81.24%\",\"81.28%\",\"81.32%\",\"81.36%\",\"81.40%\",\"81.43%\",\"81.47%\",\"81.51%\",\"81.55%\",\"81.58%\",\"81.62%\",\"81.66%\",\"81.69%\",\"81.73%\",\"81.77%\",\"81.80%\",\"81.84%\",\"81.88%\",\"81.91%\",\"81.95%\",\"81.98%\",\"82.02%\",\"82.06%\",\"82.09%\",\"82.12%\",\"82.16%\",\"82.19%\",\"82.23%\",\"82.26%\",\"82.30%\",\"82.33%\",\"82.37%\",\"82.40%\",\"82.43%\",\"82.47%\",\"82.50%\",\"82.53%\",\"82.56%\",\"82.60%\",\"82.63%\",\"82.66%\",\"82.69%\",\"82.73%\",\"82.76%\",\"82.79%\",\"82.82%\",\"82.85%\",\"82.88%\",\"82.92%\",\"82.95%\",\"82.98%\",\"83.01%\",\"83.04%\",\"83.07%\",\"83.10%\",\"83.13%\",\"83.16%\",\"83.19%\",\"83.22%\",\"83.25%\",\"83.28%\",\"83.31%\",\"83.34%\",\"83.37%\",\"83.40%\",\"83.43%\",\"83.46%\",\"83.49%\",\"83.52%\",\"83.54%\",\"83.57%\",\"83.60%\",\"83.63%\",\"83.66%\",\"83.69%\",\"83.72%\",\"83.74%\",\"83.77%\",\"83.80%\",\"83.83%\",\"83.86%\",\"83.89%\",\"83.91%\",\"83.94%\",\"83.97%\",\"84.00%\",\"84.02%\",\"84.05%\",\"84.08%\",\"84.11%\",\"84.13%\",\"84.16%\",\"84.19%\",\"84.22%\",\"84.24%\",\"84.27%\",\"84.29%\",\"84.32%\",\"84.35%\",\"84.37%\",\"84.40%\",\"84.43%\",\"84.45%\",\"84.48%\",\"84.50%\",\"84.53%\",\"84.55%\",\"84.58%\",\"84.61%\",\"84.63%\",\"84.66%\",\"84.68%\",\"84.71%\",\"84.73%\",\"84.76%\",\"84.78%\",\"84.81%\",\"84.83%\",\"84.86%\",\"84.88%\",\"84.90%\",\"84.93%\",\"84.95%\",\"84.98%\",\"85.00%\",\"85.03%\",\"85.05%\",\"85.07%\",\"85.10%\",\"85.12%\",\"85.14%\",\"85.17%\",\"85.19%\",\"85.22%\",\"85.24%\",\"85.26%\",\"85.29%\",\"85.31%\",\"85.33%\",\"85.36%\",\"85.38%\",\"85.40%\",\"85.42%\",\"85.45%\",\"85.47%\",\"85.49%\",\"85.52%\",\"85.54%\",\"85.56%\",\"85.58%\",\"85.61%\",\"85.63%\",\"85.65%\",\"85.67%\",\"85.69%\",\"85.72%\",\"85.74%\",\"85.76%\",\"85.78%\",\"85.80%\",\"85.83%\",\"85.85%\",\"85.87%\",\"85.89%\",\"85.91%\",\"85.93%\",\"85.95%\",\"85.98%\",\"86.00%\",\"86.02%\",\"86.04%\",\"86.06%\",\"86.08%\",\"86.10%\",\"86.12%\",\"86.14%\",\"86.16%\",\"86.18%\",\"86.20%\",\"86.22%\",\"86.24%\",\"86.26%\",\"86.28%\",\"86.30%\",\"86.32%\",\"86.34%\",\"86.36%\",\"86.38%\",\"86.40%\",\"86.42%\",\"86.44%\",\"86.46%\",\"86.48%\",\"86.50%\",\"86.52%\",\"86.54%\",\"86.56%\",\"86.58%\",\"86.60%\",\"86.61%\",\"86.63%\",\"86.65%\",\"86.67%\",\"86.69%\",\"86.71%\",\"86.73%\",\"86.75%\",\"86.76%\",\"86.78%\",\"86.80%\",\"86.82%\",\"86.84%\",\"86.86%\",\"86.88%\",\"86.89%\",\"86.91%\",\"86.93%\",\"86.95%\",\"86.97%\",\"86.99%\",\"87.00%\",\"87.02%\",\"87.04%\",\"87.06%\",\"87.07%\",\"87.09%\",\"87.11%\",\"87.13%\",\"87.14%\",\"87.16%\",\"87.18%\",\"87.20%\",\"87.21%\",\"87.23%\",\"87.25%\",\"87.26%\",\"87.28%\",\"87.30%\",\"87.31%\",\"87.33%\",\"87.35%\",\"87.36%\",\"87.38%\",\"87.40%\",\"87.41%\",\"87.43%\",\"87.45%\",\"87.46%\",\"87.48%\",\"87.50%\",\"87.51%\",\"87.53%\",\"87.54%\",\"87.56%\",\"87.58%\",\"87.59%\",\"87.61%\",\"87.62%\",\"87.64%\",\"87.65%\",\"87.67%\",\"87.68%\",\"87.70%\",\"87.72%\",\"87.73%\",\"87.75%\",\"87.76%\",\"87.78%\",\"87.79%\",\"87.81%\",\"87.82%\",\"87.84%\",\"87.85%\",\"87.87%\",\"87.88%\",\"87.90%\",\"87.91%\",\"87.93%\",\"87.94%\",\"87.96%\",\"87.97%\",\"87.99%\",\"88.00%\",\"88.02%\",\"88.03%\",\"88.05%\",\"88.06%\",\"88.07%\",\"88.09%\",\"88.10%\",\"88.12%\",\"88.13%\",\"88.15%\",\"88.16%\",\"88.18%\",\"88.19%\",\"88.20%\",\"88.22%\",\"88.23%\",\"88.25%\",\"88.26%\",\"88.27%\",\"88.29%\",\"88.30%\",\"88.32%\",\"88.33%\",\"88.34%\",\"88.36%\",\"88.37%\",\"88.38%\",\"88.40%\",\"88.41%\",\"88.43%\",\"88.44%\",\"88.45%\",\"88.47%\",\"88.48%\",\"88.49%\",\"88.51%\",\"88.52%\",\"88.53%\",\"88.55%\",\"88.56%\",\"88.58%\",\"88.59%\",\"88.60%\",\"88.62%\",\"88.63%\",\"88.64%\",\"88.66%\",\"88.67%\",\"88.68%\",\"88.69%\",\"88.71%\",\"88.72%\",\"88.73%\",\"88.75%\",\"88.76%\",\"88.77%\",\"88.79%\",\"88.80%\",\"88.81%\",\"88.83%\",\"88.84%\",\"88.85%\",\"88.86%\",\"88.88%\",\"88.89%\",\"88.90%\",\"88.91%\",\"88.93%\",\"88.94%\",\"88.95%\",\"88.96%\",\"88.98%\",\"88.99%\",\"89.00%\",\"89.01%\",\"89.02%\",\"89.04%\",\"89.05%\",\"89.06%\",\"89.07%\",\"89.08%\",\"89.10%\",\"89.11%\",\"89.12%\",\"89.13%\",\"89.14%\",\"89.16%\",\"89.17%\",\"89.18%\",\"89.19%\",\"89.20%\",\"89.21%\",\"89.22%\",\"89.24%\",\"89.25%\",\"89.26%\",\"89.27%\",\"89.28%\",\"89.29%\",\"89.30%\",\"89.32%\",\"89.33%\",\"89.34%\",\"89.35%\",\"89.36%\",\"89.37%\",\"89.38%\",\"89.39%\",\"89.40%\",\"89.42%\",\"89.43%\",\"89.44%\",\"89.45%\",\"89.46%\",\"89.47%\",\"89.48%\",\"89.49%\",\"89.50%\",\"89.51%\",\"89.52%\",\"89.54%\",\"89.55%\",\"89.56%\",\"89.57%\",\"89.58%\",\"89.59%\",\"89.60%\",\"89.61%\",\"89.62%\",\"89.63%\",\"89.64%\",\"89.65%\",\"89.66%\",\"89.67%\",\"89.68%\",\"89.69%\",\"89.71%\",\"89.72%\",\"89.73%\",\"89.74%\",\"89.75%\",\"89.76%\",\"89.77%\",\"89.78%\",\"89.79%\",\"89.80%\",\"89.81%\",\"89.82%\",\"89.83%\",\"89.84%\",\"89.85%\",\"89.86%\",\"89.87%\",\"89.88%\",\"89.89%\",\"89.90%\",\"89.91%\",\"89.92%\",\"89.93%\",\"89.94%\",\"89.95%\",\"89.96%\",\"89.97%\",\"89.98%\",\"89.99%\",\"90.00%\",\"90.01%\",\"90.02%\",\"90.03%\",\"90.04%\",\"90.05%\",\"90.06%\",\"90.07%\"]},\"columns\":[{\"accessor\":\"index\",\"name\":\"index\",\"type\":\"numeric\"},{\"accessor\":\"n\",\"name\":\"n\",\"type\":\"numeric\"},{\"accessor\":\"word\",\"name\":\"word\",\"type\":\"character\"},{\"accessor\":\"cumulative proportion\",\"name\":\"cumulative proportion\",\"type\":\"character\"}],\"defaultPageSize\":10,\"paginationType\":\"numbers\",\"showPageInfo\":true,\"minRows\":1,\"dataKey\":\"7526a5e0f65fcaf6b6a12ce4556c30c2\",\"key\":\"7526a5e0f65fcaf6b6a12ce4556c30c2\"},\"children\":[]},\"class\":\"reactR_markup\"},\"evals\":[],\"jsHooks\":[]}\r\r","date":1596499200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596575162,"objectID":"47b012af73c448f1c7045d11d52c184f","permalink":"/2020/08/04/friends/","publishdate":"2020-08-04T00:00:00Z","relpermalink":"/2020/08/04/friends/","section":"post","summary":"A few years ago, The New York Times published an article about several Major League Baseball players who use the sitcom Friends to improve their English. Friends seems to be a very popular tool for learning English: there’s even an ESL program developed around it, and you can easily find advice on how to use Friends as a language learning tool.\nIn my own language learning I’ve relied heavily on frequency dictionaries, which order words by their popularity.","tags":[],"title":"How Many Words Do You Need to Know to Watch Friends?","type":"post"},{"authors":[],"categories":[],"content":"\rA Gaussian Process (GP) is a process for which any finite set of observations follows a multivariate normal distribution. GPs are defined by their mean and a kernel function that gives the covariance between any two observations. They are useful in Bayesian statistics as priors over function spaces.\nTo denote a GP prior over a set of observations \\(\\bm{y} = \\{y_1, y_2, \\cdots, y_n\\}\\) at points \\(\\bm{x} = \\{ x_1, x_2, \\cdots, x_n\\}\\), we write:\r\\[\r\\bm{y} \\sim \\mathcal{GP}(\\bm{\\mu}, \\bm{\\Sigma})\r\\]\nwhere the covariance matrix \\(\\bm{\\Sigma}\\) is computed via a kernel function \\(k(x, x^\\prime)\\). A popular choice of kernel function that we will consider in this post is the squared exponential kernel:\r\\[\rk(x_i, x_j) = \\alpha^2 \\exp\\left(-\r\\frac{(x_i - x_j)^2}{2\\ell^2} \\right)\r\\]\nwhere \\(\\alpha\\) is a scale parameter and \\(\\ell\\) is a length-scale parameter which controls the strength of the association between points as they become farther apart.\nIntuitively, in order to define a GP we need to be able to write down the covariance between any two points.\nLet’s write this kernel function in R, and use it to draw samples from a GP. For simplicity, we will fix \\(\\bm{\\mu} = \\bm{0}\\) for all the GPs we work with in this post. First, define the kernel function:\nk \u0026lt;- function(x_i, x_j, alpha, l) {\ralpha^2 * exp(-(x_i - x_j)^2 / (2 * l^2))\r}\rLet’s write a helper function that, given a kernel and a set of points, generates a full covariance matrix:\ncovariance_from_kernel \u0026lt;- function(x, kernel, ...) {\router(x, x, kernel, ...)\r}\rand a function to draw from a mean-zero GP with a given covariance kernel:\ngp_draw \u0026lt;- function(draws, x, Sigma, ...) {\rmu \u0026lt;- rep(0, length(x))\rmvtnorm::rmvnorm(draws, mu, Sigma)\r}\rNow we can draw 10 samples from a mean-zero GP with a squared-exponential kernel with parameters \\(\\alpha=1\\), \\(l=1\\):\nn \u0026lt;- 100 # number of points to draw\rx \u0026lt;- seq(1, 10, length.out = n) # position of each point\r# Kernel parameters\ralpha \u0026lt;- 1\rl \u0026lt;- 1\rset.seed(1)\r# Draw 10 samples\rSigma \u0026lt;- covariance_from_kernel(x, k, alpha = alpha, l = l)\ry \u0026lt;- gp_draw(10, x, Sigma)\rmatplot(x, t(y), type = \u0026#39;l\u0026#39;, xlab = \u0026#39;x\u0026#39;, ylab = \u0026#39;y\u0026#39;)\rDerivative of a GP\rThe derivative of a GP is also a GP, with its existence determined by the differentiability of its mean and kernel functions. The squared exponential kernel is infinitely differentiable, so the associated Gaussian Process has infinitely many derivatives.\nThis derivative is useful because we may have prior knowledge about likely values of the derivative. For example, monotonicity constraints can be encoded as prior knowledge that the derivative is always positive or negative. In other cases we may have direct observations of the derivative that we would like to incorporate into model fitting.\nLet \\(\\bm{y}^\\prime = \\{ y^\\prime_1, y^\\prime_2, \\cdots, y^\\prime_{n^\\prime} \\}\\) be a set of derivative observations. The derivative GP, which we’ll denote \\(\\mathcal{GP}^\\prime\\), is given by\r\\[\r\\bm{y}^\\prime \\sim \\mathcal{GP}^\\prime(\\frac{d}{dx}\\bm{\\mu}, \\frac{d}{dx}\\bm{\\Sigma})\r\\]\rwhere the derivative of the covariance matrix is defined by the derivative of the original kernel function with respect to both of its inputs, which we will denote \\(k_{11}\\) to indicate that both arguments are derivative observations:\r\\[\rk_{11}(x_i, x_j) = \\frac{\\partial}{\\partial x_i \\ \\partial x_j} k(x_i, x_j) = \\frac{ \\alpha^2 }{ \\ell^4 }\\left( l^2 - (x_i - x_j)^2 \\right) \\exp\\left( -\\frac{(x_i - x_j)^2}{2\\ell^2} \\right)\r\\]\nThis is enough information to be able to draw random samples from the derivative of a GP. Let’s write the new kernel \\(k_11\\) in R, and sample from the derivative GP:\nk_11 \u0026lt;- function(x_i, x_j, alpha, l) {\ralpha^2 / l^4 * (l^2 - (x_i - x_j)^2) * exp(-(x_i - x_j)^2 / (2*l^2))\r}\rn_prime \u0026lt;- 100\rx_prime \u0026lt;- seq(1, 10, length.out = n_prime)\r# Draw 10 samples\rSigma_prime \u0026lt;- covariance_from_kernel(x_prime, k_11, alpha = alpha, l = l)\ry_prime \u0026lt;- gp_draw(10, x_prime, Sigma_prime)\rmatplot(x_prime, t(y_prime), type = \u0026#39;l\u0026#39;, xlab = \u0026#39;x\u0026#39;, ylab = \u0026quot;y\u0026#39;\u0026quot;)\rIt’s hard to interpret this plot because we can’t compare these derivative values to the corresponding normal GP. Fortunately, it’s possible to sample from the joint distributions of the observations and its derivatives, which is in fact also a GP. To define it, we need to know the covariance between an observation and a derivative observation.\nLet \\(k_{01}(x_i, x_j)\\) be the covariance between an observation at \\(x_i\\) and a derivative observation at \\(x_j\\). Then\r\\[\rk_{01}(x_i, x_j) = \\frac{\\alpha^2}{\\ell^2} (x_i - x_j) \\exp\\left( -\\frac{(x_i - x_j)^2}{2\\ell^2} \\right)\r\\]\nSimilarly, \\(k_{10}(x_i, x_j)\\) is the covariance between a derivative observation at \\(x_i\\) and an observation at \\(x_j\\):\r\\[\rk_{10}(x_i, x_j) = \\frac{\\alpha^2}{\\ell^2} (x_j - x_i) \\exp\\left( -\\frac{(x_i - x_j)^2}{2\\ell^2} \\right)\r\\]\rAs we would expect from the symmetry of covariance matrices, \\(k_{01}(x_i, x_j) = k_{10}(x_j, x_i)\\). That means we can get away with just defining one of them in R:\nk_01 \u0026lt;- function(x_i, x_j, alpha, l) {\ralpha^2 / l^2 * (x_i - x_j) * exp(-(x_i - x_j)^2 / (2*l^2))\r}\rNow construct a combined vector by concatenating the observations \\(y\\) and derivative observations \\(y^\\prime\\), denoted \\(y^\\mathrm{all}\\), of length \\(n + n^\\prime\\). Let \\(\\bm{x}^{\\mathrm{all}}\\) be the corresponding positions of each observation in \\(\\bm{y}^{\\mathrm{all}}\\). Let \\(\\bm{d}^{\\mathrm{all}}\\) be a \\(n+n^\\prime\\) length vector where \\(d^\\mathrm{all}_i = 1\\) if \\(y^{\\mathrm{all}}_i\\) is a derivative observation, and \\(d^{\\mathrm{all}}_i = 0\\) if \\(y^{\\mathrm{all}}_i\\) is a normal observation. Then define a new kernel over the joint observations:\r\\[\rk^{\\mathrm{all}}(x_i, x_j, d_i, d_j) = \\begin{cases}\rk(x_i, x_j) \u0026amp; d_i = 0, d_j = 0 \\text{ (both normal observations)} \\\\\rk_{01}(x_i, x_j) \u0026amp; d_i = 0, d_j = 0 \\text{ (one derivative, one normal)} \\\\\rk_{10}(x_i, x_j) \u0026amp; d_i = 1, d_j = 0 \\text{ (one derivative, one normal)} \\\\\rk_{11}(x_i, x_j) \u0026amp; d_i = 1, d_j = 0 \\text{ (both derivatives)}\r\\end{cases}\r\\]\nIn R:\nk_all \u0026lt;- function(x_i, x_j, d_i, d_j, ...) {\rdplyr::case_when(\rd_i == 0 \u0026amp; d_j == 0 ~ k(x_i, x_j, ...),\rd_i == 0 \u0026amp; d_j == 1 ~ k_01(x_i, x_j, ...),\rd_i == 1 \u0026amp; d_j == 0 ~ k_01(x_j, x_i, ...),\rd_i == 1 \u0026amp; d_j == 1 ~ k_11(x_i, x_j, ...),\r)\r}\rWe also need a new function to form the joint covariance matrix:\njoint_covariance_from_kernel \u0026lt;- function(x, d, kernel, ...) {\router(1:length(x), 1:length(x),\rfunction(i, j) kernel(x[i], x[j], d[i], d[j], ...))\r}\rFinally, I’m going to split out the plotting code into a separate function as it gets more complicated than before:\nplot_joint_gp \u0026lt;- function(x, y, d) {\rplot(x[d == 0], y[d == 0], type = \u0026#39;l\u0026#39;, ylim = range(y), col = \u0026#39;black\u0026#39;, xlab = \u0026#39;x\u0026#39;, ylab = \u0026#39;y\u0026#39;)\rlines(x[d == 1], y[d == 1], type = \u0026#39;l\u0026#39;, col = \u0026#39;blue\u0026#39;, lty = 2)\rabline(h = 0, lty = 3, col = \u0026quot;gray\u0026quot;)\rlegend(\u0026#39;topright\u0026#39;, legend = c(\u0026quot;GP\u0026quot;, \u0026quot;Derivative of GP\u0026quot;),\rcol = c(\u0026quot;black\u0026quot;, \u0026quot;blue\u0026quot;), lty = 1:2)\r}\rNow we can sample from the joint distribution of the observations and derivatives:\nx_all \u0026lt;- c(x, x_prime)\rd_all \u0026lt;- c(rep(0, length(x)), rep(1, length(x_prime)))\rSigma_all \u0026lt;- joint_covariance_from_kernel(x_all, d_all, k_all, alpha = alpha, l = l)\ry_all \u0026lt;- gp_draw(1, x_all, Sigma_all)\rplot_joint_gp(x_all, y_all[1,], d_all)\rLet’s plot one more:\ny_all \u0026lt;- gp_draw(1, x_all, Sigma_all)\rplot_joint_gp(x_all, y_all[1,], d_all)\rSo far we’ve seen how derivatives of GPs are defined, and how to draw from the joint distribution of a GP and its derivative. In future posts we’ll look at fitting GPs in Stan with derivative observations, and at shape-constrained GPs.\nReferences\r\rAndrew McHutchon, Differentiating Gaussian Processes\rRasmussen et al. 2006, Gaussian Processes for Machine Learning, Chapter 9\r\r\r\r","date":1593993600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594057361,"objectID":"429fbe5f84ac3c02f23058e8f92b4797","permalink":"/2020/07/06/gaussian-process-derivatives/","publishdate":"2020-07-06T00:00:00Z","relpermalink":"/2020/07/06/gaussian-process-derivatives/","section":"post","summary":"A Gaussian Process (GP) is a process for which any finite set of observations follows a multivariate normal distribution. GPs are defined by their mean and a kernel function that gives the covariance between any two observations. They are useful in Bayesian statistics as priors over function spaces.\nTo denote a GP prior over a set of observations \\(\\bm{y} = \\{y_1, y_2, \\cdots, y_n\\}\\) at points \\(\\bm{x} = \\{ x_1, x_2, \\cdots, x_n\\}\\), we write:\r\\[\r\\bm{y} \\sim \\mathcal{GP}(\\bm{\\mu}, \\bm{\\Sigma})\r\\]","tags":[],"title":"Derivatives of a Gaussian Process","type":"post"},{"authors":[],"categories":[],"content":"\rGun dealers in the U.S. are required to conduct instant background checks before selling weapons to individuals. The FBI provides data for the number of these background checks performed by month/year, which serves as a proxy for the total number of gun sales in the U.S.\nI brought the data into R for a quick and dirty analysis, with the intent of finding spikes in background checks around major events.\nFirst, let’s take a look raw data. There are a few obvious spikes in the later years, which correspond to the Sandy Hook (December 2012) and San Bernadino (December 2015) shootings.\nNext, I fit a negative binomial generalized linear model that accounts for an overall trend using a 3rd order cubic spline and monthly seasonal variation:\nmodel \u0026lt;- MASS::glm.nb(value ~ bs(date) + month, dat)\rEven such a simple model does a decent job fitting the data, although it gets much worse in later years as the variance in the data increases:\nMore interesting is the plot of the residuals from the model, which show spikes in background checks that aren’t accounted for by the model. This makes a couple of other peaks jump out that are correlated with notable events, like 9/11 and Obama’s election:\nThere are a few other peaks that I don’t have explanations for, like in late 1999 and in the beginning of 2014.\nI think it’s interesting how the model residuals let us see spikes in background checks that we couldn’t see in the raw data. The tradeoff is that the model residuals are conditional on the model choice; choosing a different model might lead to a different plot. If we want to answer the question “were there spikes in gun background checks”, we now have to condition our conclusions on that model choice, which complicates interpretation.\n","date":1580515200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580604526,"objectID":"709451c03a1a5ac9666b3e3d8a1ff1df","permalink":"/2020/02/01/firearm-background-check-timeseries-modeling/","publishdate":"2020-02-01T00:00:00Z","relpermalink":"/2020/02/01/firearm-background-check-timeseries-modeling/","section":"post","summary":"Gun dealers in the U.S. are required to conduct instant background checks before selling weapons to individuals. The FBI provides data for the number of these background checks performed by month/year, which serves as a proxy for the total number of gun sales in the U.S.\nI brought the data into R for a quick and dirty analysis, with the intent of finding spikes in background checks around major events.","tags":[],"title":"Firearm Background Check Timeseries Modeling","type":"post"},{"authors":["Herb Susmann","Laurel A. Schaider","Kathryn M. Rodgers","Ruthann A. Rudel"],"categories":null,"content":"","date":1570579200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570579200,"objectID":"0a8f393f4d0a2bbac4244b8e586078e7","permalink":"/publication/pfas/","publishdate":"2019-10-09T00:00:00Z","relpermalink":"/publication/pfas/","section":"publication","summary":"DERBI is a digital tool for reporting back personal results to participants in exposure studies.","tags":["Silent Spring Institute"],"title":"Dietary Habits Related to Food Packaging and Population Exposure to PFASs","type":"publication"},{"authors":["Herb Susmann"],"categories":[],"content":"\rI often use random walk/autoregressive models in my research as a component in time-series analysis, and I wanted to get some more experience fitting them to data. FiveThirtyEight publishes several polling datasets, including polling for the 2020 Democratic presidential primary. I used Stan to fit a Bayesian random walk model to the polling data, which I describe below. The Stan and R code used in this post is available as a Github gist.\nLet \\(\\delta_{c,t}\\) be the true proportion of voters in favor of candidate \\(c\\) at time \\(t\\). Our modeling assumption is that the logit-transform of \\(\\delta_{c,t}\\) follows a random walk; that is:\r\\[\r\\mathrm{logit}(\\delta_{c,t}) \\sim \\mathrm{N}\\left(\\mathrm{logit}(\\delta_{c,t-1}), \\tau^2\\right)\r\\]\rWe can’t observe \\(\\delta_{c,t}\\) directly; we have to infer it through the noisy observations we have from polls.\nLet \\(s_{i}\\) be the sample size of poll \\(c[i]\\) and \\(y_{i}\\) the number of poll respondents in favor of candidate \\(c[i]\\) at time \\(t[i]\\). Let \\(\\phi_i\\) be the proportion of poll respondents in favor of candidate \\(c[i]\\) at time \\(t[i]\\). To incorporate sampling error, we model \\(y_i\\) as binomial:\r\\[\ry_i \\sim \\mathrm{Binomial}(s_i, \\phi_i)\r\\]\rWe also allow for added variance in our observations by relating \\(\\phi_i\\) to the true logit proportion \\(\\delta_{c[i], t[i]}\\) with a normal distribution:\r\\[\r\\mathrm{logit}(\\phi_i) \\sim \\mathrm{N}(\\mathrm{logit}(\\delta_{c[i], t[i]}), \\sigma^2)\r\\]\nTo finish defining the model half-normal priors on the hyperparameters. The prior for \\(\\tau^2\\) has a small variance to improve identification of the model (a vaguer prior can cause the MCMC chains to not mix well.)\r\\[\r\\begin{aligned}\r\\tau^2 \u0026amp;\\sim \\mathrm{N}(0, 0.02)[0, \\infty] \\\\\r\\sigma^2 \u0026amp;\\sim \\mathrm{N}(0, 1)[0, \\infty] \\end{aligned}\r\\]\nHere is the Stan representation of the statistical model:\nS4 class stanmodel \u0026#39;random_walk\u0026#39; coded as follows:\rdata {\rint\u0026lt;lower=0\u0026gt; T; // Number of timepoints\rint\u0026lt;lower=0\u0026gt; C; // Number of candidates\rint\u0026lt;lower=0\u0026gt; N; // Number of poll observations\rint sample_size[N]; // Sample size of each poll\rint y[N]; // Number of respondents in poll for candidate (approximate)\rint\u0026lt;lower=1, upper=T\u0026gt; get_t_i[N]; // timepoint for ith observation\rint\u0026lt;lower=1, upper=C\u0026gt; get_c_i[N]; // candidate for ith observation\r}\rparameters {\rmatrix[C, T] delta_logit; // Percent for candidate c at time t\rreal\u0026lt;lower=0, upper=1\u0026gt; phi[N]; // Percent of participants in poll for candidate\rreal\u0026lt;lower=0\u0026gt; tau; // Random walk variance\rreal\u0026lt;lower=0,upper=0.5\u0026gt; sigma; // Overdispersion of observations\r}\rmodel {\r// Priors\rtau ~ normal(0, 0.2);\rsigma ~ normal(0, 1);\r// Random walk\rfor(c in 1:C) {\rdelta_logit[c, 2:T] ~ normal(delta_logit[c, 1:(T - 1)], tau);\r}\r// Observed data\ry ~ binomial(sample_size, phi);\rfor(i in 1:N) {\r// Overdispersion\rdelta_logit[get_c_i[i], get_t_i[i]] ~ normal(logit(phi[i]), sigma);\r}\r}\rgenerated quantities {\rmatrix[C, T] delta = inv_logit(delta_logit);\r} \rThe raw dataset that we are going to fit:\rI fitted the Stan model to the data using the standard HMC-NUTS algorithm and 1000 MCMC iterations. The plot below shows the posterior median with 75% and 95% credible intervals.\nOne issue with this model is that it oversmooths large bumps in the polls. For example, Harris had a bump after the first debate, which the model smooths into an uptick leading into the debate that is not justified in the data. The model could be improved by allowing for these shocks, for example by restarting the random walk after key dates like the debates which we know are likely to cause discontinuities in the results.\n","date":1565395200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565437002,"objectID":"1c3815dbbb0384f1a8d64e47e9cf8c24","permalink":"/2019/08/10/presidential-primary-polling-analysis-in-stan/","publishdate":"2019-08-10T00:00:00Z","relpermalink":"/2019/08/10/presidential-primary-polling-analysis-in-stan/","section":"post","summary":"I often use random walk/autoregressive models in my research as a component in time-series analysis, and I wanted to get some more experience fitting them to data. FiveThirtyEight publishes several polling datasets, including polling for the 2020 Democratic presidential primary. I used Stan to fit a Bayesian random walk model to the polling data, which I describe below. The Stan and R code used in this post is available as a Github gist.","tags":["r","stan"],"title":"Presidential Primary Polling Analysis in Stan","type":"post"},{"authors":null,"categories":null,"content":"","date":1565395200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565395200,"objectID":"2b3c8a25669ebedda573f1ac8c1265f0","permalink":"/project/reactor/","publishdate":"2019-08-10T00:00:00Z","relpermalink":"/project/reactor/","section":"project","summary":"Reactive notebooks for R","tags":["R, 📦"],"title":"Reactor","type":"project"},{"authors":null,"categories":null,"content":"\rAutoregressive (AR) processes are a popular choice for modeling time-varying processes. AR processes are typically written down as a set of conditional distributions, but if we do some algebra we can show how they can also be written as a Gaussian process. One reason having a Guassian process representation is useful is because it makes it more clear how an AR process can be incorporated into larger models, like a spatio-temporal model. In this post, we’ll start with defining an AR process and deriving its mean and variance, then we’ll derive its joint distribution, which is a Gaussian process.\nAR processes\rLet \\(\\mathbf{Y} = \\left\\{ Y_1, Y_2, \\dots, Y_n \\right\\}\\) be a set of random variables indexed by time. An aurogressive model assumes that \\(\\mathbf{Y}\\) is correlated over time. An AR model is typically described by defining \\(Y_t\\) in terms of \\(Y_{t-1}\\):\n\\[\rY_{t} = \\rho Y_{t-1} + \\epsilon_{t}\r\\]\rwhere \\(\\epsilon_{t}\\sim N\\left(0,\\sigma_{\\epsilon}^{2}\\right)\\) and \\(\\rho \\in \\mathbb{R}\\) is a parameter that controls the degree to which \\(Y_t\\) is correlated with \\(Y_{t-1}\\). This model is called an AR process of order 1 because \\(Y_t\\) only depends on \\(Y_{t-1}\\).\nWe can also rearrange terms to emphasize that this representation defines the conditional distribution of \\(Y_{t}\\) given \\(Y_{t-1}\\):\n\\[\r\\begin{aligned}\rY_{t} \\vert Y_{t-1} \\sim\u0026amp; N(\\rho Y_{t-1}, \\sigma_\\epsilon^2) \\\\\rY_1 \\sim\u0026amp; N(0, \\frac{\\sigma_\\epsilon^2}{1-\\rho^2})\r\\end{aligned}\r\\]\nWhere the variance of \\(Y_1\\) comes from the unconditional variance, which is derived below. The stationarity condition of an AR process is that each \\(Y_t\\) has the same distribution; that is, \\(\\mu = \\mathrm{E}(Y_i) = \\mathrm{E}Y_j\\) and \\(\\sigma^2 = \\mathrm{Var}(Y_i) = \\mathrm{Var}(Y_j)\\) for all \\(i, j\\).\nNow we can derive the unconditional mean and variance of \\(Y_t\\):\n\\[\r\\begin{aligned}\r\\mathrm{E}\\left(Y_{t}\\right) \u0026amp;= \\mathrm{E}\\left(\\rho Y_{t - 1} + \\epsilon_{t} \\right)\\\\\r\u0026amp;= \\rho \\mathrm{E}\\left( Y_{t - 1 } \\right)\\\\\r\\mu \u0026amp;= \\rho\\mu\\ \\text{(apply stationarity condition)} \\\\\r\\mu \u0026amp;= 0 \\\\\r\\mathrm{Var}\\left(Y_{t}\\right) \u0026amp;= \\mathrm{Var}\\left(\\rho Y_{t-1} + \\epsilon_{t}\\right)\\\\\r\u0026amp;= \\rho^{2}\\mathrm{Var}(Y_{t-1}) + \\mathrm{Var}\\left(\\epsilon_{t}\\right)\\\\\r\u0026amp;= \\rho^{2}\\mathrm{Var}(Y_{t-1}) + \\sigma_{\\epsilon}^{2}\\\\\r\\sigma^{2} \u0026amp;= \\rho^{2}\\sigma^{2} + \\sigma_{\\epsilon}^{2}\\ \\text{(apply stationarity condition)}\\\\\r\\sigma^{2}\\left(1-\\rho^{2}\\right) \u0026amp;= \\sigma_{\\epsilon}^{2}\\\\\r\\sigma^{2} \u0026amp;= \\frac{\\sigma_{\\epsilon}^{2}}{1 - \\rho^{2}}\r\\end{aligned}\r\\]\nThe plot below shows several examples of draws from an AR(1) process with differing values of \\(\\rho\\) and \\(\\sigma^2_\\epsilon = 1\\):\r\rGaussian processes\rGaussian processes model a set of variables as being multivariate normally distributed with mean \\(\\boldsymbol{\\mu}\\) and variance/covariance matrix \\(\\boldsymbol{\\Sigma}\\):\n\\[\r\\mathbf{Y} \\sim MVN(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\r\\]\nUsually the mean vector is set to \\(\\boldsymbol{0}\\), which means the Gaussian process is fully defined by its choice of variance/covariance matrix \\(\\boldsymbol{\\Sigma}\\). The variance/covariance matrix is defined by a kernel function which defines the covariance between any two variables:\n\\[\r\\Sigma_{i,j} = K(i, j)\r\\]\n\rAn AR(1) process is a Gaussian process\rWe want to show that an AR process can be represented as a Gaussian process. To do this, we need to show that \\(\\mathbf{Y}\\) is jointly normally distributed with some mean vector and variance/covariance matrix.\nWe already know that \\(\\mathrm{E}(Y_t)=0\\), so the mean vector of its joint distribution will be \\(\\mathbf{0}\\).\nTo find the variance/covariance matrix, we need to derive the covariance between \\(Y_{t_1}\\) and \\(Y_{t_2}\\). First, let’s consider the simpler case of the covariance between \\(Y_t\\) and \\(Y_{t+1}\\):\n\\[\r\\begin{aligned}\r\\mathrm{cov}(Y_{t}, Y_{t+1}) \u0026amp;= \\mathrm{E} \\left[ \\left( Y_t - \\mathrm{E}[Y_t] \\right) \\left( Y_{t+1} - \\mathrm{E}[Y_{t+1}] \\right) \\right] \\text{ (definition of covariance) } \\\\\r\u0026amp;= \\mathrm{E} \\left[ Y_t Y_{t+1} \\right] \\text{ (because } \\mathrm{E}[Y_t] = \\mathrm{E}[Y_{t+1}] = 0 \\text{)} \\\\\r\u0026amp;= \\mathrm{E} \\left[ Y_t \\left( \\rho Y_{t} + \\epsilon_{t+1} \\right) \\right] \\\\\r\u0026amp;= \\mathrm{E} \\left[ \\rho Y_t^2 + Y_t \\epsilon_{t+1} \\right] \\\\\r\u0026amp;= \\rho \\mathrm{E}\\left[ Y_t^2 \\right] \\\\\r\u0026amp;= \\rho (\\mathrm{Var}(Y_t) + \\mathrm{E}[Y_t]^2) \\\\\r\u0026amp;= \\rho \\frac{\\sigma_\\epsilon^2}{1 - \\rho^2}\r\\end{aligned}\r\\]\nfor \\(Y\\)s separated by more than one time point, iterating the above result yields the expression\n\\[\r\\begin{aligned}\r\\mathrm{cov}(Y_{t_1}, Y_{t_2}) = \\rho^{\\vert t_1 - t_2 \\vert} \\frac{\\sigma_\\epsilon^2}{1 - \\rho^2}\r\\end{aligned}\r\\]\nNow we can fully define the joint distribution of \\(\\mathbf{Y}\\):\n\\[\r\\mathbf{Y} \\sim MVN(\\mathbf{0}, \\boldsymbol{\\Sigma})\r\\]\nwhere \\(\\Sigma_{i,j} = \\rho^{\\vert i - j \\vert} \\frac{\\sigma_\\epsilon^2}{1-\\rho^2}\\). This is a Gaussian process!\n\rCombining kernel functions\rThe nice thing about Gaussian processes is that we can combine multiple kernel functions to model processes with dependence from different sources. Two ways kernels can be combined are by multiplication and addition. Multiplying two kernels is like an “AND” operation: the correlation between points will be high if the correlation from both kernels is high. Adding two kernels together is like an “OR” operation: correlation is high if either kernel indicates high covariance.\nAs an example, let’s build a Gaussian process that combines an AR process (for temporal correlation) and a spatial process (for spatial correlation) by combining two kernel functions. First, we need to define an outcome variable \\(Y\\) that varies in time and space: let \\(Y_{c,t}\\) be a random variable indexed by spatial site \\(c\\) at timepoint \\(t\\). We take the AR covariance as the first kernel function, to model temporal correlation:\n\\[\rK_1(i, j) = \\rho^{\\vert t_i - t_j \\vert} \\frac{\\sigma_\\epsilon^2}{1 - \\rho^2}\r\\]\nand a squared-exponential kernel function to model spatial dependence:\n\\[\rK_2(i, j) = \\alpha^2 \\exp\\left( -\\frac{d(i, j)}{2\\lambda^2} \\right)\r\\]\nwhere \\(d(i, j)\\) is the spatial distance between sites \\(i\\) and \\(j\\), \\(\\lambda\\) is a length-scale parameter, and \\(\\alpha^2\\) is a parameter controlling the magnitude of the covariance.\nCombine the two kernel functions so that two data points are correlated if they are close together in time and space:\n\\[\r\\begin{aligned}\rK(i, j) \u0026amp;= K_1(i, j) \\times K_2(i, j) \\\\\r\u0026amp;= \\rho^{\\vert t_i - t_j \\vert} \\frac{\\sigma_\\epsilon^2}{1 - \\rho^2} \\alpha^2 \\exp\\left( -\\frac{d(i, j)}{2\\lambda^2} \\right)\r\\end{aligned}\r\\]\nNote the parameters \\(\\sigma^2_\\epsilon\\) and \\(\\alpha^2\\), which are multipled together, would be unidentifiable in parameter estimation and should be replaced by a single parameter that controls the magnitude of the covariance.\nTo illustrate this Gaussian process model, I started by generating a set of sites with random locations:\nthen I drew from the Gaussian process using the parameters temporal parameters \\(\\rho=0.9\\), \\(\\sigma_\\epsilon^2=1\\) and spatial parameters \\(\\alpha = 1\\) and \\(\\lambda=2\\).\nThe plot below shows the time trend in the first six sites:\nAnd the spatial distribution over time of \\(Y_{c,t}\\) is shown below:\rVisually we can see that the Gaussian process generates data that is correlated in both time and space.\n\rModeling using the mean and the covariance\rThe spatio-temporal Gaussian process we defined in the previous section does its modeling through the variance/covariance matrix, with its mean function set to zero. An alternative way to think about a spatio-temporal process is akin to the first AR representation we looked at, and define \\(\\mathbf{Y}_t\\) (the set of all \\(Y_{c,t}\\) at time \\(t\\)) relative to \\(\\mathbf{Y}_{t-1}\\):\n\\[\r\\begin{aligned}\r\\mathbf{Y}_{t} = \\rho \\mathbf{Y}_{t-1} + \\boldsymbol{\\epsilon}_t\r\\end{aligned}\r\\]\nwhere \\(\\boldsymbol{\\epsilon_t} \\sim MVN(\\mathbf{0}, \\boldsymbol{\\Sigma}_\\epsilon)\\).\nIf we set \\(\\boldsymbol{\\Sigma_\\epsilon}\\) to be the diagonal matrix \\(\\boldsymbol{\\Sigma}_\\epsilon = \\sigma^2_\\epsilon \\mathbf{I}_n\\) then we will have an independent AR(1) independent process for each spatial site. It gets more interesting if we define \\(\\boldsymbol{\\Sigma}_\\epsilon\\) by a covariance function so we can include dependence between sites, for example dependence based on the distance between the sites. For now, let’s use the squared exponential kernel and define \\(\\Sigma_{i,j} = \\alpha^2 \\exp\\left(-\\frac{d(i, j)}{2\\lambda^2} \\right)\\).\nIs this process also equivalent to a mean zero Gaussian process with some covariance kernel? We’ll answer this question by deriving the covariance between any two points.\nThe mean of \\(\\mathbf{Y_t}\\) can be shown to be zero in the same way we showed a univariate AR process has mean 0. We also need to know the overall variance/covariance matrix of \\(\\mathbf{Y}_t\\), which we’ll call \\(\\boldsymbol{\\Phi}\\); the logic is imilar to the univariate case, and I’ll show it here for completeness:\n\\[\r\\begin{aligned}\r\\mathrm{Var}\\left(\\boldsymbol{Y}_{t}\\right) \u0026amp; =\\mathrm{Var}\\left(\\rho^{2}\\mathbf{Y}_{t-1} + \\boldsymbol{\\epsilon}_{t}\\right) \\\\\r\u0026amp;= \\rho^{2}\\mathrm{Var}\\left(\\boldsymbol{Y}_{t-1}\\right)+\\mathrm{Var}\\left(\\boldsymbol{\\epsilon}_{t}\\right) \\\\\r\\boldsymbol{\\Phi} \u0026amp; =\\rho^{2}\\boldsymbol{\\Phi}+\\boldsymbol{\\Sigma}_\\epsilon \\\\\r\\boldsymbol{\\Phi}-\\rho^{2}\\boldsymbol{\\Sigma} \u0026amp;= \\boldsymbol{\\Sigma}_\\epsilon \\\\\r\\boldsymbol{\\Phi}\\left(\\mathbf{I}-\\rho^{2}\\mathbf{I}\\right) \u0026amp; =\\boldsymbol{\\Sigma}_\\epsilon \\\\\r\\boldsymbol{\\Phi} \u0026amp;=\\boldsymbol{\\Sigma}_{\\epsilon}\\left(\\mathbf{I}-\\rho^{2}\\mathbf{I}\\right)^{-1}\r\\end{aligned}\r\\]\nIf we pull out two sites at the same time point, their covariance is \\(\\mathrm{cov}(Y_{t,c_1}, Y_{t,c_2}) = \\frac{\\Sigma_{\\epsilon, c_1, c_2}}{1-\\rho^2}\\), which looks very similar to the unidimensional AR(1) process variance.\nNow we derive the covariance between any two sites that are one time point apart:\n\\[\r\\begin{aligned}\r\\mathrm{cov}\\left(y_{c_1,t},y_{c_2,t+1}\\right) \u0026amp; =\\mathrm{E}\\left[\\left(y_{c_1,t}-\\mathrm{E}\\left[y_{c_1,t}\\right]\\right)\\left(y_{c_2,t}-\\mathrm{E}\\left[y_{c_2,t}\\right]\\right)\\right]\\\\\r\u0026amp; =\\mathrm{E}\\left[y_{c_1,t}y_{c_2,t}\\right]\\\\\r\u0026amp; =\\mathrm{E}\\left[y_{c_1,t}\\left[\\rho y_{c_2,t}+\\epsilon_{c_2,t+1}\\right]\\right]\\\\\r\u0026amp; =\\rho\\mathrm{E}\\left[y_{c_1,t}y_{c_2,t}\\right]\\\\\r\u0026amp; =\\rho\\mathrm{cov}\\left(y_{c_1,t}y_{c_2,t}\\right)\\\\\r\u0026amp; =\\rho\\frac{\\Sigma_{i,j}}{1-\\rho^2} \\\\\r\u0026amp;= \\rho \\frac{1}{1-\\rho^2} \\Sigma_{i,j} \\\\\r\u0026amp;= \\rho \\frac{1}{1-\\rho^2} \\alpha^2 \\exp\\left(-\\frac{d(i, j)}{2\\lambda^2} \\right)\r\\end{aligned}\r\\]\nfor sites more than one time point away from each other, we can iterate the above result to get a general expression of the covariance between any two points:\n\\[\r\\begin{aligned}\r\\mathrm{cov}\\left(y_{c_1,t_1},y_{c_2,t_2}\\right) \u0026amp;= \\rho^{\\vert t_1 - t_2 \\vert}\\frac{1}{1-\\rho^2} \\alpha^2 \\exp\\left(-\\frac{d(i, j)}{2\\lambda^2} \\right)\r\\end{aligned}\r\\]\nif we reparameterize \\(\\alpha\\) to be the product of two parameters \\(\\alpha = \\sigma^2_\\epsilon \\alpha\\), we get\n\\[\r\\begin{aligned}\r\\mathrm{cov}\\left(y_{c_1,t_1},y_{c_2,t_2}\\right) \u0026amp;= \\rho^{\\vert t_1 - t_2 \\vert}\\frac{\\sigma^2_\\epsilon}{1-\\rho^2} \\alpha^2 \\exp\\left(-\\frac{d(i, j)}{2\\lambda^2} \\right) \\\\\r\u0026amp;= K_1(i, j) \\times K_2(i,j)\r\\end{aligned}\r\\]\nwhich is the product of an AR(1) and squared exponential kernel function as defined in the previous section. In practice we wouldn’t want to separate these parameters because both of them will not be identifiable given observed data, but I separated them here to show how the covariance structure is the product of two kernel functions.\nTherefore, we can write this process in the form of a Gaussian process with mean zero and covariance kernel given by the product of a temporal and spatial kernel:\n\\[\r\\begin{aligned}\r\\mathbf{Y} \\sim\u0026amp; MVN(\\mathbf{0}, \\boldsymbol{\\Sigma}) \\\\\r\\Sigma_{i,j} =\u0026amp; K_1(i, j) \\times K_2(i, j) \\end{aligned}\r\\]\nThe spatio-temporal processes defined as a set of conditional distributions and as a Gaussian process are equivalent.\nTo summarize, AR processes can be written as a Gaussian process model, which is useful because a temporal process can then be easily combined with other sources of dependence. In general, we can build our models by defining conditional distributions with a given mean and covariance, or a joint distribution with mean zero where the model is fully defined by a variance/covariance kernel function. In a future post I will look at Bayesian parameter estimation in these models using Stan.\n\r","date":1565308800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565308800,"objectID":"2f0aa3449bee7bdf0f02ec3c442c0fcf","permalink":"/2019/08/09/autoregressive-processes-are-gaussian-processes/","publishdate":"2019-08-09T00:00:00Z","relpermalink":"/2019/08/09/autoregressive-processes-are-gaussian-processes/","section":"post","summary":"Autoregressive (AR) processes are a popular choice for modeling time-varying processes. AR processes are typically written down as a set of conditional distributions, but if we do some algebra we can show how they can also be written as a Gaussian process. One reason having a Guassian process representation is useful is because it makes it more clear how an AR process can be incorporated into larger models, like a spatio-temporal model.","tags":null,"title":"Autoregressive Processes are Gaussian Processes","type":"post"},{"authors":null,"categories":null,"content":"\rMany statistical routines in R use R formulas as a flexible way to specify the terms of a model. With a little setup, we can use formulas to build inputs to Stan and avoid hard-coding any variables in the model.\nFor example, say you are writing a Stan model for linear regression. You would like to regress a response variable \\(y\\) on two predictors, \\(x_1\\) and \\(y_1\\):\n// linear_regression.stan\rdata {\rint N; // Number of observations\rvector[N] y;\rvector[N] x1;\rvector[N] x2;\r} parameters {\rreal beta_0;\rreal beta_1;\rreal beta_2;\rreal\u0026lt;lower=0\u0026gt; sigma;\r} model {\ry ~ normal(beta_0 + beta_1 * x1 + beta_2 * x2, sigma);\r}\rBut what if you later decide to add more predictors? We can make the above model more flexible by allowing a matrix of predictors \\(X\\) of arbitrary size:\n// linear_regression.stan\rdata {\rint N; // Number of observations\rint K; // Number of predictors\rvector[N] y;\rmatrix[N, K] X;\r} parameters {\rvector[K] beta;\rreal\u0026lt;lower=0\u0026gt; sigma;\r} model {\ry ~ normal(beta * X, sigma);\r}\rThen we can use R formulas to build the predictor matrix \\(X\\) and pass it to Stan:\nfit_linear_regression \u0026lt;- function(formula, data, ...) {\rmodel \u0026lt;- stan_model(\u0026quot;./linear_regression.stan\u0026quot;)\rX \u0026lt;- model.matrix(formula, data)\ry \u0026lt;- model.extract(model.frame(formula, data), \u0026quot;response\u0026quot;)\rdata \u0026lt;- list(\rN = nrow(X),\rK = ncol(X),\rX = X,\ry = y\r)\rsampling(model, data, ...)\r}\rNow it’s easy to fit the model with different predictors:\nN \u0026lt;- 100\rsimulated_data \u0026lt;- tibble::tibble(\rx1 = rnorm(N, 0, 1),\rx2 = rnorm(N, 0, 1),\ry = x1 + 2*x2 + rnorm(N, 0, 0.1)\r)\rfit_linear_model(y ~ x1, simulated_data)\rfit_linear_model(y ~ x1 + x2, simulated_data)\rWriting a separate function for preparing the data for Stan based on a formula makes the model more usable and flexible.\n","date":1563818400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563818400,"objectID":"a7d78d13e918aa8adf2d4672e7b325dd","permalink":"/2019/07/22/r-formulas-stan/","publishdate":"2019-07-22T18:00:00Z","relpermalink":"/2019/07/22/r-formulas-stan/","section":"post","summary":"Write flexible Stan models by using the R formula interface.","tags":null,"title":"Using R formulas to pass data to Stan","type":"post"},{"authors":null,"categories":null,"content":"One of the themes that I worked on at Silent Spring Institute was on how to report complex personal data to our study participants. In the PROTECT study, mothers in Puerto Rico were tested for a host of environmental chemicals. Our job was to design a tool to report-back individual results to the participants.\nWhile I was on the project, I designed and implemented a novel smartphone interface for communicating personal results to study participants. One aspect was designing a visualization that allowed participants to compare their results to other women in the study. Our approach uses a SinaPlot where the participant\u0026rsquo;s personal results are represented by an avatar that they chose when they enter their report.\nLast May I left Silent Spring Institute to pursue graduate school, and I was happy to see that the tool was launched in October! To learn more, check out the article the PROTECT team wrote about the launch of the reports.\n","date":1554746400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554746400,"objectID":"f32df025b8f59e4a6a1c91aa0b45ef06","permalink":"/2019/04/08/smartphone-interface-for-reporting-research-results-to-study-participants/","publishdate":"2019-04-08T18:00:00Z","relpermalink":"/2019/04/08/smartphone-interface-for-reporting-research-results-to-study-participants/","section":"post","summary":"We developed a novel interface for reporting results to participants of exposure studies.","tags":null,"title":"Smartphone interface for reporting research results to study participants","type":"post"},{"authors":[],"categories":[],"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"Nick Kristof, columnist for the New York Times, recently wrote about Detox Me Action Kit, a study I manage and helped launch at Silent Spring Institute.\nThe column includes a nice graphic summarizing his results:\n Source: The New York Times\nYou can check out the study on the Silent Spring Institute website: Detox Me Action Kit.\n","date":1519581600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519581600,"objectID":"f90a1553478abd13ccd9727aaf42dd87","permalink":"/2018/02/25/what-poisons-are-in-your-body-nick-kristof/","publishdate":"2018-02-25T18:00:00Z","relpermalink":"/2018/02/25/what-poisons-are-in-your-body-nick-kristof/","section":"post","summary":"New York Times columnist Nick Kristof covers the Detox Me Action Kit project.","tags":null,"title":"What Poisons Are in Your Body? - Nick Kristof","type":"post"},{"authors":null,"categories":null,"content":"More from Twitter, to complete a very silly triptych:\n.@common_squirrel spends most of its time running and blinking. One time, it hoped. pic.twitter.com/gO6y3bmSp3\n\u0026mdash; Herb Susmann (@herbps10) February 10, 2018  The answer to @isacatinthesink is more often \u0026quot;no\u0026quot; than \u0026quot;yes\u0026quot;. I\u0026#39;m surprised. Cats love sinks. pic.twitter.com/si1q7VoW1N\n\u0026mdash; Herb Susmann (@herbps10) February 9, 2018 Code for the @common_squirrel plot and the @isacatinthesink plot are available as Github Gists.\n","date":1519560000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519560000,"objectID":"0695d523858037052a81c737b23df1e5","permalink":"/2018/02/25/more-animals-from-twitter/","publishdate":"2018-02-25T12:00:00Z","relpermalink":"/2018/02/25/more-animals-from-twitter/","section":"post","summary":"Analyzing tweets from @common_squirrel and @isacatinthesink.","tags":null,"title":"More animals from Twitter","type":"post"},{"authors":null,"categories":null,"content":"\rFrom Twitter:\nThe most common rating for dogs from @dog_rates is 13/10. A few are 15/10 but all dogs deserve that rating in my opinion. Pictured with Ellie (12/10) from @KatieNicoleF. pic.twitter.com/B46rRDSCRY\r— Herb Susmann (@herbps10) February 3, 2018\r\r\rHere’s the R code I used to generate the histogram:\nlibrary(rtweet)\rlibrary(tidyverse)\rlibrary(stringr)\rlibrary(cowplot)\rlibrary(grid)\rlibrary(jpeg)\rg \u0026lt;- rasterGrob(readJPEG(\u0026quot;ellie.jpg\u0026quot;), interpolate = TRUE)\rtmls \u0026lt;- get_timelines(\u0026quot;dog_rates\u0026quot;, n = 3200)\rratings \u0026lt;- tmls %\u0026gt;%\rfilter(str_detect(text, \u0026quot;t.co\u0026quot;)) %\u0026gt;%\rfilter(!str_detect(text, \u0026quot;^RT\u0026quot;)) %\u0026gt;%\rfilter(!str_detect(text, \u0026quot;Here\u0026#39;s a little more info on Dew\u0026quot;)) %\u0026gt;%\rmutate(rating = map(text, str_extract_all, \u0026quot;1[0-5]/10\u0026quot;),\rrating = map(rating, `[[`, 1)) %\u0026gt;%\runnest(rating) %\u0026gt;%\rcount(rating)\rggplot(ratings, aes(x = rating, y = n)) +\rannotation_custom(g) +\rgeom_col(fill = \u0026quot;white\u0026quot;, alpha = 0.8) +\rlabs(caption = \u0026quot;Data: @dog_rates, photo: @KatieNicoleF\u0026quot;,\rtitle = \u0026quot;577 WeRateDogs™ Ratings\u0026quot;)\rThe R code is also available as a gist.\n","date":1517745600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517745600,"objectID":"6c0b2c6f206715b3dcd042074b70d4db","permalink":"/2018/02/04/theyre-all-good-dogs/","publishdate":"2018-02-04T12:00:00Z","relpermalink":"/2018/02/04/theyre-all-good-dogs/","section":"post","summary":"Analyzing tweets from @dog_rates.","tags":null,"title":"They're all good dogs","type":"post"},{"authors":null,"categories":null,"content":"There have been a few articles in the last couple years that have used traveling salesman problem solvers to find the fastest way to see all the national parks or 72 breweries in the US. I\u0026rsquo;m going to join the trend on a smaller geographic scale by plotting the fastest way to see 18 breweries (including one cider house) in the Boston area.\nThe fastest route takes about 2.5 hours of driving (by your designated driver or using a ride service, of course) from start to finish:\n  If you take 15 minutes at each brewery to have a beer, and it takes 2.5 hours to drive to each one, you could do it all in only 7 hours. Not bad, that\u0026rsquo;s less than a full day at work!\nYou won\u0026rsquo;t be able to drive this yourself if you have a beer at every stop, so you\u0026rsquo;ll either need to find a friend to drive you around for seven hours, or take something like a Lyft. I used the Lyft API to estimate how much it would cost, and it comes in at about $176. If you can split this with three friends, and you pay, say, $8 per drink, your total cost would be $188.\nAm I going to do this? Probably not. I really don\u0026rsquo;t think I could stomach 17 beers and a cider in one day. And think of all the fun I could have programming in R for seven hours, instead. Yeah. Easy choice.\nHere\u0026rsquo;s a list of all the breweries, in order. You could start your tour anywhere, but if I were doing this I\u0026rsquo;d start at Aeronaut, my favorite brewery around here. You\u0026rsquo;d also get to end at Bantam Cider Company to cap off very long night out on a different note.\n Aeuronaut Brewery Winter Hill Brewing Company Idle Hands Craft Ales Night Shift Brewing Bone Up Brewing Company Mystic Brewery Downeast Cider House Boston Beer Works Trillium Brewing Company Harpoon Brewery Dorchester Brewing Company Sam Adams Brewery Turtle Swamp Brewing John Harvards Brewery Lamplighter Brewing Company Cambridge Brewing Company Somerville Brewing Company Bantam Cider Company  If you want to want do this, here\u0026rsquo;s a Google Map with all the breweries entered in order. Good luck. Please drink responsibly.\nP.S. Let me know if I missed any breweries in the area!\nDo it yourself: All the code for this project is on Github. I used the Google Maps API for calculating a distance matrix between all the breweries and to get detailed directions between each point on the final tour. The optimal tour was calculated using the asymmetric traveling salesman problem solver from the TSP R package. The Lyft price estimate came from it\u0026rsquo;s public API. If you want to run the code yourself, you\u0026rsquo;ll need to get a Google Maps API key and a Lyft API key.\n","date":1507550400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1507550400,"objectID":"140abb7283ecb524b96e5c8e1f18f80b","permalink":"/2017/10/09/fastest-way-to-see-17-boston-breweries-and-one-cider-house/","publishdate":"2017-10-09T12:00:00Z","relpermalink":"/2017/10/09/fastest-way-to-see-17-boston-breweries-and-one-cider-house/","section":"post","summary":"Calculating an optimal route between all the Boston area breweries using a Traveling Salesman Problem solver.","tags":null,"title":"Fastest way to see 17 Boston breweries (and one cider house)","type":"post"},{"authors":null,"categories":null,"content":"\rGoal: Predict the winner of Jeopardy. As the game progresses, update the predictions to take into account the current score profile.\nBackground: A quick search revealed work by The Jeopardy Fan on building a model to predict the Tournament of Champions contestants. He used player’s Coryat scores to predict the length of their winning streak, and whether they would qualify for the tournament. I’m going for something slightly different than him by focusing on predicting the winner of a single game. I’m also going to use the real game score, instead of Coryat scores.\nData: The J-Archive is an incredible resource for Jeopardy! data, thanks to the work of their archivists. They have every game ever played, every question asked, along with which contestants answered and whether they were correct or not. I downloaded data from seasons 22-33 for fitting and testing the models.\nIntuition: Let’s explore the dataset a little bit to get a sense of how we might build a classification model to predict winners. You can easily make plots that show how a contestant’s score changes over the course of a game, like this one that shows Roger Craig setting a one-day earnings record:\rExpanding this type of plot beyond a single game, here is a plot showing all of the games from season 27, with each trajectory colored by whether the contestant won the game:\nThe winners tend to have higher scores throughout the game, but you can still see a few people who had high scores and still lost. If we show more seasons at once we can see more of the overall trend, but we lose the ability to see individual trajectories clearly:\nAnother way to look at this is by plotting the median scores, with a ribbon showing the 5% and 95% quantiles:\nIt looks like there is some separation between the winners and losers just in terms of their score, and the separation becomes more pronounced as the game progresses, which a model should be able to pick up on and use for prediction.\nWe should temper our expectations, though. Especially in the first graph, you can see how much of a randomizer the Final Jeopardy round is. Here’s a table showing how the contestant’s rank going in to Final Jeopardy corresponds to winning or losing (data from seasons 22-33):\n\r\rRank\rWon\rLost\r\r\r\rThird\r171 (6%)\r2554 (94%)\r\rTied for second\r8 (12%)\r60 (88%)\r\rSecond\r591 (22%)\r2116 (75%)\r\rTied for first\r17 (47%)\r19 (53%)\r\rFirst\r1991 (73%)\r750 (27%)\r\r\r\r27% of contestants in first place going in to Final Jeopardy still lose. It’s going to be very difficult to accurately predict when an upset will happen, so this gives us a sense of the limits of any prediction model.\nModeling and Results: One of the goals is to predict the winner as the game progresses. In each game of Jeopardy! there are up to 61 questions: 30 in Single Jeopardy, 30 in Double Jeopardy, and 1 in Final Jeopardy. I decided to fit a logistic regression model after every question, so we can see how the classification accuracy improves as the game gets closer to the end.\nI used data from seasons 22-32 for training, and held out season 33 for testing.\nThere are 61 questions in each game; call them \\(q_{1},q_{2},...,q_{i},...,q_{61}\\), where \\(q_{1}\\) is the first question asked in the game, \\(q_{2}\\) is the second, and so on. The actual point value of each question might be different, depending on the order they are chosen in the game (for example, \\(q_{1}\\) might be a $200 question in one game, and a $1,000 question in another.) I fitted 61 logistic regression models \\({M_{1}, M_{2}, ...,M_{n}}\\) that predict whether the player won the game based on their score at the end of question \\(i\\). We would expect model \\(M_{1}\\) to do poorly, because the first question isn’t very informative of who is going to end up winning; and the accuracy to improve as the game progresses.\nFor example, here is the fitted logistic model for question 30:\nWe can visualize the accuracy of the all models at once by plotting their ROC curves.\nAs we would expect, the model has lousy accuracy at the beginning of the game, but improves steadily as the game progresses. However, it is not perfect even after the game is over. This is because the score itself doesn’t determine the winner; what matters is who has the highest score.\nTo address this, I added two new features to bring in information about the contestants compare to each other within the game:\n\rrank - categorical variable indicating the contestant’s current rank (first place, tied for first, second place, etc.)\rdistance from lead - the difference in points between the contestant and the leader. If the contestant is in the lead, it is a negative number indicating how far they are ahead.\r\rI built two new sets of models using these features. I also added a very simple model for comparison: predict the winner of the game to be whoever is in the lead. This model doesn’t output probabilities, so it will show up on the ROC curves as a single point (I call this model “current leader” in the legend.) Here’s how they compare to the original model:\nThe new models aren’t that much better than the original model. The biggest difference I see is that they have perfect accuracy at the end of the game, as you would expect since they have access to the final ranking of the contestants.\nThe “current leader” reference model falls right on the curves, indicating the logistic models don’t do better than it. They may be more useful, though, since they output probabilities rather than a dichotomous prediction.\nVisualizing predictions: Now that we have these models, let’s see a contestant’s probability of winning (conditioned on the model) evolves over the course of a game. I’m going to take the set of models that use the distance from lead predictor and apply them to a game from season 33.\nGavin takes the lead in the prediction model at the same time he takes the lead, in the middle of Double Jeopardy.\nHere’s another one from season 33 where the prediction flips towards the end of the game:\nThe highest probability of winning is assigned to whoever is in the lead, which reflects the logic of the simple reference model.\nNext steps:\rI think a significant shortcoming of the approach I took here is that I fit completely separate models for each question; they aren’t connected in any way, and so they don’t have any “memory” of how each contestant has performed previously in the game (except via their current score), or in prior games. Perhaps there’s a way to incorporate some ideas from partial pooling models to share strength between questions. Or maybe a model could be built that estimates a latent “ability” score for each participant, conditioned on their previous performance. A bonus of this would be that the winners ability score could be fed in to their next game, as a form of prior information about how well the contestant will do. Doing this in a Bayesian framework seems like a good choice.\nI think there are also a few things that could be done to improve performance in the endgame. Right now the models don’t take into account how much money is still available. Incorporating this should help, especially in the end game when there isn’t very much money still on the board. It could also be used to find runaway scores, where one contestant has more money than is possible for a competitor to gain. We could also build a model for predicting the Final Jeopardy bets, so the end game model could have a better understanding of how likely an upset will be.\nFinally, I’d also be interested in changing the goal slightly to predict winners in terms of Coryat score, which I’m sure would perform better since the uncertainty of daily doubles and the Final Jeopardy wager would be removed.\nSource code:\rThe source code for this analysis is on Github: https://github.com/herbps10/jeopardy\n","date":1507377600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1507377600,"objectID":"f50edd4e15c447b027c84186782fa01b","permalink":"/2017/10/07/predicting-jeopardy-winners/","publishdate":"2017-10-07T12:00:00Z","relpermalink":"/2017/10/07/predicting-jeopardy-winners/","section":"post","summary":"Using logistic regression to predict the winner of Jeopardy!","tags":null,"title":"Predicting Jeopardy! Winners","type":"post"},{"authors":null,"categories":null,"content":"\r.banana {\rtext-align: center;\rmargin-bottom: 20px;\r}\r.banana span {\rdisplay: inline-block;\rbackground: #e3dbcf;\rcolor: black;\rborder-radius: 5px;\rtext-align: center;\rbox-shadow: inset 10px 5px 5px 0px rgba(255, 255, 255, 0.1),\rinset -3px -3px 7px 0px rgba(0, 0, 0, 0.2),\rinset 0px 0px 5px 7px rgba(255, 255, 255, 0.2),\r0px 1px 2px 0px rgba(0, 0, 0, 0.3);\r}\rdiv.banana span {\rwidth: 50px;\rheight: 50px;\rfont-size: 18pt;\rline-height: 50px;\rfont-weight: bold;\r}\rspan.banana span {\rwidth: 30px;\rheight: 30px;\r}\r\rThere was a fabled game of Bananagrams in which my Dad drew his initial 11 tiles, and immediately spelled:\nRASTAFARIAN\r\rIf you haven’t played the game, it’s like a free-form version of Scrabble. You start by drawing a number of tiles (typically 11 or 21), and try to form a word or words out of them.\nThe story made me wonder how likely it is to spell an 11 letter word on the first draw.\nThe first step is to calculate the probability of drawing a particular word. Consider a bananagrams bag filled with only two letters, S for success and F for failure. Start pulling out tiles from the bag at random, without replacing each tile back in the bag after drawing it, and count how many S tiles you get. The hypergeometric distribution models the probability that you will get a certain number of S tiles for a given number of draws. The multivariate hypergeometric distribution extends this to the multivariate case; that is, it models the probability you’ll draw a certain number of As, Bs, Cs, etc. after drawing a number of tiles from the bag.\nFortunately, the R package extraDistr provides an R version of the multivariate hypergeometric probability mass function. Here’s a function that, given a word of length \\(N\\) and the number of each letter tile in a bag, gives the probability of drawing that word in \\(N\\) draws:\nUsing this function, we can find the probability of drawingRASTAFARIAN:\nAnd the result is \\(4.28\\times10^{-6}\\%\\). Pretty lucky!\nNow, what is the probability of drawing any valid 11 letter word to start the game? Note that in most cases, spelling a word using all your 11 tiles excludes the possibility of spelling another word. This suggests the the probability of spelling word \\(A\\) OR word \\(B\\) is given by \\(P(A \\cap B)=P(A) + P(B)\\).\nHowever, there is a special case: what if word \\(A\\) and word \\(B\\) are spelled with the same letters? In order to avoid double counting, we need to only want to include words with the same letters once.\nI downloaded a list of words in the SOWPODS scrabble dictionary from a GitHub repository and loaded them into R. To deduplicate words with the same letters, I sorted the letters in each word and removed duplicates:\nI then used the word_probability function to calculate the probability of drawing each 11 letter word, and then summed them all up:\nWhich computes the probability of drawing a valid 11 letter word in the opening tiles to be \\(~0.28\\%\\).\nNow, suppose you start the game by drawing a different number of tiles. We can compute the probability of starting with a valid word for a range of starting tile numbers:\nDrawing 3 letters has the highest probability of forming a word, at \\(53.7\\%\\). This validates my strategy of dumping early in the game to get new tiles when I get stuck, because the new letters often help me get out of the rut.\n","date":1501467194,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501467194,"objectID":"46ca0160a6e13de27c05de2908690562","permalink":"/2017/07/30/bananagrams-probabilities/","publishdate":"2017-07-30T21:13:14-05:00","relpermalink":"/2017/07/30/bananagrams-probabilities/","section":"post","summary":"Calculating the probability of starting with a complete word in Bananagrams.","tags":null,"title":"Bananagrams Probabilities","type":"post"},{"authors":["Katerine E. Boronow","Herb Susmann","Krzysztof Z. Gajos","Ruthann A. Rudel","Kenneth C. Arnold","Phil Brown","Rachel Morello-Frosch","Laurie Havas","Julia Green Brody"],"categories":null,"content":"","date":1485907200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1485907200,"objectID":"0e98638b9f7d4ed905505121c8db40cd","permalink":"/publication/derbi/","publishdate":"2017-02-01T00:00:00Z","relpermalink":"/publication/derbi/","section":"publication","summary":"DERBI is a digital tool for reporting back personal results to participants in exposure studies.","tags":["DERBI, Silent Spring Institute"],"title":"DERBI: A Digital Method to Help Researchers Offer “Right-to-Know” Personal Exposure Results","type":"publication"},{"authors":null,"categories":null,"content":"There is a website with scripts for every episode of Star Trek, so for fun I downloaded them and generated a visualization of which characters were in each episode of Star Trek.\nEnjoy!\n","date":1483012800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483012800,"objectID":"ada8e67090109d7700e79ec8bc7874e9","permalink":"/2016/12/29/who-was-in-each-episode-of-star-trek/","publishdate":"2016-12-29T12:00:00Z","relpermalink":"/2016/12/29/who-was-in-each-episode-of-star-trek/","section":"post","summary":"A visualization of which characters were in each episode of Star Trek.","tags":null,"title":"Who was in each episode of Star Trek?","type":"post"},{"authors":null,"categories":null,"content":"I had fun leading a workshop a few weeks ago on building a crystal radio. We used a simple design that incorporates a loop antenna (doubling as an inductor), a variable capacitor, a germanium diode, and an earpiece.\nTwo pieces of wood make a frame for the antenna. We nailed in picture hanger hooks to the ends to wind wire around to form an antenna. Then we used a few nails to hold the variable capacitor in place, and soldered all the components together.\nThe walls of the library we were working in dampened outside radio waves a lot, but once we stepped outside we could all hear some nice strong AM stations.\n","date":1480334400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480334400,"objectID":"0bea80aabc6b039962e9f4484f93102e","permalink":"/2016/11/28/build-a-crystal-radio/","publishdate":"2016-11-28T12:00:00Z","relpermalink":"/2016/11/28/build-a-crystal-radio/","section":"post","summary":"We built a crystal radio sets at a workshop for teens.","tags":null,"title":"Build a Crystal Radio","type":"post"},{"authors":null,"categories":null,"content":"Last year I read The Cigarette Century by Allan M. Brandt, a history of the tobacco industry and a finalist for a Pulitzer Prize. One of the points that has stuck with me is how regulations that were intended to curtail the tobacco industry ended up benefiting them.\nIn what appeard to be a blow to the tobacco industry, the advertising of tobacco products on the radio and television was banned by the FCC in 1969. In 1967, a lawyer named John F. Banzhaf III successfully petitioned the FCC under the fairness doctrine to force radio and television to play anti-tobacco public service announcements if they played tobacco company ads. When the tobacco ads went off the air, so did the public service announcements. This was to the benefit of the tobacco industry who desperately wanted to suppress anti-tobacco information. Furthermore, the ban on advertising saved money being spent on expensive ad campaigns (the industry spent $230 million on television advertising in 1970 alone.)\nIn 1972, the FTC was finally able to require warning labels on tobacco products. Again, this perceived concession to the tobacco industry provided them with alternative benefits. Now the tobacco companies could argue that smokers were clearly warned of the health risks of smoking, and that they made an informed decision to start smoking despite the risks. Because these risks were accepted, the companies argued, they should not be liable for resulting health issues.\nThe tobacco companies negotiated regulations that, while appearing to be concessions, offered them benefits. That they were successful shows their savviness and the difficulty regulators have in reigning in companies that are determined to preserve their business model, and the profits that go along with it.\n","date":1477051200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1477051200,"objectID":"c436e17b258a932c6627b6068f2f9eea","permalink":"/2016/10/21/tobacco-industry-concessions-that-werent-concessions/","publishdate":"2016-10-21T12:00:00Z","relpermalink":"/2016/10/21/tobacco-industry-concessions-that-werent-concessions/","section":"post","summary":"Regulations had positive side effects for the tobacco industry","tags":null,"title":"Tobacco Industry Concessions that Weren't Concessions","type":"post"},{"authors":null,"categories":null,"content":"Long streaks are rare in Jeopardy. Most winners only win one game, and slightly less than 40% win two games in a row. In fact, only 6 contestants have won more than ten games in a row.\nThis Kaplan-Meier survival plot visualizes the survival function of Jeopardy winners:\nThe data includes seasons 1-33 (aired 1984-2016), and does not include any championship or tournament games. The point on the extreme right is due, of course, to Ken Jennings and his 74 game winning streak.\nHere\u0026rsquo;s the R code for the figure.\n","date":1466337600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1466337600,"objectID":"2bbd44e5370cd39be15de25629201f2e","permalink":"/2016/06/19/jeopardy-survival-analysis/","publishdate":"2016-06-19T12:00:00Z","relpermalink":"/2016/06/19/jeopardy-survival-analysis/","section":"post","summary":"A Kaplan-Meier survival plot of Jeopardy winning streaks.","tags":null,"title":"Jeopardy! Survival Analysis","type":"post"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"7f2fdd86e7162859c3b70f337d2940b3","permalink":"/project/rnhanes/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/rnhanes/","section":"project","summary":"R package for accessing and analyzing CDC NHANES data","tags":["R, 📦"],"title":"RNHANES","type":"project"},{"authors":null,"categories":null,"content":"I was fortunate enough to be able to present at UP-Stat 2014 on some of the things I\u0026rsquo;ve learned about writing performant R code while I was working on speeding up an R package for fitting mixed effects nested models. The talk seemed to be a big hit - I was awarded \u0026ldquo;Best Student Presentation\u0026rdquo;!\n ","date":1398132794,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1398132794,"objectID":"d0d147501a0cd29995e4532d98d2238e","permalink":"/2014/04/21/performant-r/","publishdate":"2014-04-21T21:13:14-05:00","relpermalink":"/2014/04/21/performant-r/","section":"post","summary":"Presentation on writing performan R packages that won a best student presentation award.","tags":["r"],"title":"Performant R","type":"post"},{"authors":null,"categories":null,"content":"Last semester I took an introductory course in raytracing.\nWe practiced an iterative development cycle in which we built up more and more complex ray tracers over the course of the semester. The very first ray tracer was pretty simple: it had to be able to intersect rays with simple geometric objects and display the results, but there didn’t have to be any lighting calculations or anything yet.\nOnce I figured out the assignment in OCaml, I decided to give it a shot entirely in Bash!\n(Well, not ENTIRELY in Bash. I will admit I shelled out to bc for floating point operations. A friend pointed out you could do floating point in Bash by having seperate variables for the integer and decimal components, but that’ll have to wait for version two)\nIt prints out the raytraced image directly to the console using special unicode characters and coloring through escape codes. Here’s what the result looks like:\nNot exactly pretty, but it works! (if only there was a way to change the line spacing in gnome-terminal).\nThe script is available as a gist: raytracer.sh\n","date":1396663994,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1396663994,"objectID":"11d6ff2660d89556feb54418b2f23957","permalink":"/2014/04/04/raytracing-in-bash/","publishdate":"2014-04-04T21:13:14-05:00","relpermalink":"/2014/04/04/raytracing-in-bash/","section":"post","summary":"It turns out it is possible to write a minimal raytracer in Bash.","tags":["bash","raytracing"],"title":"Raytracing in Bash","type":"post"},{"authors":null,"categories":null,"content":"\rAutoregressive (AR) processes are a popular choice for modeling time-varying processes. AR processes are typically written down as a set of conditional distributions, but if we do some algebra we can show how they can also be written as a Gaussian process. Having a Guassian process representation is useful because it is more clear how the AR process could be incorporated into larger models, like a spatio-temporal model. In this post, we’ll start with defining an AR process and deriving its mean and variance, then we’ll derive its joint distribution, which is a Gaussian process.\nAR processes\rLet \\(\\mathbf{Y} = \\left\\\\{ Y_1, Y_2, \\dots, Y_n \\right\\\\}\\) be a set of random variables indexed by time. An aurogressive model assumes that \\(\\mathbf{Y}\\) is correlated over time. An AR model is typically described by defining \\(Y_t\\) in terms of \\(Y_{t-1}\\):\n\\[\rY_{t} = \\rho Y_{t-1} + \\epsilon_{t}\r\\]\rwhere \\(\\epsilon_{t}\\sim N\\left(0,\\sigma_{\\epsilon}^{2}\\right)\\) and \\(\\rho \\in \\mathbb{R}\\) is a parameter that controls the degree to which \\(Y_t\\) is correlated with \\(Y_{t-1}\\). This model is called an AR process of order 1 because \\(Y_t\\) only depends on \\(Y_{t-1}\\).\nWe can also rearrange terms to emphasize that this representation defines the conditional distribution of \\(Y_{t}\\) given \\(Y_{t-1}\\):\n\\[\r\\begin{aligned}\rY_{t} \\vert Y_{t-1} \\sim\u0026amp; N(\\rho Y_{t-1}, \\sigma_\\epsilon^2) \\\\\rY_1 \\sim\u0026amp; N(0, \\frac{\\sigma_\\epsilon^2}{1-\\rho^2})\r\\end{aligned}\r\\]\nWhere the variance of \\(Y_1\\) comes from the unconditional variance, which is derived below. The stationarity condition of an AR process is that each \\(Y_t\\) has the same distribution; that is, \\(\\mu = \\mathrm{E}(Y_i) = \\mathrm{E}Y_j\\) and \\(\\sigma^2 = \\mathrm{Var}(Y_i) = \\mathrm{Var}(Y_j)\\) for all \\(i, j\\).\nNow we can derive the unconditional mean and variance of \\(Y_t\\):\n$$\r\\begin{aligned}\r(Y_{t}) \u0026amp; = (Y_{t - 1} + {t} )\\\r\u0026amp; = ( Y{t - 1 } )\\\r\u0026amp; = \\\r\u0026amp; = 0 \\\n(Y_{t}) \u0026amp; = (Y_{t-1} + {t})\\\r\u0026amp; = ^{2}(Y{t-1}) + ({t})\\\r\u0026amp; = ^{2}(Y{t-1}) + {}{2}\\\r{2} \u0026amp; = {2}{2} + {}{2} \\\r{2}(1-^{2}) \u0026amp; = _{}{2}\\\r{2} \u0026amp; = \\end{aligned}\r$$\nThe plot below shows several examples of draws from an AR(1) process with differing values of \\(\\rho\\) and \\(\\sigma^2_\\epsilon = 1\\):\r\rGaussian processes\rGaussian processes model a set of variables as being multivariate normally distributed with mean \\(\\boldsymbol{\\mu}\\) and variance/covariance matrix \\(\\boldsymbol{\\Sigma}\\):\n\\[\r\\mathbf{Y} \\sim MVN(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\r\\]\nUsually the mean vector is set to \\(\\boldsymbol{0}\\), which means the Gaussian process is fully defined by its choice of variance/covariance matrix \\(\\boldsymbol{\\Sigma}\\). The variance/covariance matrix is defined by a kernel function which defines the covariance between any two variables:\n\\[\r\\Sigma_{i,j} = K(i, j)\r\\]\n\rAn AR(1) process is a Gaussian process\rWe want to show that an AR process can be represented as a Gaussian process. To do this, we need to show that \\(\\mathbf{Y}\\) is jointly normally distributed with some mean vector and variance/covariance matrix.\nWe already know that \\(\\mathrm{E}(Y_t)=0\\), so the mean vector of its joint distribution will be \\(\\mathbf{0}\\).\nTo find the variance/covariance matrix, we need to derive the covariance between \\(Y_{t_1}\\) and \\(Y_{t_2}\\). First, let’s consider the simpler case of the covariance between \\(Y_t\\) and \\(Y_{t+1}\\):\n\\[\r\\begin{aligned}\r\\operatorname{cov}(Y_{t}, Y_{t+1}) \u0026amp;= \\operatorname{E} \\left[ \\left( Y_t - \\operatorname{E}[Y_t] \\right) \\left( Y_{t+1} - \\operatorname{E}[Y_{t+1}] \\right) \\right] \\text{ (definition of covariance) } \\\\\r\u0026amp;= \\operatorname{E} \\left[ Y_t Y_{t+1} \\right] \\text{ (because } \\operatorname{E}[Y_t] = \\operatorname{E}[Y_{t+1}] = 0 \\text{)} \\\\\r\u0026amp;= \\operatorname{E} \\left[ Y_t \\left( \\rho Y_{t} + \\epsilon_{t+1} \\right) \\right] \\\\\r\u0026amp;= \\operatorname{E} \\left[ \\rho Y_t^2 + Y_t \\epsilon_{t+1} \\right] \\\\\r\u0026amp;= \\rho \\operatorname{E}\\left[ Y_t^2 \\right] \\\\\r\u0026amp;= \\rho (\\operatorname{Var}(Y_t) + \\operatorname{E}[Y_t]^2) \\\\\r\u0026amp;= \\rho \\frac{\\sigma_\\epsilon^2}{1 - \\rho^2}\r\\end{aligned}\r\\]\nfor \\(Y\\)s separated by more than one time point, iterating the above result yields the expression\n\\[\r\\begin{aligned}\r\\operatorname{cov}(Y_{t_1}, Y_{t_2}) = \\rho^{\\vert t_1 - t_2 \\vert} \\frac{\\sigma_\\epsilon^2}{1 - \\rho^2}\r\\end{aligned}\r\\]\nNow we can fully define the joint distribution of \\(\\mathbf{Y}\\):\n\\[\r\\mathbf{Y} \\sim MVN(\\mathbf{0}, \\boldsymbol{\\Sigma})\r\\]\nwhere \\(\\Sigma_{i,j} = \\rho^{\\vert i - j \\vert} \\frac{\\sigma_\\epsilon^2}{1-\\rho^2}\\). This is a Gaussian process!\n\rCombining kernel functions\rThe nice thing about Gaussian processes is that we can combine multiple kernel functions to model processes with dependence from different sources. Two ways kernels can be combined are by multiplication and addition. Multiplying two kernels is like an “AND” operation: the correlation between points will be high if the correlation from both kernels is high. Adding two kernels together is like an “OR” operation: correlation is high if either kernel indicates high covariance.\nAs an example, let’s build a Gaussian process that combines an AR process (for temporal correlation) and a spatial process (for spatial correlation) by combining two kernel functions. First, we need to define an outcome variable \\(Y\\) that varies in time and space: let \\(Y_{c,t}\\) be a random variable indexed by spatial site \\(c\\) at timepoint \\(t\\). We take the AR covariance as the first kernel function, to model temporal correlation:\n\\[\rK_1(i, j) = \\rho^{\\vert t_i - t_j \\vert} \\frac{\\sigma_\\epsilon^2}{1 - \\rho^2}\r\\]\nand a squared-exponential kernel function to model spatial dependence:\n\\[\rK_2(i, j) = \\alpha^2 \\exp\\left( -\\frac{d(i, j)}{2\\lambda^2} \\right)\r\\]\nwhere \\(d(i, j)\\) is the spatial distance between sites \\(i\\) and \\(j\\), \\(\\lambda\\) is a length-scale parameter, and \\(\\alpha^2\\) is a parameter controlling the magnitude of the covariance.\nCombine the two kernel functions so that two data points are correlated if they are close together in time and space:\n\\[\r\\begin{aligned}\rK(i, j) \u0026amp;= K_1(i, j) \\times K_2(i, j) \\\\\r\u0026amp;= \\rho^{\\vert t_i - t_j \\vert} \\frac{\\sigma_\\epsilon^2}{1 - \\rho^2} \\alpha^2 \\exp\\left( -\\frac{d(i, j)}{2\\lambda^2} \\right)\r\\end{aligned}\r\\]\nNote the parameters \\(\\sigma^2_\\epsilon\\) and \\(\\alpha^2\\), which are multipled together, would be unidentifiable in parameter estimation and should be replaced by a single parameter that controls the magnitude of the covariance.\nTo illustrate this Gaussian process model, I started by generating a set of sites with random locations:\nthen I drew from the Gaussian process using the parameters temporal parameters \\(\\rho=0.9\\), \\(\\sigma_\\epsilon^2=1\\) and spatial parameters \\(\\alpha = 1\\) and \\(\\lambda=2\\).\nThe plot below shows the time trend in the first six sites:\nAnd the spatial distribution over time of \\(Y_{c,t}\\) is shown below:\rVisually we can see that the Gaussian process generates data that is correlated in both time and space.\n\rModeling using the mean and the covariance\rThe spatio-temporal Gaussian process we defined in the previous section does its modeling through the variance/covariance matrix, with its mean function set to zero. An alternative way to think about a spatio-temporal process is akin to the first AR representation we looked at, and define \\(\\mathbf{Y}_t\\) (the set of all \\(Y_{c,t}\\) at time \\(t\\)) relative to \\(\\mathbf{Y}_{t-1}\\):\n\\[\r\\begin{aligned}\r\\mathbf{Y}_{t} = \\rho \\mathbf{Y}_{t-1} + \\boldsymbol{\\epsilon}_t\r\\end{aligned}\r\\]\nwhere \\(\\boldsymbol{\\epsilon_t} \\sim MVN(\\mathbf{0}, \\boldsymbol{\\Sigma}_\\epsilon)\\).\nIf we set \\(\\boldsymbol{\\Sigma_\\epsilon}\\) to be the diagonal matrix \\(\\boldsymbol{\\Sigma}_\\epsilon = \\sigma^2_\\epsilon \\mathbf{I}_n\\) then we will have an independent AR(1) independent process for each spatial site. It gets more interesting if we define \\(\\boldsymbol{\\Sigma}_\\epsilon\\) by a covariance function so we can include dependence between sites, for example dependence based on the distance between the sites. For now, let’s use the squared exponential kernel and define \\(\\Sigma_{i,j} = \\alpha^2 \\exp\\left(-\\frac{d(i, j)}{2\\lambda^2} \\right)\\).\nIs this process also equivalent to a mean zero Gaussian process with some covariance kernel? We’ll answer this question by deriving the covariance between any two points.\nThe mean of \\(\\mathbf{Y_t}\\) can be shown to be zero in the same way we showed a univariate AR process has mean 0. We also need to know the overall variance/covariance matrix of \\(\\mathbf{Y}_t\\), which we’ll call \\(\\boldsymbol{\\Phi}\\); the logic is imilar to the univariate case, and I’ll show it here for completeness:\n\\[\r\\begin{aligned}\r\\operatorname{Var}\\left(\\boldsymbol{Y}_{t}\\right) \u0026amp; =\\operatorname{Var}\\left(\\rho^{2}\\mathbf{Y}_{t-1} + \\boldsymbol{\\epsilon}_{t}\\right) \\\\\r\u0026amp;= \\rho^{2}\\operatorname{Var}\\left(\\boldsymbol{Y}_{t-1}\\right)+\\operatorname{Var}\\left(\\boldsymbol{\\epsilon}_{t}\\right) \\\\\r\\boldsymbol{\\Phi} \u0026amp; =\\rho^{2}\\boldsymbol{\\Phi}+\\boldsymbol{\\Sigma}_\\epsilon \\\\\r\\boldsymbol{\\Phi}-\\rho^{2}\\boldsymbol{\\Sigma} \u0026amp;= \\boldsymbol{\\Sigma}_\\epsilon \\\\\r\\boldsymbol{\\Phi}\\left(\\mathbf{I}-\\rho^{2}\\mathbf{I}\\right) \u0026amp; =\\boldsymbol{\\Sigma}_\\epsilon \\\\\r\\boldsymbol{\\Phi} \u0026amp;=\\boldsymbol{\\Sigma}_{\\epsilon}\\left(\\mathbf{I}-\\rho^{2}\\mathbf{I}\\right)^{-1}\r\\end{aligned}\r\\]\nIf we pull out two sites at the same time point, their covariance is \\(\\mathrm{cov}(Y_{t,c_1}, Y_{t,c_2}) = \\frac{\\Sigma_{\\epsilon, c_1, c_2}}{1-\\rho^2}\\), which looks very similar to the unidimensional AR(1) process variance.\nNow we derive the covariance between any two sites that are one time point apart:\n\\[\r\\begin{aligned}\r\\mathrm{cov}\\left(y_{c_1,t},y_{c_2,t+1}\\right) \u0026amp; =\\mathrm{E}\\left[\\left(y_{c_1,t}-\\mathrm{E}\\left[y_{c_1,t}\\right]\\right)\\left(y_{c_2,t}-\\mathrm{E}\\left[y_{c_2,t}\\right]\\right)\\right]\\\\\r\u0026amp; =\\mathrm{E}\\left[y_{c_1,t}y_{c_2,t}\\right]\\\\\r\u0026amp; =\\mathrm{E}\\left[y_{c_1,t}\\left[\\rho y_{c_2,t}+\\epsilon_{c_2,t+1}\\right]\\right]\\\\\r\u0026amp; =\\rho\\mathrm{E}\\left[y_{c_1,t}y_{c_2,t}\\right]\\\\\r\u0026amp; =\\rho\\mathrm{cov}\\left(y_{c_1,t}y_{c_2,t}\\right)\\\\\r\u0026amp; =\\rho\\frac{\\Sigma_{i,j}}{1-\\rho^2} \\\\\r\u0026amp;= \\rho \\frac{1}{1-\\rho^2} \\Sigma_{i,j} \\\\\r\u0026amp;= \\rho \\frac{1}{1-\\rho^2} \\alpha^2 \\exp\\left(-\\frac{d(i, j)}{2\\lambda^2} \\right)\r\\end{aligned}\r\\]\nfor sites more than one time point away from each other, we can iterate the above result to get a general expression of the covariance between any two points:\n\\[\r\\begin{aligned}\r\\mathrm{cov}\\left(y_{c_1,t_1},y_{c_2,t_2}\\right) \u0026amp;= \\rho^{\\vert t_1 - t_2 \\vert}\\frac{1}{1-\\rho^2} \\alpha^2 \\exp\\left(-\\frac{d(i, j)}{2\\lambda^2} \\right)\r\\end{aligned}\r\\]\nif we reparameterize \\(\\alpha\\) to be the product of two parameters \\(\\alpha = \\sigma^2_\\epsilon \\alpha\\), we get\n\\[\r\\begin{aligned}\r\\mathrm{cov}\\left(y_{c_1,t_1},y_{c_2,t_2}\\right) \u0026amp;= \\rho^{\\vert t_1 - t_2 \\vert}\\frac{\\sigma^2_\\epsilon}{1-\\rho^2} \\alpha^2 \\exp\\left(-\\frac{d(i, j)}{2\\lambda^2} \\right) \\\\\r\u0026amp;= K_1(i, j) \\times K_2(i,j)\r\\end{aligned}\r\\]\nwhich is the product of an AR(1) and squared exponential kernel function as defined in the previous section. In practice we wouldn’t want to separate these parameters because both of them will not be identifiable given observed data, but I separated them here to show how the covariance structure is the product of two kernel functions.\nTherefore, we can write this process in the form of a Gaussian process with mean zero and covariance kernel given by the product of a temporal and spatial kernel:\n\\[\r\\begin{aligned}\r\\mathbf{Y} \\sim\u0026amp; MVN(\\mathbf{0}, \\boldsymbol{\\Sigma}) \\\\\r\\Sigma_{i,j} =\u0026amp; K_1(i, j) \\times K_2(i, j) \\end{aligned}\r\\]\nThe spatio-temporal processes defined as a set of conditional distributions and as a joint Gaussian process are equivalent.\nTo summarize, AR processes can be written as a Gaussian process model, which is useful because a temporal process can then be easily combined with other sources of dependence. In general, we can build our models by defining conditional distributions with a given mean and covariance, or a joint distribution with mean zero where the model is fully defined by a variance/covariance kernel function. In a future post I will look at Bayesian parameter estimation in these models using Stan.\n\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"341c20a888a3edbdbc9eb7d70be5b24a","permalink":"/1/01/01/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/1/01/01/","section":"post","summary":"Autoregressive (AR) processes are a popular choice for modeling time-varying processes. AR processes are typically written down as a set of conditional distributions, but if we do some algebra we can show how they can also be written as a Gaussian process. Having a Guassian process representation is useful because it is more clear how the AR process could be incorporated into larger models, like a spatio-temporal model. In this post, we’ll start with defining an AR process and deriving its mean and variance, then we’ll derive its joint distribution, which is a Gaussian process.","tags":null,"title":"","type":"post"}]